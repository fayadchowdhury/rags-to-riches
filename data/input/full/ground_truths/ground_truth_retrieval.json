[{"references":["benchmarks.pdf"],"question":"Which benchmark evaluates a model's general world knowledge across diverse subjects and is crucial for applications in education and professional assistance?","answer":"MMLU"},{"references":["benchmarks.pdf"],"question":"True\/False:\nThe StoryCloze dataset tests a model's ability to reason about narrative coherence by selecting the most plausible story ending.","answer":"True"},{"references":["benchmarks.pdf"],"question":"Fill-in-the-Blank:\nThe ________ benchmark focuses on evaluating multilingual sentence embeddings for cross-lingual search and document retrieval.","answer":"MTEB"},{"references":["benchmarks.pdf"],"question":"Match the following benchmarks with their evaluation focus:\n1. MTEB\n2. SWAG\n3. Lambada\n4. MMLU\na) Narrative comprehension\nb) Multilingual sentence embeddings\nc) General world knowledge\nd) Grounded commonsense inference","answer":"MTEB \u2192 b) Multilingual sentence embeddings\nSWAG \u2192 d) Grounded commonsense inference\nLambada \u2192 a) Narrative comprehension\nMMLU \u2192 c) General world knowledge"},{"references":["benchmarks.pdf"],"question":"Explain the significance of SWAG as a benchmark in grounded commonsense inference tasks.","answer":"SWAG evaluates large-scale adversarial datasets for grounded commonsense inference and includes tasks like completing partial descriptions. It tests a model's ability to understand situational dynamics and make logical predictions."},{"references":["benchmarks.pdf"],"question":"Compare and contrast GLUE and SuperGLUE in terms of their focus and difficulty.","answer":"GLUE focuses on evaluating general language understanding through simpler tasks, while SuperGLUE includes more rigorous and challenging tasks, such as commonsense reasoning and multi-sentence comprehension, which are harder for large language models."},{"references":["benchmarks.pdf"],"question":"Given two sentences, 'She loves classical music.' and 'Classical music is her favorite genre.', determine their similarity score based on the Semantic Textual Similarity Benchmark criteria.","answer":"The similarity score would likely be high, around 4.5 to 5, as the sentences are semantically equivalent but use different phrasing."},{"references":["benchmarks.pdf"],"question":"Diagram-Based Question:\nDesign a process flowchart for evaluating a model's performance on the SST-2 dataset.","answer":"Steps for the flowchart:\nInput Sentence \u2192 Tokenization \u2192 Sentiment Classification (Positive\/Negative) \u2192 Compare Predicted Labels to True Labels \u2192 Calculate Accuracy and F1 Score."},{"references":["benchmarks.pdf"],"question":"Essay-Type Question:\nDiscuss the role of adversarial datasets like SWAG in improving the robustness of language models. Provide examples of their practical applications.","answer":"Adversarial datasets like SWAG enhance robustness by challenging models to predict logical completions in grounded scenarios, testing their understanding of situational contexts. Practical applications include automated story generation, virtual assistants, and interactive narrative-based games."},{"references":["benchmarks.pdf"],"question":"Practical Implementation Question:\nWrite a Python function to compute the BLEU score for evaluating a model's output against reference sentences.","answer":"from nltk.translate.bleu_score import sentence_bleu\n\ndef calculate_bleu(reference, candidate):\n    \"\"\"\n    Calculates the BLEU score for a candidate sentence.\n\n    Args:\n        reference (list): List of reference sentences (list of tokens).\n        candidate (list): List of tokens in the candidate sentence.\n        \n    Returns:\n        float: BLEU score.\n    \"\"\"\n    return sentence_bleu([reference], candidate)\n\n# Example Usage\nreference = ['This', 'is', 'a', 'test']\ncandidate = ['This', 'is', 'a', 'trial']\nprint(\"BLEU Score:\", calculate_bleu(reference, candidate))\n"},{"references":["compression.pdf"],"question":"Which of the following techniques is used in model compression to reduce the size of a language model by mimicking a larger model?\n\na) Pruning\nb) Weight Sharing\nc) Distillation\nd) Low-rank factorization","answer":"c) Distillation"},{"references":["compression.pdf"],"question":"True\/False:\nDistilBERT reduces the number of layers in BERT by a factor of two while keeping the hidden size identical.","answer":"TRUE"},{"references":["compression.pdf"],"question":"Fill-in-the-Blank:\nThe ____________ method involves training a smaller student model using the outputs of a larger teacher model to mimic its behavior.","answer":"Distillation"},{"references":["compression.pdf"],"question":"Match the following compression techniques with their descriptions:\n\nPruning\nWeight Sharing\nLow-Rank Factorization\nReduce Precision\n\na) Sharing parameter values among layers (e.g., ALBERT).\nb) Representing weight matrices as the product of smaller matrices.\nc) Converting weights to formats like FP16 or int8.\nd) Removing less important weights, layers, or attention heads.","answer":"Pruning \u2192 d) Removing less important weights, layers, or attention heads.\nWeight Sharing \u2192 a) Sharing parameter values among layers (e.g., ALBERT).\nLow-Rank Factorization \u2192 b) Representing weight matrices as the product of smaller matrices.\nReduce Precision \u2192 c) Converting weights to formats like FP16 or int8."},{"references":["compression.pdf"],"question":"Why does DistilBERT maintain the hidden size of BERT while reducing the number of layers?","answer":"The number of layers is the primary factor affecting inference time, so reducing layers while keeping the hidden size intact preserves the ability to initialize the student model from the teacher while improving inference efficiency."},{"references":["compression.pdf"],"question":"Compare the standard log-likelihood loss with the distillation loss in language model training. How does the latter utilize the teacher's outputs?","answer":"The standard log-likelihood loss relies on ground truth labels to minimize the prediction error, while the distillation loss uses the teacher model\u2019s output distribution as soft targets. This helps the student model learn from the knowledge encoded in the teacher's predictions, including nuances not captured by hard labels."},{"references":["compression.pdf"],"question":"Suppose you have a language model with 12 layers. If you apply pruning to iteratively remove 10% of the least important parameters per round, how many rounds will it take to reduce the parameters to less than 50% of the original count?","answer":"After n rounds, the remaining parameters are (1\u22120.1)^n\u00d7100%. Solving (0.9)^n<0.5: n=log(0.5)\/log(0.9)\u200b\u22487.27, It will take approximately 8 rounds."},{"references":["compression.pdf"],"question":"Diagram-Based Question:\nCreate a flowchart illustrating the steps in iterative pruning, starting with initializing parameters and ending with identifying the 'winning ticket.'","answer":"Steps for the flowchart:\n1. Initialize the model with random parameters.\n2. Train the model for a set number of iterations.\n3. Prune a percentage of parameters with the smallest magnitude.\n4. Reset the remaining parameters to their initial values.\n5. Retrain the pruned model.\n6. Repeat steps 3\u20135 iteratively until the 'winning ticket' is identified."},{"references":["compression.pdf"],"question":"Discuss the trade-offs involved in compressing large language models using distillation, pruning, and weight sharing. Provide examples of scenarios where each method might be optimal.","answer":"Distillation: Provides a smaller model with similar task performance. Optimal for scenarios like deploying models on edge devices requiring fast inference. The trade-off is a slight drop in accuracy compared to the teacher model.\nPruning: Reduces memory and compute requirements by removing redundant parameters. It is optimal when inference speed is critical, such as in real-time applications. However, it requires careful tuning to avoid significant performance loss.\nWeight Sharing: Greatly reduces memory usage by sharing parameters across model components. Best for training multilingual models where parameters can be shared across languages. However, it might limit the model\u2019s flexibility."},{"references":["compression.pdf"],"question":"Write a Python snippet that implements the softmax function for soft targets in knowledge distillation, given logits and a temperature parameter.","answer":"import numpy as np\n\ndef softmax_with_temperature(logits, temperature=1.0):\n    \"\"\"\n    Compute softmax with a temperature scaling factor.\n\n    Args:\n        logits (np.ndarray): Logits from the model.\n        temperature (float): Temperature scaling factor.\n\n    Returns:\n        np.ndarray: Softmax probabilities.\n    \"\"\"\n    scaled_logits = logits \/ temperature\n    exp_values = np.exp(scaled_logits - np.max(scaled_logits))  # For numerical stability\n    return exp_values \/ np.sum(exp_values)\n\n# Example usage\nlogits = np.array([2.0, 1.0, 0.1])\ntemperature = 2.0\nprint(\"Softmax probabilities:\", softmax_with_temperature(logits, temperature))"},{"references":["cross_attention.pdf"],"question":"What is the primary purpose of attention in Neural Machine Translation (NMT)?\n\na) To create embeddings for the source language\nb) To align source and target language representations\nc) To initialize weights for the model\nd) To compute the word embeddings","answer":"b) To align source and target language representations"},{"references":["cross_attention.pdf"],"question":"True\/False:\nThe dot product mechanism is one of the ways to compute attention scores in neural machine translation.","answer":"TRUE"},{"references":["cross_attention.pdf"],"question":"Fill-in-the-Blank:\nIn attention-based NMT, the context vector is computed as a weighted sum of all _________ vectors from the source sequence.","answer":"hidden"},{"references":["cross_attention.pdf"],"question":"Match the terms to their descriptions in an attention mechanism:\n\nQuery\nKey\nValue\nContext vector\n\na) A representation of the source sentence used for alignment.\nb) The result of weighted aggregation of value vectors.\nc) A vector that represents the target sentence's current state.\nd) A vector used to compute the relevance of each key.","answer":"Query \u2192 c) A vector that represents the target sentence's current state.\nKey \u2192 d) A vector used to compute the relevance of each key.\nValue \u2192 a) A representation of the source sentence used for alignment.\nContext vector \u2192 b) The result of weighted aggregation of value vectors."},{"references":["cross_attention.pdf"],"question":"Explain why attention mechanisms improve neural machine translation compared to traditional sequence-to-sequence models without attention.","answer":"Attention mechanisms allow the model to focus on specific parts of the source sequence when predicting each target token. This avoids the bottleneck of compressing the entire source sequence into a single fixed-length vector and improves translation quality, especially for long sentences or complex dependencies."},{"references":["cross_attention.pdf"],"question":"Compare and contrast 'no attention' versus 'with attention' approaches in neural machine translation. Discuss the implications on model performance and alignment.","answer":"No Attention: Relies on a fixed-length vector to represent the source sentence, leading to poor performance for long or complex sequences.\nWith Attention: Dynamically computes a context vector for each target token, using weighted contributions of source hidden states, resulting in better alignment and improved translation accuracy."},{"references":["cross_attention.pdf"],"question":"Given a source sequence with attention scores, compute the context vector if the source hidden states are:\n\nc_t = \u03a3 \u03b1_t,j h_j\nExample: c_t = (0.1 \u00d7 [1,0]) + (0.2 \u00d7 [0,1]) + (0.3 \u00d7 [1,1]) + (0.4 \u00d7 [0,0])\nCompute c_t.","answer":"c_t = [0.4, 0.5]"},{"references":["cross_attention.pdf"],"question":"Diagram-Based Question:\nDraw a diagram to show how the attention mechanism works in Neural Machine Translation, illustrating the interaction between query, key, value, and context vector.","answer":"Steps for the diagram:\n1. Represent the source sequence as hidden states (h1, h2, ..., hTx).\n2. Compute alignment scores using dot product\/MLP between query (decoder state) and keys (encoder states).\n3. Apply softmax to alignment scores to get attention weights.\n4. Compute the context vector as a weighted sum of the value vectors."},{"references":["cross_attention.pdf"],"question":"Discuss the role of softmax in attention mechanisms and why it is essential for computing alignment scores.","answer":"The softmax function normalizes alignment scores into a probability distribution, ensuring that the weights assigned to source hidden states sum to 1. This allows the attention mechanism to focus on the most relevant parts of the source sequence while still considering the entire input, improving the model's interpretability and performance."},{"references":["cross_attention.pdf"],"question":"Write a Python snippet that computes attention scores using the dot product mechanism between a query and keys.","answer":"import numpy as np\n\ndef compute_attention_scores(query, keys):\n    \"\"\"\n    Compute attention scores using dot product.\n\n    Args:\n        query (np.ndarray): Query vector (shape: [d]).\n        keys (np.ndarray): Key matrix (shape: [T, d]).\n\n    Returns:\n        np.ndarray: Attention scores (shape: [T]).\n    \"\"\"\n    scores = np.dot(keys, query)  # Dot product between query and keys\n    softmax_scores = np.exp(scores) \/ np.sum(np.exp(scores))  # Softmax\n    return softmax_scores\n\n# Example usage\nquery = np.array([1, 0])\nkeys = np.array([[1, 0], [0, 1], [1, 1], [0, 0]])\nattention_scores = compute_attention_scores(query, keys)\nprint(\"Attention Scores:\", attention_scores)"},{"references":["decoding.pdf"],"question":"Which decoding method keeps the top k most probable words at each step and normalizes their probabilities?\n\na) Beam Search\nb) Top-k Sampling\nc) Contrastive Search\nd) Greedy Decoding","answer":"b) Top-k Sampling"},{"references":["decoding.pdf"],"question":"True\/False:\nGreedy decoding is deterministic and always selects the most probable token at each step.","answer":"TRUE"},{"references":["decoding.pdf"],"question":"Fill-in-the-Blank:\nIn _________ sampling, the smallest set of words whose cumulative probability exceeds a threshold p is selected.","answer":"Top-p Nucleus"},{"references":["decoding.pdf"],"question":"Match the following decoding methods to their descriptions:\n\nGreedy Decoding\nBeam Search\nTop-p Sampling\nContrastive Search\n\na) Selects the smallest set of words whose cumulative probability exceeds a threshold.\nb) Penalizes repetitive n-grams to reduce model degeneration.\nc) Combines top-k predictions with a cosine similarity penalty to discourage redundancy.\nd) Selects the most probable word at each step.","answer":"Greedy Decoding \u2192 d) Selects the most probable word at each step.\nBeam Search \u2192 b) Penalizes repetitive n-grams to reduce model degeneration.\nTop-p Sampling \u2192 a) Selects the smallest set of words whose cumulative probability exceeds a threshold.\nContrastive Search \u2192 c) Combines top-k predictions with a cosine similarity penalty to discourage redundancy."},{"references":["decoding.pdf"],"question":"Why is temperature used in sampling-based decoding methods?","answer":"Temperature is used to control the randomness of token selection in sampling-based methods. A lower temperature (e.g., T<1) makes the distribution more deterministic by emphasizing higher-probability tokens, while a higher temperature (T>1) introduces more diversity by flattening the probability distribution."},{"references":["decoding.pdf"],"question":"Compare and contrast Beam Search and Top-k Sampling in terms of diversity and deterministic behavior.","answer":"Beam Search: Deterministic method that keeps the top k sequences at each step, often leading to repetitive outputs or similar continuations due to overlapping sub-trees. It lacks diversity.\nTop-k Sampling: Introduces randomness by selecting from the top k tokens at each step, improving diversity in the generated text but sacrificing determinism and predictability."},{"references":["decoding.pdf"],"question":"If a language model generates tokens with probabilities [0.5, 0.3, 0.15, 0.05] and you use Top-3 sampling, which tokens are included in the next-step prediction?","answer":"Top-3 sampling selects the top 3 tokens based on their probabilities. The selected tokens are those with probabilities 0.5, 0.3, and 0.15. The fourth token (0.05) is excluded."},{"references":["decoding.pdf"],"question":"Diagram-Based Question:\nDraw a flowchart illustrating how Top-p Nucleus Sampling works, starting from token probabilities and ending with selected tokens for generation.","answer":"Steps for the flowchart:\n1. Start with token probabilities sorted in descending order.\n2. Calculate cumulative probabilities for each token.\n3. Identify the smallest subset of tokens whose cumulative probability exceeds the threshold p.\n4. Normalize the probabilities within this subset.\n5. Randomly sample a token from the subset for generation."},{"references":["decoding.pdf"],"question":"Discuss the trade-offs of using greedy decoding versus sampling-based methods for generating text in large language models. Provide examples of tasks where each method might be more appropriate.","answer":"Greedy Decoding: Greedy decoding is deterministic and suitable for tasks requiring exactness, such as translation or summarization. However, it often produces repetitive or less diverse text, making it unsuitable for creative tasks.\nSampling-Based Methods: Sampling-based methods introduce randomness, generating diverse and creative text, ideal for tasks like storytelling or dialogue generation. However, they risk incoherence or gibberish outputs if not tuned properly with techniques like temperature control."},{"references":["decoding.pdf"],"question":"Write a Python function to normalize a set of token probabilities using Top-k sampling.","answer":"import numpy as np\n\ndef top_k_sampling(probabilities, k):\n    \"\"\"\n    Normalize probabilities using Top-k sampling.\n\n    Args:\n        probabilities (list or np.ndarray): Probabilities of tokens.\n        k (int): Number of top tokens to keep.\n\n    Returns:\n        np.ndarray: Normalized probabilities for the top-k tokens.\n    \"\"\"\n    probabilities = np.array(probabilities)\n    # Get indices of top-k tokens\n    top_k_indices = np.argsort(probabilities)[-k:]\n    top_k_probs = probabilities[top_k_indices]\n    # Normalize top-k probabilities\n    normalized_probs = top_k_probs \/ np.sum(top_k_probs)\n    return top_k_indices, normalized_probs\n\n# Example Usage\nprobabilities = [0.5, 0.3, 0.15, 0.05]\nk = 3\nindices, normalized_probs = top_k_sampling(probabilities, k)\nprint(\"Top-k Indices:\", indices)\nprint(\"Normalized Probabilities:\", normalized_probs)"},{"references":["tokenization.pdf"],"question":"Which tokenization method learns a vocabulary of parts of words (subwords) and merges the most frequent pairs during training?\n\na) Wordpiece\nb) Byte Pair Encoding (BPE)\nc) SentencePiece\nd) Space-based Tokenization","answer":"b) Byte Pair Encoding (BPE)"},{"references":["tokenization.pdf"],"question":"True\/False:\nIn traditional NLP models, all novel words at test time were mapped to the [UNK] token.","answer":"TRUE"},{"references":["tokenization.pdf"],"question":"Fill-in-the-Blank:\nIn the Byte Pair Encoding algorithm, subwords are initialized with individual _________ or bytes and an 'end of word' token.","answer":"characters"},{"references":["tokenization.pdf"],"question":"Match the following terms with their definitions or examples:\n\n[UNK]\nSubword Tokenization\nNeologisms\nCommon Words\n\na) Kept as part of the vocabulary and ignored during morphological processing.\nb) Words like 'Transformerify' that are newly coined.\nc) The token used to replace unknown words in traditional NLP.\nd) Splits rare words into meaningful units like prefixes and suffixes.","answer":"[UNK] \u2192 c) The token used to replace unknown words in traditional NLP.\nSubword Tokenization \u2192 d) Splits rare words into meaningful units like prefixes and suffixes.\nNeologisms \u2192 b) Words like 'Transformerify' that are newly coined.\nCommon Words \u2192 a) Kept as part of the vocabulary and ignored during morphological processing."},{"references":["tokenization.pdf"],"question":"Why is subword tokenization preferred over space-based tokenization in modern NLP models?","answer":"Subword tokenization is preferred because it handles rare words, spelling errors, and neologisms effectively by breaking them into meaningful subunits rather than mapping them to [UNK]. This increases vocabulary efficiency and model performance, especially for unseen or low-frequency words."},{"references":["tokenization.pdf"],"question":"Compare space-based tokenization and Byte Pair Encoding (BPE) in terms of their handling of rare words and out-of-vocabulary tokens.","answer":"Space-based Tokenization: Splits text into words based on spaces, which often leads to high numbers of [UNK] tokens for rare or unseen words.\nByte Pair Encoding (BPE): Breaks rare words into subwords or characters, significantly reducing the need for [UNK] tokens. It captures meaningful patterns like prefixes and suffixes, enabling better handling of rare or novel words."},{"references":["tokenization.pdf"],"question":"If the word 'Transformerify' is tokenized using BPE with the current vocabulary containing 'Transformer' and 'ify,' how will the word be split?","answer":"'Transformerify' will be tokenized into two subwords: 'Transformer' and 'ify,' as both are present in the vocabulary."},{"references":["tokenization.pdf"],"question":"Diagram-Based Question:\nDraw a diagram to illustrate the steps of the Byte Pair Encoding (BPE) algorithm, starting from initializing characters to merging subwords.","answer":"Steps for the diagram:\n1. Start with each character as a subword unit (e.g., T, r, a, n, s, etc.).\n2. Identify the most frequent adjacent pair of characters or subwords.\n3. Merge the pair into a new subword token.\n4. Replace all instances of this pair in the sequence with the new subword.\n5. Repeat steps 2\u20134 until reaching the maximum number of merges."},{"references":["tokenization.pdf"],"question":"Discuss the role of subword tokenization in handling spelling errors, variations, and rare words. Provide examples of how BPE improves NLP model performance in these scenarios.","answer":"Subword tokenization addresses the challenges posed by rare words, spelling errors, and neologisms by breaking words into smaller meaningful units. For example, 'taaasty' may be split into ta## and ##aasty, allowing the model to capture its similarity to 'tasty.' Similarly, BPE reduces the reliance on [UNK] tokens and increases the model's ability to generalize. This approach also improves embedding efficiency by representing multiple words with overlapping subwords, enhancing performance in tasks like translation and summarization."},{"references":["tokenization.pdf"],"question":"Write a Python function to merge the most common pair of adjacent subwords in a sequence during the BPE algorithm.","answer":"from collections import Counter\n\ndef merge_most_common_pair(sequence, merges):\n    \"\"\"\n    Merges the most common pair of adjacent subwords in a sequence.\n\n    Args:\n        sequence (list): List of subwords (tokens).\n        merges (int): Maximum number of merges to perform.\n\n    Returns:\n        list: Updated sequence after merging.\n    \"\"\"\n    for _ in range(merges):\n        # Count pairs of adjacent tokens\n        pairs = Counter(zip(sequence, sequence[1:]))\n        if not pairs:\n            break\n        # Find the most frequent pair\n        most_common_pair = max(pairs, key=pairs.get)\n        # Merge the pair in the sequence\n        merged_sequence = []\n        i = 0\n        while i < len(sequence):\n            if i < len(sequence) - 1 and (sequence[i], sequence[i+1]) == most_common_pair:\n                merged_sequence.append(sequence[i] + sequence[i+1])\n                i += 2  # Skip the merged pair\n            else:\n                merged_sequence.append(sequence[i])\n                i += 1\n        sequence = merged_sequence\n    return sequence\n\n# Example usage\ntokens = ['T', 'r', 'a', 'n', 's', 'f', 'o', 'r', 'm', 'e', 'r', 'i', 'f', 'y']\nmerged_sequence = merge_most_common_pair(tokens, merges=2)\nprint('Merged Sequence:', merged_sequence)"},{"references":["wordvectors.pdf"],"question":"Which word vector model uses co-occurrence probabilities and a global loss function for training?\n\na) Singular Value Decomposition (SVD)\nb) Word2Vec\nc) GloVe\nd) Continuous Bag of Words (CBOW)","answer":"c) GloVe"},{"references":["wordvectors.pdf"],"question":"True\/False:\nIn Word2Vec's Skip-gram model, the target word is predicted using individual context words.","answer":"TRUE"},{"references":["wordvectors.pdf"],"question":"Fill-in-the-Blank:\nIn the GloVe model, the co-occurrence matrix is denoted as \\( X_{ij} \\), where \\( X_{ij} \\) is the number of times word \\( i \\) occurs in the _________ of word \\( j \\).","answer":"context"},{"references":["wordvectors.pdf"],"question":"Match the following models to their characteristics:\n\nOne-hot Encoding\nSVD\nWord2Vec (CBOW)\nGloVe\n\na) Reduces dimensionality using matrix factorization.\nb) Predicts a center word from its surrounding context.\nc) Uses sparse, high-dimensional vectors with only one active value.\nd) Optimizes the squared error between observed and predicted log-cooccurrence values.","answer":"One-hot Encoding \u2192 c) Uses sparse, high-dimensional vectors with only one active value.\nSVD \u2192 a) Reduces dimensionality using matrix factorization.\nWord2Vec (CBOW) \u2192 b) Predicts a center word from its surrounding context.\nGloVe \u2192 d) Optimizes the squared error between observed and predicted log-cooccurrence values."},{"references":["wordvectors.pdf"],"question":"Why is dimensionality reduction important in word vector models?","answer":"Dimensionality reduction transforms large, sparse word representations into dense, lower-dimensional vectors that capture semantic relationships more efficiently, reducing storage requirements and computational complexity while improving generalization."},{"references":["wordvectors.pdf"],"question":"Compare the loss functions of Word2Vec (CBOW) and GloVe. How do they differ in terms of their optimization objectives?","answer":"Word2Vec (CBOW): Optimizes cross-entropy loss by predicting the center word given its context. It is trained using softmax probabilities to maximize the likelihood of observed context words.\nGloVe: Optimizes the squared error between the log of observed co-occurrence probabilities and the predicted probabilities. It uses global co-occurrence statistics to capture both local and long-range semantic relationships."},{"references":["wordvectors.pdf"],"question":"Given the context words 'the,' 'general,' and 'troops,' and their corresponding vectors, calculate the average context vector \\( v^ \\) for Word2Vec (CBOW).","answer":"The average context vector \\( v^ \\) is calculated as:\n\n\\( v^ = (v_{the} + v_{general} + v_{troops}) \/ 3 \\)\n\nIf \\( v_{the} = [1, 0] \\), \\( v_{general} = [0, 1] \\), \\( v_{troops} = [1, 1] \\), then:\n\n\\( v^ = ([1, 0] + [0, 1] + [1, 1]) \/ 3 = [2, 2] \/ 3 = [0.67, 0.67] \\)."},{"references":["wordvectors.pdf"],"question":"Diagram-Based Question:\nDraw a flowchart to illustrate the steps in Word2Vec's Continuous Bag of Words (CBOW) model, from averaging context vectors to computing softmax probabilities.","answer":"Steps for the diagram:\n1. Input context words and their vectors.\n2. Compute the average of the context vectors (\\( v^ \\)).\n3. Multiply \\( v^ \\) by the weight matrix to obtain scores (\\( z \\)).\n4. Apply softmax to \\( z \\) to calculate probabilities (\\( \\hat{y} \\)).\n5. Compute cross-entropy loss between predicted (\\( \\hat{y} \\)) and actual (\\( y \\)) outputs."},{"references":["wordvectors.pdf"],"question":"Discuss the trade-offs between intrinsic and extrinsic evaluation of word vectors. Provide examples of tasks for each type of evaluation.","answer":"Intrinsic Evaluation: Measures performance on intermediate tasks like analogy solving or word similarity. It is fast and helps identify specific strengths, such as capturing semantic relationships (e.g., 'king:man :: queen:woman'). However, it may not reflect performance on real-world tasks.\nExtrinsic Evaluation: Tests performance on downstream tasks like sentiment analysis or machine translation. While it provides practical insights, it is slower and makes it harder to pinpoint issues in the word vectors themselves."},{"references":["wordvectors.pdf"],"question":"Write a Python function to calculate cosine similarity between two word vectors.","answer":"import numpy as np\n\ndef cosine_similarity(vec1, vec2):\n    \"\"\"\n    Calculate cosine similarity between two word vectors.\n\n    Args:\n        vec1 (np.ndarray): First word vector.\n        vec2 (np.ndarray): Second word vector.\n\n    Returns:\n        float: Cosine similarity value.\n    \"\"\"\n    dot_product = np.dot(vec1, vec2)\n    norm_vec1 = np.linalg.norm(vec1)\n    norm_vec2 = np.linalg.norm(vec2)\n    return dot_product \/ (norm_vec1 * norm_vec2)\n\n# Example usage\nvec1 = np.array([1, 2, 3])\nvec2 = np.array([4, 5, 6])\nprint(\"Cosine Similarity:\", cosine_similarity(vec1, vec2))"}]