<!doctype html>
<html class="">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no" />
		<meta name="description" content="Weâ€™re on a journey to advance and democratize artificial intelligence through open source and open science." />
		<meta property="fb:app_id" content="1321688464574422" />
		<meta name="twitter:card" content="summary_large_image" />
		<meta name="twitter:site" content="@huggingface" />
		<meta name="twitter:image" content="https://huggingface.co/front/thumbnails/docs/transformers.png" />
		<meta property="og:title" content="Text generation strategies" />
		<meta property="og:type" content="website" />
		<meta property="og:url" content="https://huggingface.co/docs/transformers/generation_strategies" />
		<meta property="og:image" content="https://huggingface.co/front/thumbnails/docs/transformers.png" />

		<link rel="stylesheet" href="/front/build/kube-8e721ee/style.css" />

		<link rel="preconnect" href="https://fonts.gstatic.com" />
		<link
			href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700;1,900&display=swap"
			rel="stylesheet"
		/>
		<link
			href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;600;700&display=swap"
			rel="stylesheet"
		/>

		<link
			rel="preload"
			href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css"
			as="style"
			onload="this.onload=null;this.rel='stylesheet'"
		/>
		<noscript>
			<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" />
		</noscript>

		<script>if (window.matchMedia('(prefers-color-scheme: dark)').matches) { document.documentElement.classList.add('dark'); }</script>
<link rel="canonical" href="https://huggingface.co/docs/transformers/generation_strategies">
<link rel="alternate" hreflang="en" href="https://huggingface.co/docs/transformers/en/generation_strategies">
<link rel="alternate" hreflang="ja" href="https://huggingface.co/docs/transformers/ja/generation_strategies">
<link rel="alternate" hreflang="ko" href="https://huggingface.co/docs/transformers/ko/generation_strategies">
<link rel="alternate" hreflang="x-default" href="https://huggingface.co/docs/transformers/generation_strategies">  

		<title>Text generation strategies</title>

		<script
			defer
			data-domain="huggingface.co"
			event-loggedIn="true"
			src="/js/script.pageview-props.js"
		></script>
		<script>
			window.plausible =
				window.plausible ||
				function () {
					(window.plausible.q = window.plausible.q || []).push(arguments);
				};
		</script>
		<script>
			window.hubConfig = {"features":{"signupDisabled":false},"sshGitUrl":"git@hf.co","moonHttpUrl":"https:\/\/huggingface.co","captchaApiKey":"bd5f2066-93dc-4bdd-a64b-a24646ca3859","captchaDisabledOnSignup":false,"datasetViewerPublicUrl":"https:\/\/datasets-server.huggingface.co","stripePublicKey":"pk_live_x2tdjFXBCvXo2FFmMybezpeM00J6gPCAAc","environment":"production","userAgent":"HuggingFace (production)","spacesIframeDomain":"hf.space","spacesApiUrl":"https:\/\/api.hf.space","docSearchKey":"ece5e02e57300e17d152c08056145326e90c4bff3dd07d7d1ae40cf1c8d39cb6","logoDev":{"apiUrl":"https:\/\/img.logo.dev\/","apiKey":"pk_UHS2HZOeRnaSOdDp7jbd5w"}};
		</script>
		<script type="text/javascript" src="https://de5282c3ca0c.edge.sdk.awswaf.com/de5282c3ca0c/526cf06acb0d/challenge.js" defer></script>
	</head>
	<body class="flex flex-col min-h-dvh bg-white dark:bg-gray-950 text-black DocBuilderPage">
		<div class="flex min-h-dvh flex-col"><div class="SVELTE_HYDRATER contents" data-target="SystemThemeMonitor" data-props="{}"></div>
	<div class="SVELTE_HYDRATER contents" data-target="MainHeader" data-props="{&quot;authLight&quot;:{&quot;csrfToken&quot;:&quot;eyJkYXRhIjp7ImV4cGlyYXRpb24iOjE3MzMyNzA0NjU5MTMsInVzZXJJZCI6IjY1NTQwMWJmNDMyYWYxYjExMTU3NDU4NSJ9LCJzaWduYXR1cmUiOiI1NDRkZjAxNjMzMWM4ZmZjYTIyZjg2Y2ExZmIyZTFhYjgxZmRkYjdiZDhhNmNiOGI3MmM4NDhlOGViZjYxNzY5In0=&quot;,&quot;hasHfLevelAccess&quot;:false,&quot;u&quot;:{&quot;avatarUrl&quot;:&quot;/avatars/60c9e476e92f24b4f1719e25c7563b73.svg&quot;,&quot;isPro&quot;:false,&quot;orgs&quot;:[],&quot;user&quot;:&quot;fayadchowdhury&quot;,&quot;canPost&quot;:false,&quot;canHaveBilling&quot;:true,&quot;canCreateOrg&quot;:true,&quot;theme&quot;:&quot;system&quot;,&quot;notifications&quot;:{},&quot;usage&quot;:{&quot;storage&quot;:{&quot;limit&quot;:500000000000,&quot;used&quot;:0,&quot;count&quot;:0},&quot;inferenceApi&quot;:{&quot;used&quot;:0,&quot;limit&quot;:1000,&quot;duration&quot;:86400,&quot;renewal&quot;:86400,&quot;lastUpdated&quot;:&quot;2024-12-03T00:00:55.093Z&quot;},&quot;zeroGpu&quot;:{&quot;base&quot;:300,&quot;current&quot;:300,&quot;lastUpdated&quot;:&quot;2024-12-03T00:00:55.093Z&quot;}}}},&quot;classNames&quot;:&quot;&quot;,&quot;avatarUrl&quot;:&quot;/avatars/60c9e476e92f24b4f1719e25c7563b73.svg&quot;,&quot;isWide&quot;:true,&quot;isZh&quot;:false,&quot;user&quot;:&quot;fayadchowdhury&quot;,&quot;unreadNotifications&quot;:0,&quot;csrf&quot;:&quot;eyJkYXRhIjp7ImV4cGlyYXRpb24iOjE3MzMyNzA0NjU5MTMsInVzZXJJZCI6IjY1NTQwMWJmNDMyYWYxYjExMTU3NDU4NSJ9LCJzaWduYXR1cmUiOiI1NDRkZjAxNjMzMWM4ZmZjYTIyZjg2Y2ExZmIyZTFhYjgxZmRkYjdiZDhhNmNiOGI3MmM4NDhlOGViZjYxNzY5In0=&quot;,&quot;canCreateOrg&quot;:true,&quot;isPro&quot;:false}"><header class="border-b border-gray-100 "><div class="w-full px-4  flex h-16 items-center"><div class="flex flex-1 items-center"><a class="mr-5 flex flex-none items-center lg:mr-6" href="/"><img alt="Hugging Face's logo" class="w-7 md:mr-2" src="/front/assets/huggingface_logo-noborder.svg">
				<span class="hidden whitespace-nowrap text-lg font-bold md:block">Hugging Face</span></a>
			<div class="relative flex-1 lg:max-w-sm mr-2 sm:mr-4 md:mr-3 xl:mr-6"><input autocomplete="off" class="w-full dark:bg-gray-950 pl-8 form-input-alt h-9 pr-3 focus:shadow-xl " name="" placeholder="Search models, datasets, users..."   spellcheck="false" type="text" value="">
	<svg class="absolute left-2.5 text-gray-400 top-1/2 transform -translate-y-1/2" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg>
	</div>
			<div class="flex flex-none items-center justify-center p-0.5 place-self-stretch lg:hidden"><button class="relative z-40 flex h-6 w-8 items-center justify-center" type="button"><svg width="1em" height="1em" viewBox="0 0 10 10" class="text-xl" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" fill="currentColor"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.65039 2.9999C1.65039 2.8066 1.80709 2.6499 2.00039 2.6499H8.00039C8.19369 2.6499 8.35039 2.8066 8.35039 2.9999C8.35039 3.1932 8.19369 3.3499 8.00039 3.3499H2.00039C1.80709 3.3499 1.65039 3.1932 1.65039 2.9999ZM1.65039 4.9999C1.65039 4.8066 1.80709 4.6499 2.00039 4.6499H8.00039C8.19369 4.6499 8.35039 4.8066 8.35039 4.9999C8.35039 5.1932 8.19369 5.3499 8.00039 5.3499H2.00039C1.80709 5.3499 1.65039 5.1932 1.65039 4.9999ZM2.00039 6.6499C1.80709 6.6499 1.65039 6.8066 1.65039 6.9999C1.65039 7.1932 1.80709 7.3499 2.00039 7.3499H8.00039C8.19369 7.3499 8.35039 7.1932 8.35039 6.9999C8.35039 6.8066 8.19369 6.6499 8.00039 6.6499H2.00039Z"></path></svg>
		</button>

	</div></div>
		<nav aria-label="Main" class="ml-auto hidden lg:block"><ul class="flex items-center space-x-1.5 2xl:space-x-2"><li class="hover:text-indigo-700"><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400" href="/models"><svg class="mr-1.5 text-gray-400 group-hover:text-indigo-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg>
					Models</a>
			</li><li class="hover:text-red-700"><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400" href="/datasets"><svg class="mr-1.5 text-gray-400 group-hover:text-red-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg>
					Datasets</a>
			</li><li class="hover:text-blue-700"><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400" href="/spaces"><svg class="mr-1.5 text-gray-400 group-hover:text-blue-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 25 25"><path opacity=".5" d="M6.016 14.674v4.31h4.31v-4.31h-4.31ZM14.674 14.674v4.31h4.31v-4.31h-4.31ZM6.016 6.016v4.31h4.31v-4.31h-4.31Z" fill="currentColor"></path><path opacity=".75" fill-rule="evenodd" clip-rule="evenodd" d="M3 4.914C3 3.857 3.857 3 4.914 3h6.514c.884 0 1.628.6 1.848 1.414a5.171 5.171 0 0 1 7.31 7.31c.815.22 1.414.964 1.414 1.848v6.514A1.914 1.914 0 0 1 20.086 22H4.914A1.914 1.914 0 0 1 3 20.086V4.914Zm3.016 1.102v4.31h4.31v-4.31h-4.31Zm0 12.968v-4.31h4.31v4.31h-4.31Zm8.658 0v-4.31h4.31v4.31h-4.31Zm0-10.813a2.155 2.155 0 1 1 4.31 0 2.155 2.155 0 0 1-4.31 0Z" fill="currentColor"></path><path opacity=".25" d="M16.829 6.016a2.155 2.155 0 1 0 0 4.31 2.155 2.155 0 0 0 0-4.31Z" fill="currentColor"></path></svg>
					Spaces</a>
			</li><li class="hover:text-yellow-700 max-xl:hidden"><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400" href="/posts"><svg class="mr-1.5 text-gray-400 group-hover:text-yellow-500 !text-yellow-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 12 12" preserveAspectRatio="xMidYMid meet"><path fill="currentColor" fill-rule="evenodd" d="M3.73 2.4A4.25 4.25 0 1 1 6 10.26H2.17l-.13-.02a.43.43 0 0 1-.3-.43l.01-.06a.43.43 0 0 1 .12-.22l.84-.84A4.26 4.26 0 0 1 3.73 2.4Z" clip-rule="evenodd"></path></svg>
					Posts</a>
			</li><li class="hover:text-yellow-700"><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400" href="/docs"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="mr-1.5 text-gray-400 group-hover:text-yellow-500" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path opacity="0.5" d="M20.9022 5.10334L10.8012 10.8791L7.76318 9.11193C8.07741 8.56791 8.5256 8.11332 9.06512 7.7914L15.9336 3.73907C17.0868 3.08811 18.5002 3.26422 19.6534 3.91519L19.3859 3.73911C19.9253 4.06087 20.5879 4.56025 20.9022 5.10334Z" fill="currentColor"></path><path d="M10.7999 10.8792V28.5483C10.2136 28.5475 9.63494 28.4139 9.10745 28.1578C8.5429 27.8312 8.074 27.3621 7.74761 26.7975C7.42122 26.2327 7.24878 25.5923 7.24756 24.9402V10.9908C7.25062 10.3319 7.42358 9.68487 7.74973 9.1123L10.7999 10.8792Z" fill="currentColor" fill-opacity="0.75"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M21.3368 10.8499V6.918C21.3331 6.25959 21.16 5.61234 20.8346 5.03949L10.7971 10.8727L10.8046 10.874L21.3368 10.8499Z" fill="currentColor"></path><path opacity="0.5" d="M21.7937 10.8488L10.7825 10.8741V28.5486L21.7937 28.5234C23.3344 28.5234 24.5835 27.2743 24.5835 25.7335V13.6387C24.5835 12.0979 23.4365 11.1233 21.7937 10.8488Z" fill="currentColor"></path></svg>
					Docs</a>
			</li><li class="hover:text-green-700"><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400" href="/enterprise"><svg class="mr-1.5 text-gray-400 group-hover:text-green-500" xmlns="http://www.w3.org/2000/svg" fill="none" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 33 27"><path fill="currentColor" fill-rule="evenodd" d="M13.5.7a8.7 8.7 0 0 0-7.7 5.7L1 20.6c-1 3.1.9 5.7 4.1 5.7h15c3.3 0 6.8-2.6 7.8-5.7l4.6-14.2c1-3.1-.8-5.7-4-5.7h-15Zm1.1 5.7L9.8 20.3h9.8l1-3.1h-5.8l.8-2.5h4.8l1.1-3h-4.8l.8-2.3H23l1-3h-9.5Z" clip-rule="evenodd"></path></svg>
					Enterprise</a>
			</li>

		<li><a class="group flex items-center px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-400" href="/pricing">Pricing
			</a></li>

		<li><div class="relative group">
	<button class="px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-600 flex items-center " type="button">
		<svg class=" text-gray-500 w-5 group-hover:text-gray-400 dark:text-gray-300 dark:group-hover:text-gray-400" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 18" preserveAspectRatio="xMidYMid meet"><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 3.30221C14.4504 2.836 14.8284 2.45807 15.2946 2.45807H28.4933C28.9595 2.45807 29.3374 2.836 29.3374 3.30221C29.3374 3.76842 28.9595 4.14635 28.4933 4.14635H15.2946C14.8284 4.14635 14.4504 3.76842 14.4504 3.30221Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 9.00002C14.4504 8.53382 14.8284 8.15588 15.2946 8.15588H28.4933C28.9595 8.15588 29.3374 8.53382 29.3374 9.00002C29.3374 9.46623 28.9595 9.84417 28.4933 9.84417H15.2946C14.8284 9.84417 14.4504 9.46623 14.4504 9.00002Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 14.6978C14.4504 14.2316 14.8284 13.8537 15.2946 13.8537H28.4933C28.9595 13.8537 29.3374 14.2316 29.3374 14.6978C29.3374 15.164 28.9595 15.542 28.4933 15.542H15.2946C14.8284 15.542 14.4504 15.164 14.4504 14.6978Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M1.94549 6.87377C2.27514 6.54411 2.80962 6.54411 3.13928 6.87377L6.23458 9.96907L9.32988 6.87377C9.65954 6.54411 10.194 6.54411 10.5237 6.87377C10.8533 7.20343 10.8533 7.73791 10.5237 8.06756L6.23458 12.3567L1.94549 8.06756C1.61583 7.73791 1.61583 7.20343 1.94549 6.87377Z" fill="currentColor"></path></svg>
			
		</button>
	
	
	</div></li>
		<li><hr class="h-5 w-0.5 border-none bg-gray-100 dark:bg-gray-800"></li>
		<li><form action="/logout" method="POST" class="hidden"><input type="hidden" name="csrf" value="eyJkYXRhIjp7ImV4cGlyYXRpb24iOjE3MzMyNzA0NjU5MTMsInVzZXJJZCI6IjY1NTQwMWJmNDMyYWYxYjExMTU3NDU4NSJ9LCJzaWduYXR1cmUiOiI1NDRkZjAxNjMzMWM4ZmZjYTIyZjg2Y2ExZmIyZTFhYjgxZmRkYjdiZDhhNmNiOGI3MmM4NDhlOGViZjYxNzY5In0="></form>
<div class="relative ml-2 w-[1.38rem] h-[1.38rem] ">
	<button class="ml-auto rounded-full ring-2 group ring-indigo-400 focus:ring-blue-500 hover:ring-offset-1 focus:ring-offset-1 focus:outline-none outline-none dark:ring-offset-gray-950 " type="button">
		
		<div class="relative"><img alt="" class="h-[1.38rem] w-[1.38rem] overflow-hidden rounded-full" src="/avatars/60c9e476e92f24b4f1719e25c7563b73.svg" crossorigin="anonymous">
			</div>
	
		</button>
	
	
	</div></li></ul></nav></div></header></div>
	
	<div class="bg-gradient-to-b py-3 text-sm md:text-base from-yellow-50 to-yellow-100 dark:from-yellow-500 dark:to-yellow-600 dark:text-gray-950 "><div class="container"><form class="flex flex-col justify-between md:flex-row md:items-center" action="/organizations/suggestions/dismiss" method="POST"><input type="hidden" name="csrf" value="eyJkYXRhIjp7ImV4cGlyYXRpb24iOjE3MzMyNzA0NjU5MTMsInVzZXJJZCI6IjY1NTQwMWJmNDMyYWYxYjExMTU3NDU4NSJ9LCJzaWduYXR1cmUiOiI1NDRkZjAxNjMzMWM4ZmZjYTIyZjg2Y2ExZmIyZTFhYjgxZmRkYjdiZDhhNmNiOGI3MmM4NDhlOGViZjYxNzY5In0=">
				<div class="mb-2 md:mb-0">Hugging Face is way more fun with friends and colleagues! ðŸ¤—
					<a class="ml-2 underline" href="/organizations/suggestions">Join an organization </a></div>
				<button class="btn text-sm" type="submit">Dismiss this message</button></form></div></div>
	
	<div class="SVELTE_HYDRATER contents" data-target="SSOBanner" data-props="{&quot;organizations&quot;:[]}"></div>
	
	

	<main class="flex flex-1 flex-col"><div class="relative lg:flex" id="hf-doc-container"><div class="sticky top-0 z-20 self-start"><div class="SVELTE_HYDRATER contents" data-target="SideMenu" data-props="{&quot;chapters&quot;:[{&quot;title&quot;:&quot;Get started&quot;,&quot;isExpanded&quot;:true,&quot;sections&quot;:[{&quot;title&quot;:&quot;ðŸ¤— Transformers&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;index&quot;,&quot;url&quot;:&quot;/docs/transformers/index&quot;},{&quot;title&quot;:&quot;Quick tour&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;quicktour&quot;,&quot;url&quot;:&quot;/docs/transformers/quicktour&quot;},{&quot;title&quot;:&quot;Installation&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;installation&quot;,&quot;url&quot;:&quot;/docs/transformers/installation&quot;},{&quot;title&quot;:&quot;Adding a new model to `transformers`&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;add_new_model&quot;,&quot;url&quot;:&quot;/docs/transformers/add_new_model&quot;}]},{&quot;title&quot;:&quot;Tutorials&quot;,&quot;isExpanded&quot;:true,&quot;sections&quot;:[{&quot;title&quot;:&quot;Run inference with pipelines&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;pipeline_tutorial&quot;,&quot;url&quot;:&quot;/docs/transformers/pipeline_tutorial&quot;},{&quot;title&quot;:&quot;Write portable code with AutoClass&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;autoclass_tutorial&quot;,&quot;url&quot;:&quot;/docs/transformers/autoclass_tutorial&quot;},{&quot;title&quot;:&quot;Preprocess data&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;preprocessing&quot;,&quot;url&quot;:&quot;/docs/transformers/preprocessing&quot;},{&quot;title&quot;:&quot;Fine-tune a pretrained model&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;training&quot;,&quot;url&quot;:&quot;/docs/transformers/training&quot;},{&quot;title&quot;:&quot;Train with a script&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;run_scripts&quot;,&quot;url&quot;:&quot;/docs/transformers/run_scripts&quot;},{&quot;title&quot;:&quot;Set up distributed training with ðŸ¤— Accelerate&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;accelerate&quot;,&quot;url&quot;:&quot;/docs/transformers/accelerate&quot;},{&quot;title&quot;:&quot;Load and train adapters with ðŸ¤— PEFT&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;peft&quot;,&quot;url&quot;:&quot;/docs/transformers/peft&quot;},{&quot;title&quot;:&quot;Share your model&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;model_sharing&quot;,&quot;url&quot;:&quot;/docs/transformers/model_sharing&quot;},{&quot;title&quot;:&quot;Agents 101&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;agents&quot;,&quot;url&quot;:&quot;/docs/transformers/agents&quot;},{&quot;title&quot;:&quot;Agents, supercharged - Multi-agents, External tools, and more&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;agents_advanced&quot;,&quot;url&quot;:&quot;/docs/transformers/agents_advanced&quot;},{&quot;title&quot;:&quot;Generation with LLMs&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;llm_tutorial&quot;,&quot;url&quot;:&quot;/docs/transformers/llm_tutorial&quot;},{&quot;title&quot;:&quot;Chatting with Transformers&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;conversations&quot;,&quot;url&quot;:&quot;/docs/transformers/conversations&quot;}]},{&quot;title&quot;:&quot;Task Guides&quot;,&quot;isExpanded&quot;:true,&quot;sections&quot;:[{&quot;title&quot;:&quot;Natural Language Processing&quot;,&quot;isExpanded&quot;:false,&quot;sections&quot;:[{&quot;title&quot;:&quot;Text classification&quot;,&quot;id&quot;:&quot;tasks/sequence_classification&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/sequence_classification&quot;},{&quot;title&quot;:&quot;Token classification&quot;,&quot;id&quot;:&quot;tasks/token_classification&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/token_classification&quot;},{&quot;title&quot;:&quot;Question answering&quot;,&quot;id&quot;:&quot;tasks/question_answering&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/question_answering&quot;},{&quot;title&quot;:&quot;Causal language modeling&quot;,&quot;id&quot;:&quot;tasks/language_modeling&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/language_modeling&quot;},{&quot;title&quot;:&quot;Masked language modeling&quot;,&quot;id&quot;:&quot;tasks/masked_language_modeling&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/masked_language_modeling&quot;},{&quot;title&quot;:&quot;Translation&quot;,&quot;id&quot;:&quot;tasks/translation&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/translation&quot;},{&quot;title&quot;:&quot;Summarization&quot;,&quot;id&quot;:&quot;tasks/summarization&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/summarization&quot;},{&quot;title&quot;:&quot;Multiple choice&quot;,&quot;id&quot;:&quot;tasks/multiple_choice&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/multiple_choice&quot;}]},{&quot;title&quot;:&quot;Audio&quot;,&quot;isExpanded&quot;:false,&quot;sections&quot;:[{&quot;title&quot;:&quot;Audio classification&quot;,&quot;id&quot;:&quot;tasks/audio_classification&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/audio_classification&quot;},{&quot;title&quot;:&quot;Automatic speech recognition&quot;,&quot;id&quot;:&quot;tasks/asr&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/asr&quot;}]},{&quot;title&quot;:&quot;Computer Vision&quot;,&quot;isExpanded&quot;:false,&quot;sections&quot;:[{&quot;title&quot;:&quot;Image classification&quot;,&quot;id&quot;:&quot;tasks/image_classification&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/image_classification&quot;},{&quot;title&quot;:&quot;Image segmentation&quot;,&quot;id&quot;:&quot;tasks/semantic_segmentation&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/semantic_segmentation&quot;},{&quot;title&quot;:&quot;Video classification&quot;,&quot;id&quot;:&quot;tasks/video_classification&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/video_classification&quot;},{&quot;title&quot;:&quot;Object detection&quot;,&quot;id&quot;:&quot;tasks/object_detection&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/object_detection&quot;},{&quot;title&quot;:&quot;Zero-shot object detection&quot;,&quot;id&quot;:&quot;tasks/zero_shot_object_detection&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/zero_shot_object_detection&quot;},{&quot;title&quot;:&quot;Zero-shot image classification&quot;,&quot;id&quot;:&quot;tasks/zero_shot_image_classification&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/zero_shot_image_classification&quot;},{&quot;title&quot;:&quot;Depth estimation&quot;,&quot;id&quot;:&quot;tasks/monocular_depth_estimation&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/monocular_depth_estimation&quot;},{&quot;title&quot;:&quot;Image-to-Image&quot;,&quot;id&quot;:&quot;tasks/image_to_image&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/image_to_image&quot;},{&quot;title&quot;:&quot;Image Feature Extraction&quot;,&quot;id&quot;:&quot;tasks/image_feature_extraction&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/image_feature_extraction&quot;},{&quot;title&quot;:&quot;Mask Generation&quot;,&quot;id&quot;:&quot;tasks/mask_generation&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/mask_generation&quot;},{&quot;title&quot;:&quot;Keypoint Detection&quot;,&quot;id&quot;:&quot;tasks/keypoint_detection&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/keypoint_detection&quot;},{&quot;title&quot;:&quot;Knowledge Distillation for Computer Vision&quot;,&quot;id&quot;:&quot;tasks/knowledge_distillation_for_image_classification&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/knowledge_distillation_for_image_classification&quot;}]},{&quot;title&quot;:&quot;Multimodal&quot;,&quot;isExpanded&quot;:false,&quot;sections&quot;:[{&quot;title&quot;:&quot;Image captioning&quot;,&quot;id&quot;:&quot;tasks/image_captioning&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/image_captioning&quot;},{&quot;title&quot;:&quot;Document Question Answering&quot;,&quot;id&quot;:&quot;tasks/document_question_answering&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/document_question_answering&quot;},{&quot;title&quot;:&quot;Visual Question Answering&quot;,&quot;id&quot;:&quot;tasks/visual_question_answering&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/visual_question_answering&quot;},{&quot;title&quot;:&quot;Text to speech&quot;,&quot;id&quot;:&quot;tasks/text-to-speech&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/text-to-speech&quot;},{&quot;title&quot;:&quot;Image-text-to-text&quot;,&quot;id&quot;:&quot;tasks/image_text_to_text&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/image_text_to_text&quot;},{&quot;title&quot;:&quot;Video-text-to-text&quot;,&quot;id&quot;:&quot;tasks/video_text_to_text&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/video_text_to_text&quot;}]},{&quot;title&quot;:&quot;Generation&quot;,&quot;isExpanded&quot;:true,&quot;sections&quot;:[{&quot;title&quot;:&quot;Customize the generation strategy&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;generation_strategies&quot;,&quot;url&quot;:&quot;/docs/transformers/generation_strategies&quot;},{&quot;title&quot;:&quot;Best Practices for Generation with Cache&quot;,&quot;id&quot;:&quot;kv_cache&quot;,&quot;url&quot;:&quot;/docs/transformers/kv_cache&quot;}]},{&quot;title&quot;:&quot;Prompting&quot;,&quot;isExpanded&quot;:false,&quot;sections&quot;:[{&quot;title&quot;:&quot;Image tasks with IDEFICS&quot;,&quot;id&quot;:&quot;tasks/idefics&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/idefics&quot;},{&quot;title&quot;:&quot;LLM prompting guide&quot;,&quot;id&quot;:&quot;tasks/prompting&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/prompting&quot;}]}]},{&quot;title&quot;:&quot;Developer guides&quot;,&quot;isExpanded&quot;:true,&quot;sections&quot;:[{&quot;title&quot;:&quot;Use fast tokenizers from ðŸ¤— Tokenizers&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;fast_tokenizers&quot;,&quot;url&quot;:&quot;/docs/transformers/fast_tokenizers&quot;},{&quot;title&quot;:&quot;Run inference with multilingual models&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;multilingual&quot;,&quot;url&quot;:&quot;/docs/transformers/multilingual&quot;},{&quot;title&quot;:&quot;Use model-specific APIs&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;create_a_model&quot;,&quot;url&quot;:&quot;/docs/transformers/create_a_model&quot;},{&quot;title&quot;:&quot;Share a custom model&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;custom_models&quot;,&quot;url&quot;:&quot;/docs/transformers/custom_models&quot;},{&quot;title&quot;:&quot;Chat templates&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;chat_templating&quot;,&quot;url&quot;:&quot;/docs/transformers/chat_templating&quot;},{&quot;title&quot;:&quot;Trainer&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;trainer&quot;,&quot;url&quot;:&quot;/docs/transformers/trainer&quot;},{&quot;title&quot;:&quot;Run training on Amazon SageMaker&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;sagemaker&quot;,&quot;url&quot;:&quot;/docs/transformers/sagemaker&quot;},{&quot;title&quot;:&quot;Export to ONNX&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;serialization&quot;,&quot;url&quot;:&quot;/docs/transformers/serialization&quot;},{&quot;title&quot;:&quot;Export to TFLite&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;tflite&quot;,&quot;url&quot;:&quot;/docs/transformers/tflite&quot;},{&quot;title&quot;:&quot;Export to TorchScript&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;torchscript&quot;,&quot;url&quot;:&quot;/docs/transformers/torchscript&quot;},{&quot;title&quot;:&quot;Benchmarks&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;benchmarks&quot;,&quot;url&quot;:&quot;/docs/transformers/benchmarks&quot;},{&quot;title&quot;:&quot;Notebooks with examples&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;notebooks&quot;,&quot;url&quot;:&quot;/docs/transformers/notebooks&quot;},{&quot;title&quot;:&quot;Community resources&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;community&quot;,&quot;url&quot;:&quot;/docs/transformers/community&quot;},{&quot;title&quot;:&quot;Troubleshoot&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;troubleshooting&quot;,&quot;url&quot;:&quot;/docs/transformers/troubleshooting&quot;},{&quot;title&quot;:&quot;Interoperability with GGUF files&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;gguf&quot;,&quot;url&quot;:&quot;/docs/transformers/gguf&quot;},{&quot;title&quot;:&quot;Interoperability with TikToken files&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;tiktoken&quot;,&quot;url&quot;:&quot;/docs/transformers/tiktoken&quot;},{&quot;title&quot;:&quot;Modularity in `transformers`&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;modular_transformers&quot;,&quot;url&quot;:&quot;/docs/transformers/modular_transformers&quot;},{&quot;title&quot;:&quot;Model Hacking (overwriting a class to your usage)&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;how_to_hack_models&quot;,&quot;url&quot;:&quot;/docs/transformers/how_to_hack_models&quot;}]},{&quot;title&quot;:&quot;Quantization Methods&quot;,&quot;isExpanded&quot;:true,&quot;sections&quot;:[{&quot;title&quot;:&quot;Getting started&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;quantization/overview&quot;,&quot;url&quot;:&quot;/docs/transformers/quantization/overview&quot;},{&quot;title&quot;:&quot;bitsandbytes&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;quantization/bitsandbytes&quot;,&quot;url&quot;:&quot;/docs/transformers/quantization/bitsandbytes&quot;},{&quot;title&quot;:&quot;GPTQ&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;quantization/gptq&quot;,&quot;url&quot;:&quot;/docs/transformers/quantization/gptq&quot;},{&quot;title&quot;:&quot;AWQ&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;quantization/awq&quot;,&quot;url&quot;:&quot;/docs/transformers/quantization/awq&quot;},{&quot;title&quot;:&quot;AQLM&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;quantization/aqlm&quot;,&quot;url&quot;:&quot;/docs/transformers/quantization/aqlm&quot;},{&quot;title&quot;:&quot;Quanto&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;quantization/quanto&quot;,&quot;url&quot;:&quot;/docs/transformers/quantization/quanto&quot;},{&quot;title&quot;:&quot;EETQ&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;quantization/eetq&quot;,&quot;url&quot;:&quot;/docs/transformers/quantization/eetq&quot;},{&quot;title&quot;:&quot;HQQ&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;quantization/hqq&quot;,&quot;url&quot;:&quot;/docs/transformers/quantization/hqq&quot;},{&quot;title&quot;:&quot;FBGEMM_FP8&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;quantization/fbgemm_fp8&quot;,&quot;url&quot;:&quot;/docs/transformers/quantization/fbgemm_fp8&quot;},{&quot;title&quot;:&quot;Optimum&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;quantization/optimum&quot;,&quot;url&quot;:&quot;/docs/transformers/quantization/optimum&quot;},{&quot;title&quot;:&quot;TorchAO&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;quantization/torchao&quot;,&quot;url&quot;:&quot;/docs/transformers/quantization/torchao&quot;},{&quot;title&quot;:&quot;BitNet&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;quantization/bitnet&quot;,&quot;url&quot;:&quot;/docs/transformers/quantization/bitnet&quot;},{&quot;title&quot;:&quot;compressed-tensors&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;quantization/compressed_tensors&quot;,&quot;url&quot;:&quot;/docs/transformers/quantization/compressed_tensors&quot;},{&quot;title&quot;:&quot;Contribute new quantization method&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;quantization/contribute&quot;,&quot;url&quot;:&quot;/docs/transformers/quantization/contribute&quot;}]},{&quot;title&quot;:&quot;Performance and scalability&quot;,&quot;isExpanded&quot;:true,&quot;sections&quot;:[{&quot;title&quot;:&quot;Overview&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;performance&quot;,&quot;url&quot;:&quot;/docs/transformers/performance&quot;},{&quot;title&quot;:&quot;LLM inference optimization&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;llm_optims&quot;,&quot;url&quot;:&quot;/docs/transformers/llm_optims&quot;},{&quot;title&quot;:&quot;Efficient training techniques&quot;,&quot;isExpanded&quot;:true,&quot;sections&quot;:[{&quot;title&quot;:&quot;Methods and tools for efficient training on a single GPU&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;perf_train_gpu_one&quot;,&quot;url&quot;:&quot;/docs/transformers/perf_train_gpu_one&quot;},{&quot;title&quot;:&quot;Multiple GPUs and parallelism&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;perf_train_gpu_many&quot;,&quot;url&quot;:&quot;/docs/transformers/perf_train_gpu_many&quot;},{&quot;title&quot;:&quot;Fully Sharded Data Parallel&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;fsdp&quot;,&quot;url&quot;:&quot;/docs/transformers/fsdp&quot;},{&quot;title&quot;:&quot;DeepSpeed&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;deepspeed&quot;,&quot;url&quot;:&quot;/docs/transformers/deepspeed&quot;},{&quot;title&quot;:&quot;Efficient training on CPU&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;perf_train_cpu&quot;,&quot;url&quot;:&quot;/docs/transformers/perf_train_cpu&quot;},{&quot;title&quot;:&quot;Distributed CPU training&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;perf_train_cpu_many&quot;,&quot;url&quot;:&quot;/docs/transformers/perf_train_cpu_many&quot;},{&quot;title&quot;:&quot;Training on TPU with TensorFlow&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;perf_train_tpu_tf&quot;,&quot;url&quot;:&quot;/docs/transformers/perf_train_tpu_tf&quot;},{&quot;title&quot;:&quot;PyTorch training on Apple silicon&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;perf_train_special&quot;,&quot;url&quot;:&quot;/docs/transformers/perf_train_special&quot;},{&quot;title&quot;:&quot;Custom hardware for training&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;perf_hardware&quot;,&quot;url&quot;:&quot;/docs/transformers/perf_hardware&quot;},{&quot;title&quot;:&quot;Hyperparameter Search using Trainer API&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;hpo_train&quot;,&quot;url&quot;:&quot;/docs/transformers/hpo_train&quot;}]},{&quot;title&quot;:&quot;Optimizing inference&quot;,&quot;isExpanded&quot;:true,&quot;sections&quot;:[{&quot;title&quot;:&quot;CPU inference&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;perf_infer_cpu&quot;,&quot;url&quot;:&quot;/docs/transformers/perf_infer_cpu&quot;},{&quot;title&quot;:&quot;GPU inference&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;perf_infer_gpu_one&quot;,&quot;url&quot;:&quot;/docs/transformers/perf_infer_gpu_one&quot;}]},{&quot;title&quot;:&quot;Instantiate a big model&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;big_models&quot;,&quot;url&quot;:&quot;/docs/transformers/big_models&quot;},{&quot;title&quot;:&quot;Debugging&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;debugging&quot;,&quot;url&quot;:&quot;/docs/transformers/debugging&quot;},{&quot;title&quot;:&quot;XLA Integration for TensorFlow Models&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;tf_xla&quot;,&quot;url&quot;:&quot;/docs/transformers/tf_xla&quot;},{&quot;title&quot;:&quot;Optimize inference using `torch.compile()`&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;perf_torch_compile&quot;,&quot;url&quot;:&quot;/docs/transformers/perf_torch_compile&quot;}]},{&quot;title&quot;:&quot;Contribute&quot;,&quot;isExpanded&quot;:true,&quot;sections&quot;:[{&quot;title&quot;:&quot;How to contribute to ðŸ¤— Transformers?&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;contributing&quot;,&quot;url&quot;:&quot;/docs/transformers/contributing&quot;},{&quot;title&quot;:&quot;How to add a model to ðŸ¤— Transformers?&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;add_new_model&quot;,&quot;url&quot;:&quot;/docs/transformers/add_new_model&quot;},{&quot;title&quot;:&quot;How to add a pipeline to ðŸ¤— Transformers?&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;add_new_pipeline&quot;,&quot;url&quot;:&quot;/docs/transformers/add_new_pipeline&quot;},{&quot;title&quot;:&quot;Testing&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;testing&quot;,&quot;url&quot;:&quot;/docs/transformers/testing&quot;},{&quot;title&quot;:&quot;Checks on a Pull Request&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;pr_checks&quot;,&quot;url&quot;:&quot;/docs/transformers/pr_checks&quot;}]},{&quot;title&quot;:&quot;Conceptual guides&quot;,&quot;isExpanded&quot;:true,&quot;sections&quot;:[{&quot;title&quot;:&quot;Philosophy&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;philosophy&quot;,&quot;url&quot;:&quot;/docs/transformers/philosophy&quot;},{&quot;title&quot;:&quot;Glossary&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;glossary&quot;,&quot;url&quot;:&quot;/docs/transformers/glossary&quot;},{&quot;title&quot;:&quot;What ðŸ¤— Transformers can do&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;task_summary&quot;,&quot;url&quot;:&quot;/docs/transformers/task_summary&quot;},{&quot;title&quot;:&quot;How ðŸ¤— Transformers solve tasks&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;tasks_explained&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks_explained&quot;},{&quot;title&quot;:&quot;The Transformer model family&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;model_summary&quot;,&quot;url&quot;:&quot;/docs/transformers/model_summary&quot;},{&quot;title&quot;:&quot;Summary of the tokenizers&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;tokenizer_summary&quot;,&quot;url&quot;:&quot;/docs/transformers/tokenizer_summary&quot;},{&quot;title&quot;:&quot;Attention mechanisms&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;attention&quot;,&quot;url&quot;:&quot;/docs/transformers/attention&quot;},{&quot;title&quot;:&quot;Padding and truncation&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;pad_truncation&quot;,&quot;url&quot;:&quot;/docs/transformers/pad_truncation&quot;},{&quot;title&quot;:&quot;BERTology&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;bertology&quot;,&quot;url&quot;:&quot;/docs/transformers/bertology&quot;},{&quot;title&quot;:&quot;Perplexity of fixed-length models&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;perplexity&quot;,&quot;url&quot;:&quot;/docs/transformers/perplexity&quot;},{&quot;title&quot;:&quot;Pipelines for webserver inference&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;pipeline_webserver&quot;,&quot;url&quot;:&quot;/docs/transformers/pipeline_webserver&quot;},{&quot;title&quot;:&quot;Model training anatomy&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;model_memory_anatomy&quot;,&quot;url&quot;:&quot;/docs/transformers/model_memory_anatomy&quot;},{&quot;title&quot;:&quot;Getting the most out of LLMs&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;llm_tutorial_optimization&quot;,&quot;url&quot;:&quot;/docs/transformers/llm_tutorial_optimization&quot;}]},{&quot;title&quot;:&quot;API&quot;,&quot;isExpanded&quot;:true,&quot;sections&quot;:[{&quot;title&quot;:&quot;Main Classes&quot;,&quot;isExpanded&quot;:true,&quot;sections&quot;:[{&quot;title&quot;:&quot;Agents and Tools&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;main_classes/agent&quot;,&quot;url&quot;:&quot;/docs/transformers/main_classes/agent&quot;},{&quot;title&quot;:&quot;Auto Classes&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;model_doc/auto&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/auto&quot;},{&quot;title&quot;:&quot;Backbones&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;main_classes/backbones&quot;,&quot;url&quot;:&quot;/docs/transformers/main_classes/backbones&quot;},{&quot;title&quot;:&quot;Callbacks&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;main_classes/callback&quot;,&quot;url&quot;:&quot;/docs/transformers/main_classes/callback&quot;},{&quot;title&quot;:&quot;Configuration&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;main_classes/configuration&quot;,&quot;url&quot;:&quot;/docs/transformers/main_classes/configuration&quot;},{&quot;title&quot;:&quot;Data Collator&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;main_classes/data_collator&quot;,&quot;url&quot;:&quot;/docs/transformers/main_classes/data_collator&quot;},{&quot;title&quot;:&quot;Keras callbacks&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;main_classes/keras_callbacks&quot;,&quot;url&quot;:&quot;/docs/transformers/main_classes/keras_callbacks&quot;},{&quot;title&quot;:&quot;Logging&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;main_classes/logging&quot;,&quot;url&quot;:&quot;/docs/transformers/main_classes/logging&quot;},{&quot;title&quot;:&quot;Models&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;main_classes/model&quot;,&quot;url&quot;:&quot;/docs/transformers/main_classes/model&quot;},{&quot;title&quot;:&quot;Text Generation&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;main_classes/text_generation&quot;,&quot;url&quot;:&quot;/docs/transformers/main_classes/text_generation&quot;},{&quot;title&quot;:&quot;ONNX&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;main_classes/onnx&quot;,&quot;url&quot;:&quot;/docs/transformers/main_classes/onnx&quot;},{&quot;title&quot;:&quot;Optimization&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;main_classes/optimizer_schedules&quot;,&quot;url&quot;:&quot;/docs/transformers/main_classes/optimizer_schedules&quot;},{&quot;title&quot;:&quot;Model outputs&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;main_classes/output&quot;,&quot;url&quot;:&quot;/docs/transformers/main_classes/output&quot;},{&quot;title&quot;:&quot;Pipelines&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;main_classes/pipelines&quot;,&quot;url&quot;:&quot;/docs/transformers/main_classes/pipelines&quot;},{&quot;title&quot;:&quot;Processors&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;main_classes/processors&quot;,&quot;url&quot;:&quot;/docs/transformers/main_classes/processors&quot;},{&quot;title&quot;:&quot;Quantization&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;main_classes/quantization&quot;,&quot;url&quot;:&quot;/docs/transformers/main_classes/quantization&quot;},{&quot;title&quot;:&quot;Tokenizer&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;main_classes/tokenizer&quot;,&quot;url&quot;:&quot;/docs/transformers/main_classes/tokenizer&quot;},{&quot;title&quot;:&quot;Trainer&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;main_classes/trainer&quot;,&quot;url&quot;:&quot;/docs/transformers/main_classes/trainer&quot;},{&quot;title&quot;:&quot;DeepSpeed&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;main_classes/deepspeed&quot;,&quot;url&quot;:&quot;/docs/transformers/main_classes/deepspeed&quot;},{&quot;title&quot;:&quot;ExecuTorch&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;main_classes/executorch&quot;,&quot;url&quot;:&quot;/docs/transformers/main_classes/executorch&quot;},{&quot;title&quot;:&quot;Feature Extractor&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;main_classes/feature_extractor&quot;,&quot;url&quot;:&quot;/docs/transformers/main_classes/feature_extractor&quot;},{&quot;title&quot;:&quot;Image Processor&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;main_classes/image_processor&quot;,&quot;url&quot;:&quot;/docs/transformers/main_classes/image_processor&quot;}]},{&quot;title&quot;:&quot;Models&quot;,&quot;isExpanded&quot;:true,&quot;sections&quot;:[{&quot;title&quot;:&quot;Text models&quot;,&quot;isExpanded&quot;:false,&quot;sections&quot;:[{&quot;title&quot;:&quot;ALBERT&quot;,&quot;id&quot;:&quot;model_doc/albert&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/albert&quot;},{&quot;title&quot;:&quot;BART&quot;,&quot;id&quot;:&quot;model_doc/bart&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/bart&quot;},{&quot;title&quot;:&quot;BARThez&quot;,&quot;id&quot;:&quot;model_doc/barthez&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/barthez&quot;},{&quot;title&quot;:&quot;BARTpho&quot;,&quot;id&quot;:&quot;model_doc/bartpho&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/bartpho&quot;},{&quot;title&quot;:&quot;BERT&quot;,&quot;id&quot;:&quot;model_doc/bert&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/bert&quot;},{&quot;title&quot;:&quot;BertGeneration&quot;,&quot;id&quot;:&quot;model_doc/bert-generation&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/bert-generation&quot;},{&quot;title&quot;:&quot;BertJapanese&quot;,&quot;id&quot;:&quot;model_doc/bert-japanese&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/bert-japanese&quot;},{&quot;title&quot;:&quot;Bertweet&quot;,&quot;id&quot;:&quot;model_doc/bertweet&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/bertweet&quot;},{&quot;title&quot;:&quot;BigBird&quot;,&quot;id&quot;:&quot;model_doc/big_bird&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/big_bird&quot;},{&quot;title&quot;:&quot;BigBirdPegasus&quot;,&quot;id&quot;:&quot;model_doc/bigbird_pegasus&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/bigbird_pegasus&quot;},{&quot;title&quot;:&quot;BioGpt&quot;,&quot;id&quot;:&quot;model_doc/biogpt&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/biogpt&quot;},{&quot;title&quot;:&quot;Blenderbot&quot;,&quot;id&quot;:&quot;model_doc/blenderbot&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/blenderbot&quot;},{&quot;title&quot;:&quot;Blenderbot Small&quot;,&quot;id&quot;:&quot;model_doc/blenderbot-small&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/blenderbot-small&quot;},{&quot;title&quot;:&quot;BLOOM&quot;,&quot;id&quot;:&quot;model_doc/bloom&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/bloom&quot;},{&quot;title&quot;:&quot;BORT&quot;,&quot;id&quot;:&quot;model_doc/bort&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/bort&quot;},{&quot;title&quot;:&quot;ByT5&quot;,&quot;id&quot;:&quot;model_doc/byt5&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/byt5&quot;},{&quot;title&quot;:&quot;CamemBERT&quot;,&quot;id&quot;:&quot;model_doc/camembert&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/camembert&quot;},{&quot;title&quot;:&quot;CANINE&quot;,&quot;id&quot;:&quot;model_doc/canine&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/canine&quot;},{&quot;title&quot;:&quot;CodeGen&quot;,&quot;id&quot;:&quot;model_doc/codegen&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/codegen&quot;},{&quot;title&quot;:&quot;CodeLlama&quot;,&quot;id&quot;:&quot;model_doc/code_llama&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/code_llama&quot;},{&quot;title&quot;:&quot;Cohere&quot;,&quot;id&quot;:&quot;model_doc/cohere&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/cohere&quot;},{&quot;title&quot;:&quot;ConvBERT&quot;,&quot;id&quot;:&quot;model_doc/convbert&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/convbert&quot;},{&quot;title&quot;:&quot;CPM&quot;,&quot;id&quot;:&quot;model_doc/cpm&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/cpm&quot;},{&quot;title&quot;:&quot;CPMANT&quot;,&quot;id&quot;:&quot;model_doc/cpmant&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/cpmant&quot;},{&quot;title&quot;:&quot;CTRL&quot;,&quot;id&quot;:&quot;model_doc/ctrl&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/ctrl&quot;},{&quot;title&quot;:&quot;DBRX&quot;,&quot;id&quot;:&quot;model_doc/dbrx&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/dbrx&quot;},{&quot;title&quot;:&quot;DeBERTa&quot;,&quot;id&quot;:&quot;model_doc/deberta&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/deberta&quot;},{&quot;title&quot;:&quot;DeBERTa-v2&quot;,&quot;id&quot;:&quot;model_doc/deberta-v2&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/deberta-v2&quot;},{&quot;title&quot;:&quot;DialoGPT&quot;,&quot;id&quot;:&quot;model_doc/dialogpt&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/dialogpt&quot;},{&quot;title&quot;:&quot;DistilBERT&quot;,&quot;id&quot;:&quot;model_doc/distilbert&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/distilbert&quot;},{&quot;title&quot;:&quot;DPR&quot;,&quot;id&quot;:&quot;model_doc/dpr&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/dpr&quot;},{&quot;title&quot;:&quot;ELECTRA&quot;,&quot;id&quot;:&quot;model_doc/electra&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/electra&quot;},{&quot;title&quot;:&quot;Encoder Decoder Models&quot;,&quot;id&quot;:&quot;model_doc/encoder-decoder&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/encoder-decoder&quot;},{&quot;title&quot;:&quot;ERNIE&quot;,&quot;id&quot;:&quot;model_doc/ernie&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/ernie&quot;},{&quot;title&quot;:&quot;ErnieM&quot;,&quot;id&quot;:&quot;model_doc/ernie_m&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/ernie_m&quot;},{&quot;title&quot;:&quot;ESM&quot;,&quot;id&quot;:&quot;model_doc/esm&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/esm&quot;},{&quot;title&quot;:&quot;Falcon&quot;,&quot;id&quot;:&quot;model_doc/falcon&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/falcon&quot;},{&quot;title&quot;:&quot;FalconMamba&quot;,&quot;id&quot;:&quot;model_doc/falcon_mamba&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/falcon_mamba&quot;},{&quot;title&quot;:&quot;FastSpeech2Conformer&quot;,&quot;id&quot;:&quot;model_doc/fastspeech2_conformer&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/fastspeech2_conformer&quot;},{&quot;title&quot;:&quot;FLAN-T5&quot;,&quot;id&quot;:&quot;model_doc/flan-t5&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/flan-t5&quot;},{&quot;title&quot;:&quot;FLAN-UL2&quot;,&quot;id&quot;:&quot;model_doc/flan-ul2&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/flan-ul2&quot;},{&quot;title&quot;:&quot;FlauBERT&quot;,&quot;id&quot;:&quot;model_doc/flaubert&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/flaubert&quot;},{&quot;title&quot;:&quot;FNet&quot;,&quot;id&quot;:&quot;model_doc/fnet&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/fnet&quot;},{&quot;title&quot;:&quot;FSMT&quot;,&quot;id&quot;:&quot;model_doc/fsmt&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/fsmt&quot;},{&quot;title&quot;:&quot;Funnel Transformer&quot;,&quot;id&quot;:&quot;model_doc/funnel&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/funnel&quot;},{&quot;title&quot;:&quot;Fuyu&quot;,&quot;id&quot;:&quot;model_doc/fuyu&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/fuyu&quot;},{&quot;title&quot;:&quot;Gemma&quot;,&quot;id&quot;:&quot;model_doc/gemma&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/gemma&quot;},{&quot;title&quot;:&quot;Gemma2&quot;,&quot;id&quot;:&quot;model_doc/gemma2&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/gemma2&quot;},{&quot;title&quot;:&quot;GLM&quot;,&quot;id&quot;:&quot;model_doc/glm&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/glm&quot;},{&quot;title&quot;:&quot;GPT&quot;,&quot;id&quot;:&quot;model_doc/openai-gpt&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/openai-gpt&quot;},{&quot;title&quot;:&quot;GPT Neo&quot;,&quot;id&quot;:&quot;model_doc/gpt_neo&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/gpt_neo&quot;},{&quot;title&quot;:&quot;GPT NeoX&quot;,&quot;id&quot;:&quot;model_doc/gpt_neox&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/gpt_neox&quot;},{&quot;title&quot;:&quot;GPT NeoX Japanese&quot;,&quot;id&quot;:&quot;model_doc/gpt_neox_japanese&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/gpt_neox_japanese&quot;},{&quot;title&quot;:&quot;GPT-J&quot;,&quot;id&quot;:&quot;model_doc/gptj&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/gptj&quot;},{&quot;title&quot;:&quot;GPT2&quot;,&quot;id&quot;:&quot;model_doc/gpt2&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/gpt2&quot;},{&quot;title&quot;:&quot;GPTBigCode&quot;,&quot;id&quot;:&quot;model_doc/gpt_bigcode&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/gpt_bigcode&quot;},{&quot;title&quot;:&quot;GPTSAN Japanese&quot;,&quot;id&quot;:&quot;model_doc/gptsan-japanese&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/gptsan-japanese&quot;},{&quot;title&quot;:&quot;GPTSw3&quot;,&quot;id&quot;:&quot;model_doc/gpt-sw3&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/gpt-sw3&quot;},{&quot;title&quot;:&quot;Granite&quot;,&quot;id&quot;:&quot;model_doc/granite&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/granite&quot;},{&quot;title&quot;:&quot;GraniteMoe&quot;,&quot;id&quot;:&quot;model_doc/granitemoe&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/granitemoe&quot;},{&quot;title&quot;:&quot;HerBERT&quot;,&quot;id&quot;:&quot;model_doc/herbert&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/herbert&quot;},{&quot;title&quot;:&quot;I-BERT&quot;,&quot;id&quot;:&quot;model_doc/ibert&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/ibert&quot;},{&quot;title&quot;:&quot;Jamba&quot;,&quot;id&quot;:&quot;model_doc/jamba&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/jamba&quot;},{&quot;title&quot;:&quot;JetMoe&quot;,&quot;id&quot;:&quot;model_doc/jetmoe&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/jetmoe&quot;},{&quot;title&quot;:&quot;Jukebox&quot;,&quot;id&quot;:&quot;model_doc/jukebox&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/jukebox&quot;},{&quot;title&quot;:&quot;LED&quot;,&quot;id&quot;:&quot;model_doc/led&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/led&quot;},{&quot;title&quot;:&quot;LLaMA&quot;,&quot;id&quot;:&quot;model_doc/llama&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/llama&quot;},{&quot;title&quot;:&quot;Llama2&quot;,&quot;id&quot;:&quot;model_doc/llama2&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/llama2&quot;},{&quot;title&quot;:&quot;Llama3&quot;,&quot;id&quot;:&quot;model_doc/llama3&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/llama3&quot;},{&quot;title&quot;:&quot;Longformer&quot;,&quot;id&quot;:&quot;model_doc/longformer&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/longformer&quot;},{&quot;title&quot;:&quot;LongT5&quot;,&quot;id&quot;:&quot;model_doc/longt5&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/longt5&quot;},{&quot;title&quot;:&quot;LUKE&quot;,&quot;id&quot;:&quot;model_doc/luke&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/luke&quot;},{&quot;title&quot;:&quot;M2M100&quot;,&quot;id&quot;:&quot;model_doc/m2m_100&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/m2m_100&quot;},{&quot;title&quot;:&quot;MADLAD-400&quot;,&quot;id&quot;:&quot;model_doc/madlad-400&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/madlad-400&quot;},{&quot;title&quot;:&quot;Mamba&quot;,&quot;id&quot;:&quot;model_doc/mamba&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/mamba&quot;},{&quot;title&quot;:&quot;mamba2&quot;,&quot;id&quot;:&quot;model_doc/mamba2&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/mamba2&quot;},{&quot;title&quot;:&quot;MarianMT&quot;,&quot;id&quot;:&quot;model_doc/marian&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/marian&quot;},{&quot;title&quot;:&quot;MarkupLM&quot;,&quot;id&quot;:&quot;model_doc/markuplm&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/markuplm&quot;},{&quot;title&quot;:&quot;MBart and MBart-50&quot;,&quot;id&quot;:&quot;model_doc/mbart&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/mbart&quot;},{&quot;title&quot;:&quot;MEGA&quot;,&quot;id&quot;:&quot;model_doc/mega&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/mega&quot;},{&quot;title&quot;:&quot;MegatronBERT&quot;,&quot;id&quot;:&quot;model_doc/megatron-bert&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/megatron-bert&quot;},{&quot;title&quot;:&quot;MegatronGPT2&quot;,&quot;id&quot;:&quot;model_doc/megatron_gpt2&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/megatron_gpt2&quot;},{&quot;title&quot;:&quot;Mistral&quot;,&quot;id&quot;:&quot;model_doc/mistral&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/mistral&quot;},{&quot;title&quot;:&quot;Mixtral&quot;,&quot;id&quot;:&quot;model_doc/mixtral&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/mixtral&quot;},{&quot;title&quot;:&quot;mLUKE&quot;,&quot;id&quot;:&quot;model_doc/mluke&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/mluke&quot;},{&quot;title&quot;:&quot;MobileBERT&quot;,&quot;id&quot;:&quot;model_doc/mobilebert&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/mobilebert&quot;},{&quot;title&quot;:&quot;MPNet&quot;,&quot;id&quot;:&quot;model_doc/mpnet&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/mpnet&quot;},{&quot;title&quot;:&quot;MPT&quot;,&quot;id&quot;:&quot;model_doc/mpt&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/mpt&quot;},{&quot;title&quot;:&quot;MRA&quot;,&quot;id&quot;:&quot;model_doc/mra&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/mra&quot;},{&quot;title&quot;:&quot;MT5&quot;,&quot;id&quot;:&quot;model_doc/mt5&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/mt5&quot;},{&quot;title&quot;:&quot;MVP&quot;,&quot;id&quot;:&quot;model_doc/mvp&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/mvp&quot;},{&quot;title&quot;:&quot;myt5&quot;,&quot;id&quot;:&quot;model_doc/myt5&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/myt5&quot;},{&quot;title&quot;:&quot;Nemotron&quot;,&quot;id&quot;:&quot;model_doc/nemotron&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/nemotron&quot;},{&quot;title&quot;:&quot;NEZHA&quot;,&quot;id&quot;:&quot;model_doc/nezha&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/nezha&quot;},{&quot;title&quot;:&quot;NLLB&quot;,&quot;id&quot;:&quot;model_doc/nllb&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/nllb&quot;},{&quot;title&quot;:&quot;NLLB-MoE&quot;,&quot;id&quot;:&quot;model_doc/nllb-moe&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/nllb-moe&quot;},{&quot;title&quot;:&quot;NystrÃ¶mformer&quot;,&quot;id&quot;:&quot;model_doc/nystromformer&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/nystromformer&quot;},{&quot;title&quot;:&quot;OLMo&quot;,&quot;id&quot;:&quot;model_doc/olmo&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/olmo&quot;},{&quot;title&quot;:&quot;OLMoE&quot;,&quot;id&quot;:&quot;model_doc/olmoe&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/olmoe&quot;},{&quot;title&quot;:&quot;Open-Llama&quot;,&quot;id&quot;:&quot;model_doc/open-llama&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/open-llama&quot;},{&quot;title&quot;:&quot;OPT&quot;,&quot;id&quot;:&quot;model_doc/opt&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/opt&quot;},{&quot;title&quot;:&quot;Pegasus&quot;,&quot;id&quot;:&quot;model_doc/pegasus&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/pegasus&quot;},{&quot;title&quot;:&quot;PEGASUS-X&quot;,&quot;id&quot;:&quot;model_doc/pegasus_x&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/pegasus_x&quot;},{&quot;title&quot;:&quot;Persimmon&quot;,&quot;id&quot;:&quot;model_doc/persimmon&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/persimmon&quot;},{&quot;title&quot;:&quot;Phi&quot;,&quot;id&quot;:&quot;model_doc/phi&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/phi&quot;},{&quot;title&quot;:&quot;Phi-3&quot;,&quot;id&quot;:&quot;model_doc/phi3&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/phi3&quot;},{&quot;title&quot;:&quot;PhiMoE&quot;,&quot;id&quot;:&quot;model_doc/phimoe&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/phimoe&quot;},{&quot;title&quot;:&quot;PhoBERT&quot;,&quot;id&quot;:&quot;model_doc/phobert&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/phobert&quot;},{&quot;title&quot;:&quot;PLBart&quot;,&quot;id&quot;:&quot;model_doc/plbart&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/plbart&quot;},{&quot;title&quot;:&quot;ProphetNet&quot;,&quot;id&quot;:&quot;model_doc/prophetnet&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/prophetnet&quot;},{&quot;title&quot;:&quot;QDQBert&quot;,&quot;id&quot;:&quot;model_doc/qdqbert&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/qdqbert&quot;},{&quot;title&quot;:&quot;Qwen2&quot;,&quot;id&quot;:&quot;model_doc/qwen2&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/qwen2&quot;},{&quot;title&quot;:&quot;Qwen2MoE&quot;,&quot;id&quot;:&quot;model_doc/qwen2_moe&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/qwen2_moe&quot;},{&quot;title&quot;:&quot;RAG&quot;,&quot;id&quot;:&quot;model_doc/rag&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/rag&quot;},{&quot;title&quot;:&quot;REALM&quot;,&quot;id&quot;:&quot;model_doc/realm&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/realm&quot;},{&quot;title&quot;:&quot;RecurrentGemma&quot;,&quot;id&quot;:&quot;model_doc/recurrent_gemma&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/recurrent_gemma&quot;},{&quot;title&quot;:&quot;Reformer&quot;,&quot;id&quot;:&quot;model_doc/reformer&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/reformer&quot;},{&quot;title&quot;:&quot;RemBERT&quot;,&quot;id&quot;:&quot;model_doc/rembert&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/rembert&quot;},{&quot;title&quot;:&quot;RetriBERT&quot;,&quot;id&quot;:&quot;model_doc/retribert&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/retribert&quot;},{&quot;title&quot;:&quot;RoBERTa&quot;,&quot;id&quot;:&quot;model_doc/roberta&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/roberta&quot;},{&quot;title&quot;:&quot;RoBERTa-PreLayerNorm&quot;,&quot;id&quot;:&quot;model_doc/roberta-prelayernorm&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/roberta-prelayernorm&quot;},{&quot;title&quot;:&quot;RoCBert&quot;,&quot;id&quot;:&quot;model_doc/roc_bert&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/roc_bert&quot;},{&quot;title&quot;:&quot;RoFormer&quot;,&quot;id&quot;:&quot;model_doc/roformer&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/roformer&quot;},{&quot;title&quot;:&quot;RWKV&quot;,&quot;id&quot;:&quot;model_doc/rwkv&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/rwkv&quot;},{&quot;title&quot;:&quot;Splinter&quot;,&quot;id&quot;:&quot;model_doc/splinter&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/splinter&quot;},{&quot;title&quot;:&quot;SqueezeBERT&quot;,&quot;id&quot;:&quot;model_doc/squeezebert&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/squeezebert&quot;},{&quot;title&quot;:&quot;StableLm&quot;,&quot;id&quot;:&quot;model_doc/stablelm&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/stablelm&quot;},{&quot;title&quot;:&quot;Starcoder2&quot;,&quot;id&quot;:&quot;model_doc/starcoder2&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/starcoder2&quot;},{&quot;title&quot;:&quot;SwitchTransformers&quot;,&quot;id&quot;:&quot;model_doc/switch_transformers&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/switch_transformers&quot;},{&quot;title&quot;:&quot;T5&quot;,&quot;id&quot;:&quot;model_doc/t5&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/t5&quot;},{&quot;title&quot;:&quot;T5v1.1&quot;,&quot;id&quot;:&quot;model_doc/t5v1.1&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/t5v1.1&quot;},{&quot;title&quot;:&quot;TAPEX&quot;,&quot;id&quot;:&quot;model_doc/tapex&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/tapex&quot;},{&quot;title&quot;:&quot;Transformer XL&quot;,&quot;id&quot;:&quot;model_doc/transfo-xl&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/transfo-xl&quot;},{&quot;title&quot;:&quot;UL2&quot;,&quot;id&quot;:&quot;model_doc/ul2&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/ul2&quot;},{&quot;title&quot;:&quot;UMT5&quot;,&quot;id&quot;:&quot;model_doc/umt5&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/umt5&quot;},{&quot;title&quot;:&quot;X-MOD&quot;,&quot;id&quot;:&quot;model_doc/xmod&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/xmod&quot;},{&quot;title&quot;:&quot;XGLM&quot;,&quot;id&quot;:&quot;model_doc/xglm&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/xglm&quot;},{&quot;title&quot;:&quot;XLM&quot;,&quot;id&quot;:&quot;model_doc/xlm&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/xlm&quot;},{&quot;title&quot;:&quot;XLM-ProphetNet&quot;,&quot;id&quot;:&quot;model_doc/xlm-prophetnet&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/xlm-prophetnet&quot;},{&quot;title&quot;:&quot;XLM-RoBERTa&quot;,&quot;id&quot;:&quot;model_doc/xlm-roberta&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/xlm-roberta&quot;},{&quot;title&quot;:&quot;XLM-RoBERTa-XL&quot;,&quot;id&quot;:&quot;model_doc/xlm-roberta-xl&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/xlm-roberta-xl&quot;},{&quot;title&quot;:&quot;XLM-V&quot;,&quot;id&quot;:&quot;model_doc/xlm-v&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/xlm-v&quot;},{&quot;title&quot;:&quot;XLNet&quot;,&quot;id&quot;:&quot;model_doc/xlnet&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/xlnet&quot;},{&quot;title&quot;:&quot;YOSO&quot;,&quot;id&quot;:&quot;model_doc/yoso&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/yoso&quot;}]},{&quot;title&quot;:&quot;Vision models&quot;,&quot;isExpanded&quot;:false,&quot;sections&quot;:[{&quot;title&quot;:&quot;BEiT&quot;,&quot;id&quot;:&quot;model_doc/beit&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/beit&quot;},{&quot;title&quot;:&quot;BiT&quot;,&quot;id&quot;:&quot;model_doc/bit&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/bit&quot;},{&quot;title&quot;:&quot;Conditional DETR&quot;,&quot;id&quot;:&quot;model_doc/conditional_detr&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/conditional_detr&quot;},{&quot;title&quot;:&quot;ConvNeXT&quot;,&quot;id&quot;:&quot;model_doc/convnext&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/convnext&quot;},{&quot;title&quot;:&quot;ConvNeXTV2&quot;,&quot;id&quot;:&quot;model_doc/convnextv2&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/convnextv2&quot;},{&quot;title&quot;:&quot;CvT&quot;,&quot;id&quot;:&quot;model_doc/cvt&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/cvt&quot;},{&quot;title&quot;:&quot;Deformable DETR&quot;,&quot;id&quot;:&quot;model_doc/deformable_detr&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/deformable_detr&quot;},{&quot;title&quot;:&quot;DeiT&quot;,&quot;id&quot;:&quot;model_doc/deit&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/deit&quot;},{&quot;title&quot;:&quot;Depth Anything&quot;,&quot;id&quot;:&quot;model_doc/depth_anything&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/depth_anything&quot;},{&quot;title&quot;:&quot;Depth Anything V2&quot;,&quot;id&quot;:&quot;model_doc/depth_anything_v2&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/depth_anything_v2&quot;},{&quot;title&quot;:&quot;DETA&quot;,&quot;id&quot;:&quot;model_doc/deta&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/deta&quot;},{&quot;title&quot;:&quot;DETR&quot;,&quot;id&quot;:&quot;model_doc/detr&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/detr&quot;},{&quot;title&quot;:&quot;DiNAT&quot;,&quot;id&quot;:&quot;model_doc/dinat&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/dinat&quot;},{&quot;title&quot;:&quot;DINOV2&quot;,&quot;id&quot;:&quot;model_doc/dinov2&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/dinov2&quot;},{&quot;title&quot;:&quot;DiT&quot;,&quot;id&quot;:&quot;model_doc/dit&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/dit&quot;},{&quot;title&quot;:&quot;DPT&quot;,&quot;id&quot;:&quot;model_doc/dpt&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/dpt&quot;},{&quot;title&quot;:&quot;EfficientFormer&quot;,&quot;id&quot;:&quot;model_doc/efficientformer&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/efficientformer&quot;},{&quot;title&quot;:&quot;EfficientNet&quot;,&quot;id&quot;:&quot;model_doc/efficientnet&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/efficientnet&quot;},{&quot;title&quot;:&quot;FocalNet&quot;,&quot;id&quot;:&quot;model_doc/focalnet&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/focalnet&quot;},{&quot;title&quot;:&quot;GLPN&quot;,&quot;id&quot;:&quot;model_doc/glpn&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/glpn&quot;},{&quot;title&quot;:&quot;Hiera&quot;,&quot;id&quot;:&quot;model_doc/hiera&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/hiera&quot;},{&quot;title&quot;:&quot;ImageGPT&quot;,&quot;id&quot;:&quot;model_doc/imagegpt&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/imagegpt&quot;},{&quot;title&quot;:&quot;LeViT&quot;,&quot;id&quot;:&quot;model_doc/levit&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/levit&quot;},{&quot;title&quot;:&quot;Mask2Former&quot;,&quot;id&quot;:&quot;model_doc/mask2former&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/mask2former&quot;},{&quot;title&quot;:&quot;MaskFormer&quot;,&quot;id&quot;:&quot;model_doc/maskformer&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/maskformer&quot;},{&quot;title&quot;:&quot;MobileNetV1&quot;,&quot;id&quot;:&quot;model_doc/mobilenet_v1&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/mobilenet_v1&quot;},{&quot;title&quot;:&quot;MobileNetV2&quot;,&quot;id&quot;:&quot;model_doc/mobilenet_v2&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/mobilenet_v2&quot;},{&quot;title&quot;:&quot;MobileViT&quot;,&quot;id&quot;:&quot;model_doc/mobilevit&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/mobilevit&quot;},{&quot;title&quot;:&quot;MobileViTV2&quot;,&quot;id&quot;:&quot;model_doc/mobilevitv2&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/mobilevitv2&quot;},{&quot;title&quot;:&quot;NAT&quot;,&quot;id&quot;:&quot;model_doc/nat&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/nat&quot;},{&quot;title&quot;:&quot;PoolFormer&quot;,&quot;id&quot;:&quot;model_doc/poolformer&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/poolformer&quot;},{&quot;title&quot;:&quot;Pyramid Vision Transformer (PVT)&quot;,&quot;id&quot;:&quot;model_doc/pvt&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/pvt&quot;},{&quot;title&quot;:&quot;Pyramid Vision Transformer v2 (PVTv2)&quot;,&quot;id&quot;:&quot;model_doc/pvt_v2&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/pvt_v2&quot;},{&quot;title&quot;:&quot;RegNet&quot;,&quot;id&quot;:&quot;model_doc/regnet&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/regnet&quot;},{&quot;title&quot;:&quot;ResNet&quot;,&quot;id&quot;:&quot;model_doc/resnet&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/resnet&quot;},{&quot;title&quot;:&quot;RT-DETR&quot;,&quot;id&quot;:&quot;model_doc/rt_detr&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/rt_detr&quot;},{&quot;title&quot;:&quot;SegFormer&quot;,&quot;id&quot;:&quot;model_doc/segformer&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/segformer&quot;},{&quot;title&quot;:&quot;SegGpt&quot;,&quot;id&quot;:&quot;model_doc/seggpt&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/seggpt&quot;},{&quot;title&quot;:&quot;SuperPoint&quot;,&quot;id&quot;:&quot;model_doc/superpoint&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/superpoint&quot;},{&quot;title&quot;:&quot;SwiftFormer&quot;,&quot;id&quot;:&quot;model_doc/swiftformer&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/swiftformer&quot;},{&quot;title&quot;:&quot;Swin Transformer&quot;,&quot;id&quot;:&quot;model_doc/swin&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/swin&quot;},{&quot;title&quot;:&quot;Swin Transformer V2&quot;,&quot;id&quot;:&quot;model_doc/swinv2&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/swinv2&quot;},{&quot;title&quot;:&quot;Swin2SR&quot;,&quot;id&quot;:&quot;model_doc/swin2sr&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/swin2sr&quot;},{&quot;title&quot;:&quot;Table Transformer&quot;,&quot;id&quot;:&quot;model_doc/table-transformer&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/table-transformer&quot;},{&quot;title&quot;:&quot;UperNet&quot;,&quot;id&quot;:&quot;model_doc/upernet&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/upernet&quot;},{&quot;title&quot;:&quot;VAN&quot;,&quot;id&quot;:&quot;model_doc/van&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/van&quot;},{&quot;title&quot;:&quot;Vision Transformer (ViT)&quot;,&quot;id&quot;:&quot;model_doc/vit&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/vit&quot;},{&quot;title&quot;:&quot;ViT Hybrid&quot;,&quot;id&quot;:&quot;model_doc/vit_hybrid&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/vit_hybrid&quot;},{&quot;title&quot;:&quot;ViTDet&quot;,&quot;id&quot;:&quot;model_doc/vitdet&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/vitdet&quot;},{&quot;title&quot;:&quot;ViTMAE&quot;,&quot;id&quot;:&quot;model_doc/vit_mae&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/vit_mae&quot;},{&quot;title&quot;:&quot;ViTMatte&quot;,&quot;id&quot;:&quot;model_doc/vitmatte&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/vitmatte&quot;},{&quot;title&quot;:&quot;ViTMSN&quot;,&quot;id&quot;:&quot;model_doc/vit_msn&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/vit_msn&quot;},{&quot;title&quot;:&quot;YOLOS&quot;,&quot;id&quot;:&quot;model_doc/yolos&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/yolos&quot;},{&quot;title&quot;:&quot;Zamba&quot;,&quot;id&quot;:&quot;model_doc/zamba&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/zamba&quot;},{&quot;title&quot;:&quot;ZoeDepth&quot;,&quot;id&quot;:&quot;model_doc/zoedepth&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/zoedepth&quot;}]},{&quot;title&quot;:&quot;Audio models&quot;,&quot;isExpanded&quot;:false,&quot;sections&quot;:[{&quot;title&quot;:&quot;Audio Spectrogram Transformer&quot;,&quot;id&quot;:&quot;model_doc/audio-spectrogram-transformer&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/audio-spectrogram-transformer&quot;},{&quot;title&quot;:&quot;Bark&quot;,&quot;id&quot;:&quot;model_doc/bark&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/bark&quot;},{&quot;title&quot;:&quot;CLAP&quot;,&quot;id&quot;:&quot;model_doc/clap&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/clap&quot;},{&quot;title&quot;:&quot;dac&quot;,&quot;id&quot;:&quot;model_doc/dac&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/dac&quot;},{&quot;title&quot;:&quot;EnCodec&quot;,&quot;id&quot;:&quot;model_doc/encodec&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/encodec&quot;},{&quot;title&quot;:&quot;Hiera&quot;,&quot;id&quot;:&quot;model_doc/hiera&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/hiera&quot;},{&quot;title&quot;:&quot;Hubert&quot;,&quot;id&quot;:&quot;model_doc/hubert&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/hubert&quot;},{&quot;title&quot;:&quot;MCTCT&quot;,&quot;id&quot;:&quot;model_doc/mctct&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/mctct&quot;},{&quot;title&quot;:&quot;Mimi&quot;,&quot;id&quot;:&quot;model_doc/mimi&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/mimi&quot;},{&quot;title&quot;:&quot;MMS&quot;,&quot;id&quot;:&quot;model_doc/mms&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/mms&quot;},{&quot;title&quot;:&quot;Moshi&quot;,&quot;id&quot;:&quot;model_doc/moshi&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/moshi&quot;},{&quot;title&quot;:&quot;MusicGen&quot;,&quot;id&quot;:&quot;model_doc/musicgen&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/musicgen&quot;},{&quot;title&quot;:&quot;MusicGen Melody&quot;,&quot;id&quot;:&quot;model_doc/musicgen_melody&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/musicgen_melody&quot;},{&quot;title&quot;:&quot;Pop2Piano&quot;,&quot;id&quot;:&quot;model_doc/pop2piano&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/pop2piano&quot;},{&quot;title&quot;:&quot;Seamless-M4T&quot;,&quot;id&quot;:&quot;model_doc/seamless_m4t&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/seamless_m4t&quot;},{&quot;title&quot;:&quot;SeamlessM4T-v2&quot;,&quot;id&quot;:&quot;model_doc/seamless_m4t_v2&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/seamless_m4t_v2&quot;},{&quot;title&quot;:&quot;SEW&quot;,&quot;id&quot;:&quot;model_doc/sew&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/sew&quot;},{&quot;title&quot;:&quot;SEW-D&quot;,&quot;id&quot;:&quot;model_doc/sew-d&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/sew-d&quot;},{&quot;title&quot;:&quot;Speech2Text&quot;,&quot;id&quot;:&quot;model_doc/speech_to_text&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/speech_to_text&quot;},{&quot;title&quot;:&quot;Speech2Text2&quot;,&quot;id&quot;:&quot;model_doc/speech_to_text_2&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/speech_to_text_2&quot;},{&quot;title&quot;:&quot;SpeechT5&quot;,&quot;id&quot;:&quot;model_doc/speecht5&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/speecht5&quot;},{&quot;title&quot;:&quot;UniSpeech&quot;,&quot;id&quot;:&quot;model_doc/unispeech&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/unispeech&quot;},{&quot;title&quot;:&quot;UniSpeech-SAT&quot;,&quot;id&quot;:&quot;model_doc/unispeech-sat&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/unispeech-sat&quot;},{&quot;title&quot;:&quot;UnivNet&quot;,&quot;id&quot;:&quot;model_doc/univnet&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/univnet&quot;},{&quot;title&quot;:&quot;VITS&quot;,&quot;id&quot;:&quot;model_doc/vits&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/vits&quot;},{&quot;title&quot;:&quot;Wav2Vec2&quot;,&quot;id&quot;:&quot;model_doc/wav2vec2&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/wav2vec2&quot;},{&quot;title&quot;:&quot;Wav2Vec2-BERT&quot;,&quot;id&quot;:&quot;model_doc/wav2vec2-bert&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/wav2vec2-bert&quot;},{&quot;title&quot;:&quot;Wav2Vec2-Conformer&quot;,&quot;id&quot;:&quot;model_doc/wav2vec2-conformer&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/wav2vec2-conformer&quot;},{&quot;title&quot;:&quot;Wav2Vec2Phoneme&quot;,&quot;id&quot;:&quot;model_doc/wav2vec2_phoneme&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/wav2vec2_phoneme&quot;},{&quot;title&quot;:&quot;WavLM&quot;,&quot;id&quot;:&quot;model_doc/wavlm&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/wavlm&quot;},{&quot;title&quot;:&quot;Whisper&quot;,&quot;id&quot;:&quot;model_doc/whisper&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/whisper&quot;},{&quot;title&quot;:&quot;XLS-R&quot;,&quot;id&quot;:&quot;model_doc/xls_r&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/xls_r&quot;},{&quot;title&quot;:&quot;XLSR-Wav2Vec2&quot;,&quot;id&quot;:&quot;model_doc/xlsr_wav2vec2&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/xlsr_wav2vec2&quot;}]},{&quot;title&quot;:&quot;Video models&quot;,&quot;isExpanded&quot;:false,&quot;sections&quot;:[{&quot;title&quot;:&quot;TimeSformer&quot;,&quot;id&quot;:&quot;model_doc/timesformer&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/timesformer&quot;},{&quot;title&quot;:&quot;VideoMAE&quot;,&quot;id&quot;:&quot;model_doc/videomae&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/videomae&quot;},{&quot;title&quot;:&quot;ViViT&quot;,&quot;id&quot;:&quot;model_doc/vivit&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/vivit&quot;}]},{&quot;title&quot;:&quot;Multimodal models&quot;,&quot;isExpanded&quot;:false,&quot;sections&quot;:[{&quot;title&quot;:&quot;ALIGN&quot;,&quot;id&quot;:&quot;model_doc/align&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/align&quot;},{&quot;title&quot;:&quot;AltCLIP&quot;,&quot;id&quot;:&quot;model_doc/altclip&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/altclip&quot;},{&quot;title&quot;:&quot;BLIP&quot;,&quot;id&quot;:&quot;model_doc/blip&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/blip&quot;},{&quot;title&quot;:&quot;BLIP-2&quot;,&quot;id&quot;:&quot;model_doc/blip-2&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/blip-2&quot;},{&quot;title&quot;:&quot;BridgeTower&quot;,&quot;id&quot;:&quot;model_doc/bridgetower&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/bridgetower&quot;},{&quot;title&quot;:&quot;BROS&quot;,&quot;id&quot;:&quot;model_doc/bros&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/bros&quot;},{&quot;title&quot;:&quot;Chameleon&quot;,&quot;id&quot;:&quot;model_doc/chameleon&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/chameleon&quot;},{&quot;title&quot;:&quot;Chinese-CLIP&quot;,&quot;id&quot;:&quot;model_doc/chinese_clip&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/chinese_clip&quot;},{&quot;title&quot;:&quot;CLIP&quot;,&quot;id&quot;:&quot;model_doc/clip&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/clip&quot;},{&quot;title&quot;:&quot;CLIPSeg&quot;,&quot;id&quot;:&quot;model_doc/clipseg&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/clipseg&quot;},{&quot;title&quot;:&quot;CLVP&quot;,&quot;id&quot;:&quot;model_doc/clvp&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/clvp&quot;},{&quot;title&quot;:&quot;Data2Vec&quot;,&quot;id&quot;:&quot;model_doc/data2vec&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/data2vec&quot;},{&quot;title&quot;:&quot;DePlot&quot;,&quot;id&quot;:&quot;model_doc/deplot&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/deplot&quot;},{&quot;title&quot;:&quot;Donut&quot;,&quot;id&quot;:&quot;model_doc/donut&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/donut&quot;},{&quot;title&quot;:&quot;FLAVA&quot;,&quot;id&quot;:&quot;model_doc/flava&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/flava&quot;},{&quot;title&quot;:&quot;GIT&quot;,&quot;id&quot;:&quot;model_doc/git&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/git&quot;},{&quot;title&quot;:&quot;Grounding DINO&quot;,&quot;id&quot;:&quot;model_doc/grounding-dino&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/grounding-dino&quot;},{&quot;title&quot;:&quot;GroupViT&quot;,&quot;id&quot;:&quot;model_doc/groupvit&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/groupvit&quot;},{&quot;title&quot;:&quot;IDEFICS&quot;,&quot;id&quot;:&quot;model_doc/idefics&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/idefics&quot;},{&quot;title&quot;:&quot;Idefics2&quot;,&quot;id&quot;:&quot;model_doc/idefics2&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/idefics2&quot;},{&quot;title&quot;:&quot;Idefics3&quot;,&quot;id&quot;:&quot;model_doc/idefics3&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/idefics3&quot;},{&quot;title&quot;:&quot;InstructBLIP&quot;,&quot;id&quot;:&quot;model_doc/instructblip&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/instructblip&quot;},{&quot;title&quot;:&quot;InstructBlipVideo&quot;,&quot;id&quot;:&quot;model_doc/instructblipvideo&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/instructblipvideo&quot;},{&quot;title&quot;:&quot;KOSMOS-2&quot;,&quot;id&quot;:&quot;model_doc/kosmos-2&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/kosmos-2&quot;},{&quot;title&quot;:&quot;LayoutLM&quot;,&quot;id&quot;:&quot;model_doc/layoutlm&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/layoutlm&quot;},{&quot;title&quot;:&quot;LayoutLMV2&quot;,&quot;id&quot;:&quot;model_doc/layoutlmv2&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/layoutlmv2&quot;},{&quot;title&quot;:&quot;LayoutLMV3&quot;,&quot;id&quot;:&quot;model_doc/layoutlmv3&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/layoutlmv3&quot;},{&quot;title&quot;:&quot;LayoutXLM&quot;,&quot;id&quot;:&quot;model_doc/layoutxlm&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/layoutxlm&quot;},{&quot;title&quot;:&quot;LiLT&quot;,&quot;id&quot;:&quot;model_doc/lilt&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/lilt&quot;},{&quot;title&quot;:&quot;Llava&quot;,&quot;id&quot;:&quot;model_doc/llava&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/llava&quot;},{&quot;title&quot;:&quot;LLaVA-NeXT&quot;,&quot;id&quot;:&quot;model_doc/llava_next&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/llava_next&quot;},{&quot;title&quot;:&quot;LLaVa-NeXT-Video&quot;,&quot;id&quot;:&quot;model_doc/llava_next_video&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/llava_next_video&quot;},{&quot;title&quot;:&quot;LLaVA-Onevision&quot;,&quot;id&quot;:&quot;model_doc/llava_onevision&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/llava_onevision&quot;},{&quot;title&quot;:&quot;LXMERT&quot;,&quot;id&quot;:&quot;model_doc/lxmert&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/lxmert&quot;},{&quot;title&quot;:&quot;MatCha&quot;,&quot;id&quot;:&quot;model_doc/matcha&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/matcha&quot;},{&quot;title&quot;:&quot;MGP-STR&quot;,&quot;id&quot;:&quot;model_doc/mgp-str&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/mgp-str&quot;},{&quot;title&quot;:&quot;mllama&quot;,&quot;id&quot;:&quot;model_doc/mllama&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/mllama&quot;},{&quot;title&quot;:&quot;Nougat&quot;,&quot;id&quot;:&quot;model_doc/nougat&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/nougat&quot;},{&quot;title&quot;:&quot;OmDet-Turbo&quot;,&quot;id&quot;:&quot;model_doc/omdet-turbo&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/omdet-turbo&quot;},{&quot;title&quot;:&quot;OneFormer&quot;,&quot;id&quot;:&quot;model_doc/oneformer&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/oneformer&quot;},{&quot;title&quot;:&quot;OWL-ViT&quot;,&quot;id&quot;:&quot;model_doc/owlvit&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/owlvit&quot;},{&quot;title&quot;:&quot;OWLv2&quot;,&quot;id&quot;:&quot;model_doc/owlv2&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/owlv2&quot;},{&quot;title&quot;:&quot;PaliGemma&quot;,&quot;id&quot;:&quot;model_doc/paligemma&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/paligemma&quot;},{&quot;title&quot;:&quot;Perceiver&quot;,&quot;id&quot;:&quot;model_doc/perceiver&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/perceiver&quot;},{&quot;title&quot;:&quot;Pix2Struct&quot;,&quot;id&quot;:&quot;model_doc/pix2struct&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/pix2struct&quot;},{&quot;title&quot;:&quot;Pixtral&quot;,&quot;id&quot;:&quot;model_doc/pixtral&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/pixtral&quot;},{&quot;title&quot;:&quot;Qwen2Audio&quot;,&quot;id&quot;:&quot;model_doc/qwen2_audio&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/qwen2_audio&quot;},{&quot;title&quot;:&quot;Qwen2VL&quot;,&quot;id&quot;:&quot;model_doc/qwen2_vl&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/qwen2_vl&quot;},{&quot;title&quot;:&quot;Segment Anything&quot;,&quot;id&quot;:&quot;model_doc/sam&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/sam&quot;},{&quot;title&quot;:&quot;SigLIP&quot;,&quot;id&quot;:&quot;model_doc/siglip&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/siglip&quot;},{&quot;title&quot;:&quot;Speech Encoder Decoder Models&quot;,&quot;id&quot;:&quot;model_doc/speech-encoder-decoder&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/speech-encoder-decoder&quot;},{&quot;title&quot;:&quot;TAPAS&quot;,&quot;id&quot;:&quot;model_doc/tapas&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/tapas&quot;},{&quot;title&quot;:&quot;TrOCR&quot;,&quot;id&quot;:&quot;model_doc/trocr&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/trocr&quot;},{&quot;title&quot;:&quot;TVLT&quot;,&quot;id&quot;:&quot;model_doc/tvlt&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/tvlt&quot;},{&quot;title&quot;:&quot;TVP&quot;,&quot;id&quot;:&quot;model_doc/tvp&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/tvp&quot;},{&quot;title&quot;:&quot;UDOP&quot;,&quot;id&quot;:&quot;model_doc/udop&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/udop&quot;},{&quot;title&quot;:&quot;VideoLlava&quot;,&quot;id&quot;:&quot;model_doc/video_llava&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/video_llava&quot;},{&quot;title&quot;:&quot;ViLT&quot;,&quot;id&quot;:&quot;model_doc/vilt&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/vilt&quot;},{&quot;title&quot;:&quot;VipLlava&quot;,&quot;id&quot;:&quot;model_doc/vipllava&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/vipllava&quot;},{&quot;title&quot;:&quot;Vision Encoder Decoder Models&quot;,&quot;id&quot;:&quot;model_doc/vision-encoder-decoder&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/vision-encoder-decoder&quot;},{&quot;title&quot;:&quot;Vision Text Dual Encoder&quot;,&quot;id&quot;:&quot;model_doc/vision-text-dual-encoder&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/vision-text-dual-encoder&quot;},{&quot;title&quot;:&quot;VisualBERT&quot;,&quot;id&quot;:&quot;model_doc/visual_bert&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/visual_bert&quot;},{&quot;title&quot;:&quot;X-CLIP&quot;,&quot;id&quot;:&quot;model_doc/xclip&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/xclip&quot;}]},{&quot;title&quot;:&quot;Reinforcement learning models&quot;,&quot;isExpanded&quot;:false,&quot;sections&quot;:[{&quot;title&quot;:&quot;Decision Transformer&quot;,&quot;id&quot;:&quot;model_doc/decision_transformer&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/decision_transformer&quot;},{&quot;title&quot;:&quot;Trajectory Transformer&quot;,&quot;id&quot;:&quot;model_doc/trajectory_transformer&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/trajectory_transformer&quot;}]},{&quot;title&quot;:&quot;Time series models&quot;,&quot;isExpanded&quot;:false,&quot;sections&quot;:[{&quot;title&quot;:&quot;Autoformer&quot;,&quot;id&quot;:&quot;model_doc/autoformer&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/autoformer&quot;},{&quot;title&quot;:&quot;Informer&quot;,&quot;id&quot;:&quot;model_doc/informer&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/informer&quot;},{&quot;title&quot;:&quot;PatchTSMixer&quot;,&quot;id&quot;:&quot;model_doc/patchtsmixer&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/patchtsmixer&quot;},{&quot;title&quot;:&quot;PatchTST&quot;,&quot;id&quot;:&quot;model_doc/patchtst&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/patchtst&quot;},{&quot;title&quot;:&quot;Time Series Transformer&quot;,&quot;id&quot;:&quot;model_doc/time_series_transformer&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/time_series_transformer&quot;}]},{&quot;title&quot;:&quot;Graph models&quot;,&quot;isExpanded&quot;:false,&quot;sections&quot;:[{&quot;title&quot;:&quot;Graphormer&quot;,&quot;id&quot;:&quot;model_doc/graphormer&quot;,&quot;url&quot;:&quot;/docs/transformers/model_doc/graphormer&quot;}]}]},{&quot;title&quot;:&quot;Internal Helpers&quot;,&quot;isExpanded&quot;:true,&quot;sections&quot;:[{&quot;title&quot;:&quot;Custom Layers and Utilities&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;internal/modeling_utils&quot;,&quot;url&quot;:&quot;/docs/transformers/internal/modeling_utils&quot;},{&quot;title&quot;:&quot;Utilities for pipelines&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;internal/pipelines_utils&quot;,&quot;url&quot;:&quot;/docs/transformers/internal/pipelines_utils&quot;},{&quot;title&quot;:&quot;Utilities for Tokenizers&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;internal/tokenization_utils&quot;,&quot;url&quot;:&quot;/docs/transformers/internal/tokenization_utils&quot;},{&quot;title&quot;:&quot;Utilities for Trainer&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;internal/trainer_utils&quot;,&quot;url&quot;:&quot;/docs/transformers/internal/trainer_utils&quot;},{&quot;title&quot;:&quot;Utilities for Generation&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;internal/generation_utils&quot;,&quot;url&quot;:&quot;/docs/transformers/internal/generation_utils&quot;},{&quot;title&quot;:&quot;Utilities for Image Processors&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;internal/image_processing_utils&quot;,&quot;url&quot;:&quot;/docs/transformers/internal/image_processing_utils&quot;},{&quot;title&quot;:&quot;Utilities for Audio processing&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;internal/audio_utils&quot;,&quot;url&quot;:&quot;/docs/transformers/internal/audio_utils&quot;},{&quot;title&quot;:&quot;General Utilities&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;internal/file_utils&quot;,&quot;url&quot;:&quot;/docs/transformers/internal/file_utils&quot;},{&quot;title&quot;:&quot;Utilities for Time Series&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;internal/time_series_utils&quot;,&quot;url&quot;:&quot;/docs/transformers/internal/time_series_utils&quot;}]}]}],&quot;chapterId&quot;:&quot;generation_strategies&quot;,&quot;docType&quot;:&quot;docs&quot;,&quot;isLoggedIn&quot;:true,&quot;lang&quot;:&quot;en&quot;,&quot;langs&quot;:[&quot;ar&quot;,&quot;de&quot;,&quot;en&quot;,&quot;es&quot;,&quot;fr&quot;,&quot;hi&quot;,&quot;it&quot;,&quot;ja&quot;,&quot;ko&quot;,&quot;pt&quot;,&quot;te&quot;,&quot;tr&quot;,&quot;zh&quot;],&quot;library&quot;:&quot;transformers&quot;,&quot;theme&quot;:&quot;system&quot;,&quot;version&quot;:&quot;v4.46.3&quot;,&quot;versions&quot;:[{&quot;version&quot;:&quot;main&quot;},{&quot;version&quot;:&quot;v4.46.3&quot;},{&quot;version&quot;:&quot;v4.46.2&quot;},{&quot;version&quot;:&quot;v4.46.0&quot;},{&quot;version&quot;:&quot;v4.45.2&quot;},{&quot;version&quot;:&quot;v4.45.1&quot;},{&quot;version&quot;:&quot;v4.44.2&quot;},{&quot;version&quot;:&quot;v4.44.1&quot;},{&quot;version&quot;:&quot;v4.44.0&quot;},{&quot;version&quot;:&quot;v4.43.4&quot;},{&quot;version&quot;:&quot;v4.43.3&quot;},{&quot;version&quot;:&quot;v4.43.2&quot;},{&quot;version&quot;:&quot;v4.43.0&quot;},{&quot;version&quot;:&quot;v4.42.4&quot;},{&quot;version&quot;:&quot;v4.42.0&quot;},{&quot;version&quot;:&quot;v4.41.2&quot;},{&quot;version&quot;:&quot;v4.41.1&quot;},{&quot;version&quot;:&quot;v4.41.0&quot;},{&quot;version&quot;:&quot;v4.40.2&quot;},{&quot;version&quot;:&quot;v4.40.1&quot;},{&quot;version&quot;:&quot;v4.40.0&quot;},{&quot;version&quot;:&quot;v4.39.3&quot;},{&quot;version&quot;:&quot;v4.39.2&quot;},{&quot;version&quot;:&quot;v4.39.1&quot;},{&quot;version&quot;:&quot;v4.39.0&quot;},{&quot;version&quot;:&quot;v4.38.2&quot;},{&quot;version&quot;:&quot;v4.38.1&quot;},{&quot;version&quot;:&quot;v4.38.0&quot;},{&quot;version&quot;:&quot;v4.37.2&quot;},{&quot;version&quot;:&quot;v4.37.1&quot;},{&quot;version&quot;:&quot;v4.37.0&quot;},{&quot;version&quot;:&quot;v4.36.1&quot;},{&quot;version&quot;:&quot;v4.36.0&quot;},{&quot;version&quot;:&quot;v4.35.2&quot;},{&quot;version&quot;:&quot;v4.35.1&quot;},{&quot;version&quot;:&quot;v4.35.0&quot;},{&quot;version&quot;:&quot;v4.34.1&quot;},{&quot;version&quot;:&quot;v4.34.0&quot;},{&quot;version&quot;:&quot;v4.33.3&quot;},{&quot;version&quot;:&quot;v4.33.2&quot;},{&quot;version&quot;:&quot;v4.33.0&quot;},{&quot;version&quot;:&quot;v4.32.1&quot;},{&quot;version&quot;:&quot;v4.32.0&quot;},{&quot;version&quot;:&quot;v4.31.0&quot;},{&quot;version&quot;:&quot;v4.30.0&quot;},{&quot;version&quot;:&quot;v4.29.1&quot;},{&quot;version&quot;:&quot;v4.29.0&quot;},{&quot;version&quot;:&quot;v4.28.1&quot;},{&quot;version&quot;:&quot;v4.28.0&quot;},{&quot;version&quot;:&quot;v4.27.2&quot;},{&quot;version&quot;:&quot;v4.27.1&quot;},{&quot;version&quot;:&quot;v4.27.0&quot;},{&quot;version&quot;:&quot;v4.26.1&quot;},{&quot;version&quot;:&quot;v4.26.0&quot;},{&quot;version&quot;:&quot;v4.25.1&quot;},{&quot;version&quot;:&quot;v4.24.0&quot;},{&quot;version&quot;:&quot;v4.23.1&quot;},{&quot;version&quot;:&quot;v4.23.0&quot;},{&quot;version&quot;:&quot;v4.22.2&quot;},{&quot;version&quot;:&quot;v4.22.1&quot;},{&quot;version&quot;:&quot;v4.22.0&quot;},{&quot;version&quot;:&quot;v4.21.3&quot;},{&quot;version&quot;:&quot;v4.21.2&quot;},{&quot;version&quot;:&quot;v4.21.1&quot;},{&quot;version&quot;:&quot;v4.21.0&quot;},{&quot;version&quot;:&quot;v4.20.1&quot;},{&quot;version&quot;:&quot;v4.20.0&quot;},{&quot;version&quot;:&quot;v4.19.4&quot;},{&quot;version&quot;:&quot;v4.19.3&quot;},{&quot;version&quot;:&quot;v4.19.2&quot;},{&quot;version&quot;:&quot;v4.19.0&quot;},{&quot;version&quot;:&quot;v4.18.0&quot;},{&quot;version&quot;:&quot;v4.17.0&quot;},{&quot;version&quot;:&quot;v4.16.2&quot;},{&quot;version&quot;:&quot;v4.16.1&quot;},{&quot;version&quot;:&quot;v4.16.0&quot;},{&quot;version&quot;:&quot;v4.15.0&quot;},{&quot;version&quot;:&quot;v4.14.1&quot;},{&quot;version&quot;:&quot;v4.13.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.12.5&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.12.4&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.12.2&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.12.1&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.12.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.11.3&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.11.2&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.11.1&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.11.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.10.1&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.10.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.9.2&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.9.1&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.9.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.8.2&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.8.1&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.8.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.7.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.6.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.5.1&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.5.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.4.2&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.4.1&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.4.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.3.3&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.3.2&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.3.1&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.3.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.2.2&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.2.1&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.2.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.1.1&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.1.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.0.1&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v4.0.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v3.5.1&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v3.5.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v3.4.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v3.3.1&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v3.3.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v3.2.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v3.1.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v3.0.2&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v3.0.1&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v3.0.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v2.11.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v2.10.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v2.9.1&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v2.9.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v2.8.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v2.7.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v2.6.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v2.5.1&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v2.5.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v2.4.1&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v2.4.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v2.3.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v2.2.2&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v2.2.1&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v2.2.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v2.1.1&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v2.0.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v1.2.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v1.1.0&quot;},{&quot;sphinx&quot;:true,&quot;version&quot;:&quot;v1.0.0&quot;},{&quot;version&quot;:&quot;doc-builder-html&quot;}],&quot;title&quot;:&quot;Text generation strategies&quot;}">







<div class="z-2 w-full flex-none lg:flex lg:h-dvh lg:w-[270px] lg:flex-col 2xl:w-[300px] false"><div class="shadow-alternate flex h-auto w-full items-center rounded-b-xl border-b bg-white py-2 text-lg leading-tight lg:hidden">
		<div class="flex flex-1 cursor-pointer flex-col justify-center self-stretch pl-6"><p class="text-sm text-gray-400 first-letter:capitalize">Transformers documentation
			</p>
			<div class="mr-2 flex items-center"><p class="font-semibold">Text generation strategies</p>
				<svg class="text-xl false" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path d="M16.293 9.293L12 13.586L7.707 9.293l-1.414 1.414L12 16.414l5.707-5.707z" fill="currentColor"></path></svg></div></div>
		<button class="hover:shadow-alternate group ml-auto mr-6 inline-flex flex-none cursor-pointer rounded-xl border p-2"><svg class="text-gray-500 group-hover:text-gray-700" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg></button></div>
	<div class="hidden flex-col justify-between border-b border-r bg-white bg-gradient-to-r p-4 lg:flex from-orange-50 to-white dark:from-gray-900 dark:to-gray-950"><div class="group relative mb-2 flex min-w-[50%] items-center self-start text-lg font-bold leading-tight first-letter:capitalize"><div class="mr-1.5 h-1.5 w-1.5 rounded-full bg-orange-500 flex-none"></div>
			<h1>Transformers</h1>
			<svg class="opacity-50 ml-0.5 flex-none group-hover:opacity-100" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path d="M16.293 9.293L12 13.586L7.707 9.293l-1.414 1.414L12 16.414l5.707-5.707z" fill="currentColor"></path></svg>

			<select class="absolute inset-0 border-none bg-white text-base opacity-0 outline-none"><option value="/docs">ðŸ¡ View all docs</option><option value="/docs/optimum-neuron" >AWS Trainium &amp; Inferentia</option><option value="/docs/accelerate" >Accelerate</option><option value="/docs/sagemaker" >Amazon SageMaker</option><option value="https://argilla-io.github.io/argilla/" >Argilla</option><option value="/docs/autotrain" >AutoTrain</option><option value="/docs/bitsandbytes" >Bitsandbytes</option><option value="/docs/chat-ui" >Chat UI</option><option value="/docs/competitions" >Competitions</option><option value="/docs/dataset-viewer" >Dataset viewer</option><option value="/docs/datasets" >Datasets</option><option value="/docs/diffusers" >Diffusers</option><option value="https://distilabel.argilla.io/" >Distilabel</option><option value="/docs/evaluate" >Evaluate</option><option value="/docs/google-cloud" >Google Cloud</option><option value="/docs/optimum-tpu" >Google TPUs</option><option value="https://www.gradio.app/docs/" >Gradio</option><option value="/docs/hub" >Hub</option><option value="/docs/huggingface_hub" >Hub Python Library</option><option value="/docs/hugs" >Hugging Face Generative AI Services (HUGS)</option><option value="/docs/huggingface.js" >Huggingface.js</option><option value="/docs/api-inference" >Inference API (serverless)</option><option value="/docs/inference-endpoints" >Inference Endpoints (dedicated)</option><option value="/docs/leaderboards" >Leaderboards</option><option value="/docs/optimum" >Optimum</option><option value="/docs/peft" >PEFT</option><option value="/docs/safetensors" >Safetensors</option><option value="https://sbert.net/" >Sentence Transformers</option><option value="/docs/trl" >TRL</option><option value="/tasks" >Tasks</option><option value="/docs/text-embeddings-inference" >Text Embeddings Inference</option><option value="/docs/text-generation-inference" >Text Generation Inference</option><option value="/docs/tokenizers" >Tokenizers</option><option value="/docs/transformers" selected>Transformers</option><option value="/docs/transformers.js" >Transformers.js</option><option value="/docs/timm" >timm</option></select></div>

		<button class="shadow-alternate mb-2 flex w-full items-center rounded-full border bg-white px-2 py-1 text-left text-sm text-gray-400 ring-indigo-200 hover:bg-indigo-50 hover:ring-2 dark:border-gray-700 dark:ring-yellow-600 dark:hover:bg-gray-900 dark:hover:text-yellow-500"><svg class="flex-none mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg>
			<div>Search documentation</div>
			</button>
		<div class="flex items-center">
				<select class="form-input !mt-0 mr-1 !w-20 rounded !border border-gray-200 p-1 text-xs uppercase dark:!text-gray-400"><option value="0" >main</option><option value="1" selected>v4.46.3</option><option value="2" >v4.45.2</option><option value="3" >v4.44.2</option><option value="4" >v4.43.4</option><option value="5" >v4.42.4</option><option value="6" >v4.41.2</option><option value="7" >v4.40.2</option><option value="8" >v4.39.3</option><option value="9" >v4.38.2</option><option value="10" >v4.37.2</option><option value="11" >v4.36.1</option><option value="12" >v4.35.2</option><option value="13" >v4.34.1</option><option value="14" >v4.33.3</option><option value="15" >v4.32.1</option><option value="16" >v4.31.0</option><option value="17" >v4.30.0</option><option value="18" >v4.29.1</option><option value="19" >v4.28.1</option><option value="20" >v4.27.2</option><option value="21" >v4.26.1</option><option value="22" >v4.25.1</option><option value="23" >v4.24.0</option><option value="24" >v4.23.1</option><option value="25" >v4.22.2</option><option value="26" >v4.21.3</option><option value="27" >v4.20.1</option><option value="28" >v4.19.4</option><option value="29" >v4.18.0</option><option value="30" >v4.17.0</option><option value="31" >v4.16.2</option><option value="32" >v4.15.0</option><option value="33" >v4.14.1</option><option value="34" >v4.13.0</option><option value="35" >v4.12.5</option><option value="36" >v4.11.3</option><option value="37" >v4.10.1</option><option value="38" >v4.9.2</option><option value="39" >v4.8.2</option><option value="40" >v4.7.0</option><option value="41" >v4.6.0</option><option value="42" >v4.5.1</option><option value="43" >v4.4.2</option><option value="44" >v4.3.3</option><option value="45" >v4.2.2</option><option value="46" >v4.1.1</option><option value="47" >v4.0.1</option><option value="48" >v3.5.1</option><option value="49" >v3.4.0</option><option value="50" >v3.3.1</option><option value="51" >v3.2.0</option><option value="52" >v3.1.0</option><option value="53" >v3.0.2</option><option value="54" >v2.11.0</option><option value="55" >v2.10.0</option><option value="56" >v2.9.1</option><option value="57" >v2.8.0</option><option value="58" >v2.7.0</option><option value="59" >v2.6.0</option><option value="60" >v2.5.1</option><option value="61" >v2.4.1</option><option value="62" >v2.3.0</option><option value="63" >v2.2.2</option><option value="64" >v2.1.1</option><option value="65" >v2.0.0</option><option value="66" >v1.2.0</option><option value="67" >v1.1.0</option><option value="68" >v1.0.0</option><option value="69" >doc-builder-html</option></select>
			
			<select class="form-input mr-1 rounded border-gray-200 p-1 text-xs dark:!text-gray-400 !w-12 !mt-0 !border"><option value="ar" >AR</option><option value="de" >DE</option><option value="en" selected>EN</option><option value="es" >ES</option><option value="fr" >FR</option><option value="hi" >HI</option><option value="it" >IT</option><option value="ja" >JA</option><option value="ko" >KO</option><option value="pt" >PT</option><option value="te" >TE</option><option value="tr" >TR</option><option value="zh" >ZH</option></select>
			
<div class="relative inline-block">
	<button class="rounded-full border border-gray-100 p-1.5  flex items-center text-sm text-gray-500 bg-white hover:bg-purple-50 hover:border-purple-200 dark:hover:bg-gray-800 dark:hover:border-gray-950 " type="button">
		<svg class="opacity-50 dark:opacity-70  text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25" fill="currentColor"><path d="M13.0101 3C13.7157 3.0078 14.4184 3.09062 15.1077 3.24652C15.2543 3.2797 15.3871 3.35848 15.4874 3.47186C15.5877 3.58523 15.6506 3.72754 15.6672 3.8789L15.8306 5.36678C15.8537 5.57655 15.925 5.77791 16.0389 5.95464C16.1527 6.13137 16.3059 6.27854 16.4861 6.38432C16.6663 6.4901 16.8685 6.55153 17.0764 6.56367C17.2843 6.57581 17.4921 6.53831 17.6831 6.4542L19.0299 5.85495C19.1667 5.79391 19.3187 5.77743 19.4651 5.8078C19.6115 5.83818 19.745 5.9139 19.847 6.02449C20.8199 7.07789 21.5443 8.3412 21.9658 9.71937C22.01 9.8642 22.0087 10.0194 21.962 10.1634C21.9153 10.3074 21.8256 10.4332 21.7053 10.5232L20.5113 11.4158C20.3434 11.5408 20.2069 11.7041 20.1128 11.8925C20.0187 12.0809 19.9697 12.2891 19.9697 12.5003C19.9697 12.7114 20.0187 12.9196 20.1128 13.108C20.2069 13.2964 20.3434 13.4597 20.5113 13.5848L21.7062 14.4763C21.8269 14.5663 21.917 14.6922 21.9638 14.8364C22.0107 14.9806 22.0121 15.1361 21.9677 15.2812C21.546 16.6593 20.8216 17.9225 19.849 18.976C19.7471 19.0864 19.6141 19.162 19.4681 19.1926C19.3221 19.2231 19.1704 19.207 19.0338 19.1466L17.6812 18.5454C17.4904 18.4606 17.2827 18.4225 17.0748 18.4343C16.8668 18.446 16.6645 18.5072 16.4842 18.6129C16.3039 18.7185 16.1508 18.8658 16.037 19.0426C15.9233 19.2195 15.8523 19.421 15.8297 19.6308L15.6672 21.1177C15.6508 21.2674 15.5892 21.4084 15.4908 21.5212C15.3923 21.6341 15.2619 21.7133 15.1173 21.7482C13.7249 22.0839 12.2742 22.0839 10.8817 21.7482C10.7371 21.7133 10.6067 21.6341 10.5083 21.5212C10.4098 21.4084 10.3482 21.2674 10.3318 21.1177L10.1703 19.6328C10.1468 19.4235 10.0751 19.2227 9.96107 19.0465C9.84703 18.8704 9.69381 18.7238 9.51373 18.6186C9.33364 18.5134 9.13172 18.4525 8.92419 18.4408C8.71666 18.4291 8.50931 18.4669 8.31882 18.5512L6.9672 19.1514C6.83048 19.2121 6.67854 19.2283 6.53235 19.1978C6.38616 19.1672 6.25292 19.0915 6.15103 18.9809C5.17789 17.9263 4.45346 16.6616 4.03227 15.2821C3.98795 15.1371 3.98931 14.9816 4.03617 14.8374C4.08304 14.6931 4.17306 14.5673 4.29375 14.4773L5.48868 13.5848C5.65676 13.4599 5.79345 13.2966 5.88768 13.1082C5.9819 12.9198 6.031 12.7115 6.031 12.5003C6.031 12.289 5.9819 12.0808 5.88768 11.8923C5.79345 11.7039 5.65676 11.5407 5.48868 11.4158L4.29375 10.5252C4.17324 10.4351 4.0834 10.3092 4.03671 10.1649C3.99003 10.0207 3.98881 9.8653 4.03323 9.72034C4.45479 8.34219 5.17922 7.07889 6.15199 6.02547C6.25407 5.91487 6.38753 5.83915 6.53391 5.80878C6.6803 5.77841 6.83238 5.79488 6.96912 5.85593L8.31498 6.45517C8.5063 6.53923 8.71441 6.57664 8.92258 6.56439C9.13075 6.55214 9.33319 6.49057 9.51363 6.38462C9.69406 6.27868 9.84747 6.13132 9.96152 5.95438C10.0756 5.77744 10.1471 5.57585 10.1703 5.36581L10.3338 3.8789C10.3503 3.72724 10.4132 3.58462 10.5137 3.47103C10.6142 3.35745 10.7473 3.2786 10.8942 3.24555C11.5835 3.09062 12.2881 3.00877 13.0101 3ZM12.9986 9.57711C12.2337 9.57711 11.5001 9.88508 10.9593 10.4333C10.4184 10.9815 10.1146 11.725 10.1146 12.5003C10.1146 13.2755 10.4184 14.0191 10.9593 14.5672C11.5001 15.1154 12.2337 15.4234 12.9986 15.4234C13.7634 15.4234 14.497 15.1154 15.0378 14.5672C15.5787 14.0191 15.8825 13.2755 15.8825 12.5003C15.8825 11.725 15.5787 10.9815 15.0378 10.4333C14.497 9.88508 13.7634 9.57711 12.9986 9.57711Z"></path></svg>
			
		</button>
	
	
	</div>
			<a href="https://github.com/huggingface/transformers" class="group ml-auto text-xs text-gray-500 hover:text-gray-700 hover:underline dark:hover:text-gray-300"><svg class="inline-block text-gray-500 group-hover:text-gray-700 dark:group-hover:text-gray-300 mr-1.5 -mt-1 w-4 h-4" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1.03em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 250"><path d="M128.001 0C57.317 0 0 57.307 0 128.001c0 56.554 36.676 104.535 87.535 121.46c6.397 1.185 8.746-2.777 8.746-6.158c0-3.052-.12-13.135-.174-23.83c-35.61 7.742-43.124-15.103-43.124-15.103c-5.823-14.795-14.213-18.73-14.213-18.73c-11.613-7.944.876-7.78.876-7.78c12.853.902 19.621 13.19 19.621 13.19c11.417 19.568 29.945 13.911 37.249 10.64c1.149-8.272 4.466-13.92 8.127-17.116c-28.431-3.236-58.318-14.212-58.318-63.258c0-13.975 5-25.394 13.188-34.358c-1.329-3.224-5.71-16.242 1.24-33.874c0 0 10.749-3.44 35.21 13.121c10.21-2.836 21.16-4.258 32.038-4.307c10.878.049 21.837 1.47 32.066 4.307c24.431-16.56 35.165-13.12 35.165-13.12c6.967 17.63 2.584 30.65 1.255 33.873c8.207 8.964 13.173 20.383 13.173 34.358c0 49.163-29.944 59.988-58.447 63.157c4.591 3.972 8.682 11.762 8.682 23.704c0 17.126-.148 30.91-.148 35.126c0 3.407 2.304 7.398 8.792 6.14C219.37 232.5 256 184.537 256 128.002C256 57.307 198.691 0 128.001 0zm-80.06 182.34c-.282.636-1.283.827-2.194.39c-.929-.417-1.45-1.284-1.15-1.922c.276-.655 1.279-.838 2.205-.399c.93.418 1.46 1.293 1.139 1.931zm6.296 5.618c-.61.566-1.804.303-2.614-.591c-.837-.892-.994-2.086-.375-2.66c.63-.566 1.787-.301 2.626.591c.838.903 1 2.088.363 2.66zm4.32 7.188c-.785.545-2.067.034-2.86-1.104c-.784-1.138-.784-2.503.017-3.05c.795-.547 2.058-.055 2.861 1.075c.782 1.157.782 2.522-.019 3.08zm7.304 8.325c-.701.774-2.196.566-3.29-.49c-1.119-1.032-1.43-2.496-.726-3.27c.71-.776 2.213-.558 3.315.49c1.11 1.03 1.45 2.505.701 3.27zm9.442 2.81c-.31 1.003-1.75 1.459-3.199 1.033c-1.448-.439-2.395-1.613-2.103-2.626c.301-1.01 1.747-1.484 3.207-1.028c1.446.436 2.396 1.602 2.095 2.622zm10.744 1.193c.036 1.055-1.193 1.93-2.715 1.95c-1.53.034-2.769-.82-2.786-1.86c0-1.065 1.202-1.932 2.733-1.958c1.522-.03 2.768.818 2.768 1.868zm10.555-.405c.182 1.03-.875 2.088-2.387 2.37c-1.485.271-2.861-.365-3.05-1.386c-.184-1.056.893-2.114 2.376-2.387c1.514-.263 2.868.356 3.061 1.403z" fill="currentColor"></path></svg>
				</a></div></div>

	<nav class="hidden flex-auto lg:flex bottom-0 left-0 w-full flex-col overflow-y-auto border-r px-4 pb-16 pt-3 text-[0.95rem] lg:w-[270px] 2xl:w-[300px]">
		
		<div class="group flex cursor-pointer items-center pl-2 text-[0.8rem] font-semibold uppercase leading-9 hover:text-gray-700 dark:hover:text-gray-300 ml-0"><div class="flex after:absolute after:right-4 after:text-gray-500 group-hover:after:content-['â–¶'] after:rotate-90 after:transform"><span><span class="inline-block space-x-1 leading-5"><span><!-- HTML_TAG_START -->Get started<!-- HTML_TAG_END --></span>
						
					</span></span>
			</div></div>
		<div class="flex flex-col"><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/index" id="index"><!-- HTML_TAG_START -->ðŸ¤— Transformers<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/quicktour" id="quicktour"><!-- HTML_TAG_START -->Quick tour<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/installation" id="installation"><!-- HTML_TAG_START -->Installation<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/add_new_model" id="add_new_model"><!-- HTML_TAG_START -->Adding a new model to `transformers`<!-- HTML_TAG_END -->
		</a>
			</div>
		<div class="group flex cursor-pointer items-center pl-2 text-[0.8rem] font-semibold uppercase leading-9 hover:text-gray-700 dark:hover:text-gray-300 ml-0"><div class="flex after:absolute after:right-4 after:text-gray-500 group-hover:after:content-['â–¶'] after:rotate-90 after:transform"><span><span class="inline-block space-x-1 leading-5"><span><!-- HTML_TAG_START -->Tutorials<!-- HTML_TAG_END --></span>
						
					</span></span>
			</div></div>
		<div class="flex flex-col"><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/pipeline_tutorial" id="pipeline_tutorial"><!-- HTML_TAG_START -->Run inference with pipelines<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/autoclass_tutorial" id="autoclass_tutorial"><!-- HTML_TAG_START -->Write portable code with AutoClass<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/preprocessing" id="preprocessing"><!-- HTML_TAG_START -->Preprocess data<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/training" id="training"><!-- HTML_TAG_START -->Fine-tune a pretrained model<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/run_scripts" id="run_scripts"><!-- HTML_TAG_START -->Train with a script<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/accelerate" id="accelerate"><!-- HTML_TAG_START -->Set up distributed training with ðŸ¤— Accelerate<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/peft" id="peft"><!-- HTML_TAG_START -->Load and train adapters with ðŸ¤— PEFT<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/model_sharing" id="model_sharing"><!-- HTML_TAG_START -->Share your model<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/agents" id="agents"><!-- HTML_TAG_START -->Agents 101<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/agents_advanced" id="agents_advanced"><!-- HTML_TAG_START -->Agents, supercharged - Multi-agents, External tools, and more<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/llm_tutorial" id="llm_tutorial"><!-- HTML_TAG_START -->Generation with LLMs<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/conversations" id="conversations"><!-- HTML_TAG_START -->Chatting with Transformers<!-- HTML_TAG_END -->
		</a>
			</div>
		<div class="group flex cursor-pointer items-center pl-2 text-[0.8rem] font-semibold uppercase leading-9 hover:text-gray-700 dark:hover:text-gray-300 ml-0"><div class="flex after:absolute after:right-4 after:text-gray-500 group-hover:after:content-['â–¶'] after:rotate-90 after:transform"><span><span class="inline-block space-x-1 leading-5"><span><!-- HTML_TAG_START -->Task Guides<!-- HTML_TAG_END --></span>
						
					</span></span>
			</div></div>
		<div class="flex flex-col">
		<div class="group flex cursor-pointer items-center pl-2 text-[0.8rem] font-semibold uppercase leading-9 hover:text-gray-700 dark:hover:text-gray-300 ml-2"><div class="flex after:absolute after:right-4 after:text-gray-500 group-hover:after:content-['â–¶'] false"><span><span class="inline-block space-x-1 leading-5"><span><!-- HTML_TAG_START -->Natural Language Processing<!-- HTML_TAG_END --></span>
						
					</span></span>
			</div></div>
		
		<div class="group flex cursor-pointer items-center pl-2 text-[0.8rem] font-semibold uppercase leading-9 hover:text-gray-700 dark:hover:text-gray-300 ml-2"><div class="flex after:absolute after:right-4 after:text-gray-500 group-hover:after:content-['â–¶'] false"><span><span class="inline-block space-x-1 leading-5"><span><!-- HTML_TAG_START -->Audio<!-- HTML_TAG_END --></span>
						
					</span></span>
			</div></div>
		
		<div class="group flex cursor-pointer items-center pl-2 text-[0.8rem] font-semibold uppercase leading-9 hover:text-gray-700 dark:hover:text-gray-300 ml-2"><div class="flex after:absolute after:right-4 after:text-gray-500 group-hover:after:content-['â–¶'] false"><span><span class="inline-block space-x-1 leading-5"><span><!-- HTML_TAG_START -->Computer Vision<!-- HTML_TAG_END --></span>
						
					</span></span>
			</div></div>
		
		<div class="group flex cursor-pointer items-center pl-2 text-[0.8rem] font-semibold uppercase leading-9 hover:text-gray-700 dark:hover:text-gray-300 ml-2"><div class="flex after:absolute after:right-4 after:text-gray-500 group-hover:after:content-['â–¶'] false"><span><span class="inline-block space-x-1 leading-5"><span><!-- HTML_TAG_START -->Multimodal<!-- HTML_TAG_END --></span>
						
					</span></span>
			</div></div>
		
		<div class="group flex cursor-pointer items-center pl-2 text-[0.8rem] font-semibold uppercase leading-9 hover:text-gray-700 dark:hover:text-gray-300 ml-2"><div class="flex after:absolute after:right-4 after:text-gray-500 group-hover:after:content-['â–¶'] after:rotate-90 after:transform"><span><span class="inline-block space-x-1 leading-5"><span><!-- HTML_TAG_START -->Generation<!-- HTML_TAG_END --></span>
						
					</span></span>
			</div></div>
		<div class="flex flex-col"><a class="rounded-xl bg-gradient-to-br from-black to-gray-900 py-1 pl-2 pr-2 text-white first:mt-1 last:mb-4 dark:from-gray-800 dark:to-gray-900 ml-4" href="/docs/transformers/generation_strategies" id="generation_strategies"><!-- HTML_TAG_START -->Customize the generation strategy<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/kv_cache" id="kv_cache"><!-- HTML_TAG_START -->Best Practices for Generation with Cache<!-- HTML_TAG_END -->
		</a>
			</div>
		<div class="group flex cursor-pointer items-center pl-2 text-[0.8rem] font-semibold uppercase leading-9 hover:text-gray-700 dark:hover:text-gray-300 ml-2"><div class="flex after:absolute after:right-4 after:text-gray-500 group-hover:after:content-['â–¶'] false"><span><span class="inline-block space-x-1 leading-5"><span><!-- HTML_TAG_START -->Prompting<!-- HTML_TAG_END --></span>
						
					</span></span>
			</div></div>
		
			</div>
		<div class="group flex cursor-pointer items-center pl-2 text-[0.8rem] font-semibold uppercase leading-9 hover:text-gray-700 dark:hover:text-gray-300 ml-0"><div class="flex after:absolute after:right-4 after:text-gray-500 group-hover:after:content-['â–¶'] after:rotate-90 after:transform"><span><span class="inline-block space-x-1 leading-5"><span><!-- HTML_TAG_START -->Developer guides<!-- HTML_TAG_END --></span>
						
					</span></span>
			</div></div>
		<div class="flex flex-col"><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/fast_tokenizers" id="fast_tokenizers"><!-- HTML_TAG_START -->Use fast tokenizers from ðŸ¤— Tokenizers<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/multilingual" id="multilingual"><!-- HTML_TAG_START -->Run inference with multilingual models<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/create_a_model" id="create_a_model"><!-- HTML_TAG_START -->Use model-specific APIs<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/custom_models" id="custom_models"><!-- HTML_TAG_START -->Share a custom model<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/chat_templating" id="chat_templating"><!-- HTML_TAG_START -->Chat templates<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/trainer" id="trainer"><!-- HTML_TAG_START -->Trainer<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/sagemaker" id="sagemaker"><!-- HTML_TAG_START -->Run training on Amazon SageMaker<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/serialization" id="serialization"><!-- HTML_TAG_START -->Export to ONNX<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/tflite" id="tflite"><!-- HTML_TAG_START -->Export to TFLite<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/torchscript" id="torchscript"><!-- HTML_TAG_START -->Export to TorchScript<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/benchmarks" id="benchmarks"><!-- HTML_TAG_START -->Benchmarks<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/notebooks" id="notebooks"><!-- HTML_TAG_START -->Notebooks with examples<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/community" id="community"><!-- HTML_TAG_START -->Community resources<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/troubleshooting" id="troubleshooting"><!-- HTML_TAG_START -->Troubleshoot<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/gguf" id="gguf"><!-- HTML_TAG_START -->Interoperability with GGUF files<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/tiktoken" id="tiktoken"><!-- HTML_TAG_START -->Interoperability with TikToken files<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/modular_transformers" id="modular_transformers"><!-- HTML_TAG_START -->Modularity in `transformers`<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/how_to_hack_models" id="how_to_hack_models"><!-- HTML_TAG_START -->Model Hacking (overwriting a class to your usage)<!-- HTML_TAG_END -->
		</a>
			</div>
		<div class="group flex cursor-pointer items-center pl-2 text-[0.8rem] font-semibold uppercase leading-9 hover:text-gray-700 dark:hover:text-gray-300 ml-0"><div class="flex after:absolute after:right-4 after:text-gray-500 group-hover:after:content-['â–¶'] after:rotate-90 after:transform"><span><span class="inline-block space-x-1 leading-5"><span><!-- HTML_TAG_START -->Quantization Methods<!-- HTML_TAG_END --></span>
						
					</span></span>
			</div></div>
		<div class="flex flex-col"><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/quantization/overview" id="quantization/overview"><!-- HTML_TAG_START -->Getting started<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/quantization/bitsandbytes" id="quantization/bitsandbytes"><!-- HTML_TAG_START -->bitsandbytes<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/quantization/gptq" id="quantization/gptq"><!-- HTML_TAG_START -->GPTQ<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/quantization/awq" id="quantization/awq"><!-- HTML_TAG_START -->AWQ<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/quantization/aqlm" id="quantization/aqlm"><!-- HTML_TAG_START -->AQLM<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/quantization/quanto" id="quantization/quanto"><!-- HTML_TAG_START -->Quanto<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/quantization/eetq" id="quantization/eetq"><!-- HTML_TAG_START -->EETQ<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/quantization/hqq" id="quantization/hqq"><!-- HTML_TAG_START -->HQQ<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/quantization/fbgemm_fp8" id="quantization/fbgemm_fp8"><!-- HTML_TAG_START -->FBGEMM_FP8<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/quantization/optimum" id="quantization/optimum"><!-- HTML_TAG_START -->Optimum<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/quantization/torchao" id="quantization/torchao"><!-- HTML_TAG_START -->TorchAO<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/quantization/bitnet" id="quantization/bitnet"><!-- HTML_TAG_START -->BitNet<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/quantization/compressed_tensors" id="quantization/compressed_tensors"><!-- HTML_TAG_START -->compressed-tensors<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/quantization/contribute" id="quantization/contribute"><!-- HTML_TAG_START -->Contribute new quantization method<!-- HTML_TAG_END -->
		</a>
			</div>
		<div class="group flex cursor-pointer items-center pl-2 text-[0.8rem] font-semibold uppercase leading-9 hover:text-gray-700 dark:hover:text-gray-300 ml-0"><div class="flex after:absolute after:right-4 after:text-gray-500 group-hover:after:content-['â–¶'] after:rotate-90 after:transform"><span><span class="inline-block space-x-1 leading-5"><span><!-- HTML_TAG_START -->Performance and scalability<!-- HTML_TAG_END --></span>
						
					</span></span>
			</div></div>
		<div class="flex flex-col"><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/performance" id="performance"><!-- HTML_TAG_START -->Overview<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/llm_optims" id="llm_optims"><!-- HTML_TAG_START -->LLM inference optimization<!-- HTML_TAG_END -->
		</a>
		<div class="group flex cursor-pointer items-center pl-2 text-[0.8rem] font-semibold uppercase leading-9 hover:text-gray-700 dark:hover:text-gray-300 ml-2"><div class="flex after:absolute after:right-4 after:text-gray-500 group-hover:after:content-['â–¶'] after:rotate-90 after:transform"><span><span class="inline-block space-x-1 leading-5"><span><!-- HTML_TAG_START -->Efficient training techniques<!-- HTML_TAG_END --></span>
						
					</span></span>
			</div></div>
		<div class="flex flex-col"><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/perf_train_gpu_one" id="perf_train_gpu_one"><!-- HTML_TAG_START -->Methods and tools for efficient training on a single GPU<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/perf_train_gpu_many" id="perf_train_gpu_many"><!-- HTML_TAG_START -->Multiple GPUs and parallelism<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/fsdp" id="fsdp"><!-- HTML_TAG_START -->Fully Sharded Data Parallel<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/deepspeed" id="deepspeed"><!-- HTML_TAG_START -->DeepSpeed<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/perf_train_cpu" id="perf_train_cpu"><!-- HTML_TAG_START -->Efficient training on CPU<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/perf_train_cpu_many" id="perf_train_cpu_many"><!-- HTML_TAG_START -->Distributed CPU training<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/perf_train_tpu_tf" id="perf_train_tpu_tf"><!-- HTML_TAG_START -->Training on TPU with TensorFlow<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/perf_train_special" id="perf_train_special"><!-- HTML_TAG_START -->PyTorch training on Apple silicon<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/perf_hardware" id="perf_hardware"><!-- HTML_TAG_START -->Custom hardware for training<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/hpo_train" id="hpo_train"><!-- HTML_TAG_START -->Hyperparameter Search using Trainer API<!-- HTML_TAG_END -->
		</a>
			</div>
		<div class="group flex cursor-pointer items-center pl-2 text-[0.8rem] font-semibold uppercase leading-9 hover:text-gray-700 dark:hover:text-gray-300 ml-2"><div class="flex after:absolute after:right-4 after:text-gray-500 group-hover:after:content-['â–¶'] after:rotate-90 after:transform"><span><span class="inline-block space-x-1 leading-5"><span><!-- HTML_TAG_START -->Optimizing inference<!-- HTML_TAG_END --></span>
						
					</span></span>
			</div></div>
		<div class="flex flex-col"><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/perf_infer_cpu" id="perf_infer_cpu"><!-- HTML_TAG_START -->CPU inference<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/perf_infer_gpu_one" id="perf_infer_gpu_one"><!-- HTML_TAG_START -->GPU inference<!-- HTML_TAG_END -->
		</a>
			</div><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/big_models" id="big_models"><!-- HTML_TAG_START -->Instantiate a big model<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/debugging" id="debugging"><!-- HTML_TAG_START -->Debugging<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/tf_xla" id="tf_xla"><!-- HTML_TAG_START -->XLA Integration for TensorFlow Models<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/perf_torch_compile" id="perf_torch_compile"><!-- HTML_TAG_START -->Optimize inference using `torch.compile()`<!-- HTML_TAG_END -->
		</a>
			</div>
		<div class="group flex cursor-pointer items-center pl-2 text-[0.8rem] font-semibold uppercase leading-9 hover:text-gray-700 dark:hover:text-gray-300 ml-0"><div class="flex after:absolute after:right-4 after:text-gray-500 group-hover:after:content-['â–¶'] after:rotate-90 after:transform"><span><span class="inline-block space-x-1 leading-5"><span><!-- HTML_TAG_START -->Contribute<!-- HTML_TAG_END --></span>
						
					</span></span>
			</div></div>
		<div class="flex flex-col"><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/contributing" id="contributing"><!-- HTML_TAG_START -->How to contribute to ðŸ¤— Transformers?<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/add_new_model" id="add_new_model"><!-- HTML_TAG_START -->How to add a model to ðŸ¤— Transformers?<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/add_new_pipeline" id="add_new_pipeline"><!-- HTML_TAG_START -->How to add a pipeline to ðŸ¤— Transformers?<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/testing" id="testing"><!-- HTML_TAG_START -->Testing<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/pr_checks" id="pr_checks"><!-- HTML_TAG_START -->Checks on a Pull Request<!-- HTML_TAG_END -->
		</a>
			</div>
		<div class="group flex cursor-pointer items-center pl-2 text-[0.8rem] font-semibold uppercase leading-9 hover:text-gray-700 dark:hover:text-gray-300 ml-0"><div class="flex after:absolute after:right-4 after:text-gray-500 group-hover:after:content-['â–¶'] after:rotate-90 after:transform"><span><span class="inline-block space-x-1 leading-5"><span><!-- HTML_TAG_START -->Conceptual guides<!-- HTML_TAG_END --></span>
						
					</span></span>
			</div></div>
		<div class="flex flex-col"><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/philosophy" id="philosophy"><!-- HTML_TAG_START -->Philosophy<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/glossary" id="glossary"><!-- HTML_TAG_START -->Glossary<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/task_summary" id="task_summary"><!-- HTML_TAG_START -->What ðŸ¤— Transformers can do<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/tasks_explained" id="tasks_explained"><!-- HTML_TAG_START -->How ðŸ¤— Transformers solve tasks<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/model_summary" id="model_summary"><!-- HTML_TAG_START -->The Transformer model family<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/tokenizer_summary" id="tokenizer_summary"><!-- HTML_TAG_START -->Summary of the tokenizers<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/attention" id="attention"><!-- HTML_TAG_START -->Attention mechanisms<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/pad_truncation" id="pad_truncation"><!-- HTML_TAG_START -->Padding and truncation<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/bertology" id="bertology"><!-- HTML_TAG_START -->BERTology<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/perplexity" id="perplexity"><!-- HTML_TAG_START -->Perplexity of fixed-length models<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/pipeline_webserver" id="pipeline_webserver"><!-- HTML_TAG_START -->Pipelines for webserver inference<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/model_memory_anatomy" id="model_memory_anatomy"><!-- HTML_TAG_START -->Model training anatomy<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-2" href="/docs/transformers/llm_tutorial_optimization" id="llm_tutorial_optimization"><!-- HTML_TAG_START -->Getting the most out of LLMs<!-- HTML_TAG_END -->
		</a>
			</div>
		<div class="group flex cursor-pointer items-center pl-2 text-[0.8rem] font-semibold uppercase leading-9 hover:text-gray-700 dark:hover:text-gray-300 ml-0"><div class="flex after:absolute after:right-4 after:text-gray-500 group-hover:after:content-['â–¶'] after:rotate-90 after:transform"><span><span class="inline-block space-x-1 leading-5"><span><!-- HTML_TAG_START -->API<!-- HTML_TAG_END --></span>
						
					</span></span>
			</div></div>
		<div class="flex flex-col">
		<div class="group flex cursor-pointer items-center pl-2 text-[0.8rem] font-semibold uppercase leading-9 hover:text-gray-700 dark:hover:text-gray-300 ml-2"><div class="flex after:absolute after:right-4 after:text-gray-500 group-hover:after:content-['â–¶'] after:rotate-90 after:transform"><span><span class="inline-block space-x-1 leading-5"><span><!-- HTML_TAG_START -->Main Classes<!-- HTML_TAG_END --></span>
						
					</span></span>
			</div></div>
		<div class="flex flex-col"><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/main_classes/agent" id="main_classes/agent"><!-- HTML_TAG_START -->Agents and Tools<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/model_doc/auto" id="model_doc/auto"><!-- HTML_TAG_START -->Auto Classes<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/main_classes/backbones" id="main_classes/backbones"><!-- HTML_TAG_START -->Backbones<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/main_classes/callback" id="main_classes/callback"><!-- HTML_TAG_START -->Callbacks<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/main_classes/configuration" id="main_classes/configuration"><!-- HTML_TAG_START -->Configuration<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/main_classes/data_collator" id="main_classes/data_collator"><!-- HTML_TAG_START -->Data Collator<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/main_classes/keras_callbacks" id="main_classes/keras_callbacks"><!-- HTML_TAG_START -->Keras callbacks<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/main_classes/logging" id="main_classes/logging"><!-- HTML_TAG_START -->Logging<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/main_classes/model" id="main_classes/model"><!-- HTML_TAG_START -->Models<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/main_classes/text_generation" id="main_classes/text_generation"><!-- HTML_TAG_START -->Text Generation<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/main_classes/onnx" id="main_classes/onnx"><!-- HTML_TAG_START -->ONNX<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/main_classes/optimizer_schedules" id="main_classes/optimizer_schedules"><!-- HTML_TAG_START -->Optimization<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/main_classes/output" id="main_classes/output"><!-- HTML_TAG_START -->Model outputs<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/main_classes/pipelines" id="main_classes/pipelines"><!-- HTML_TAG_START -->Pipelines<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/main_classes/processors" id="main_classes/processors"><!-- HTML_TAG_START -->Processors<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/main_classes/quantization" id="main_classes/quantization"><!-- HTML_TAG_START -->Quantization<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/main_classes/tokenizer" id="main_classes/tokenizer"><!-- HTML_TAG_START -->Tokenizer<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/main_classes/trainer" id="main_classes/trainer"><!-- HTML_TAG_START -->Trainer<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/main_classes/deepspeed" id="main_classes/deepspeed"><!-- HTML_TAG_START -->DeepSpeed<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/main_classes/executorch" id="main_classes/executorch"><!-- HTML_TAG_START -->ExecuTorch<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/main_classes/feature_extractor" id="main_classes/feature_extractor"><!-- HTML_TAG_START -->Feature Extractor<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/main_classes/image_processor" id="main_classes/image_processor"><!-- HTML_TAG_START -->Image Processor<!-- HTML_TAG_END -->
		</a>
			</div>
		<div class="group flex cursor-pointer items-center pl-2 text-[0.8rem] font-semibold uppercase leading-9 hover:text-gray-700 dark:hover:text-gray-300 ml-2"><div class="flex after:absolute after:right-4 after:text-gray-500 group-hover:after:content-['â–¶'] after:rotate-90 after:transform"><span><span class="inline-block space-x-1 leading-5"><span><!-- HTML_TAG_START -->Models<!-- HTML_TAG_END --></span>
						
					</span></span>
			</div></div>
		<div class="flex flex-col">
		<div class="group flex cursor-pointer items-center pl-2 text-[0.8rem] font-semibold uppercase leading-9 hover:text-gray-700 dark:hover:text-gray-300 ml-4"><div class="flex after:absolute after:right-4 after:text-gray-500 group-hover:after:content-['â–¶'] false"><span><span class="inline-block space-x-1 leading-5"><span><!-- HTML_TAG_START -->Text models<!-- HTML_TAG_END --></span>
						
					</span></span>
			</div></div>
		
		<div class="group flex cursor-pointer items-center pl-2 text-[0.8rem] font-semibold uppercase leading-9 hover:text-gray-700 dark:hover:text-gray-300 ml-4"><div class="flex after:absolute after:right-4 after:text-gray-500 group-hover:after:content-['â–¶'] false"><span><span class="inline-block space-x-1 leading-5"><span><!-- HTML_TAG_START -->Vision models<!-- HTML_TAG_END --></span>
						
					</span></span>
			</div></div>
		
		<div class="group flex cursor-pointer items-center pl-2 text-[0.8rem] font-semibold uppercase leading-9 hover:text-gray-700 dark:hover:text-gray-300 ml-4"><div class="flex after:absolute after:right-4 after:text-gray-500 group-hover:after:content-['â–¶'] false"><span><span class="inline-block space-x-1 leading-5"><span><!-- HTML_TAG_START -->Audio models<!-- HTML_TAG_END --></span>
						
					</span></span>
			</div></div>
		
		<div class="group flex cursor-pointer items-center pl-2 text-[0.8rem] font-semibold uppercase leading-9 hover:text-gray-700 dark:hover:text-gray-300 ml-4"><div class="flex after:absolute after:right-4 after:text-gray-500 group-hover:after:content-['â–¶'] false"><span><span class="inline-block space-x-1 leading-5"><span><!-- HTML_TAG_START -->Video models<!-- HTML_TAG_END --></span>
						
					</span></span>
			</div></div>
		
		<div class="group flex cursor-pointer items-center pl-2 text-[0.8rem] font-semibold uppercase leading-9 hover:text-gray-700 dark:hover:text-gray-300 ml-4"><div class="flex after:absolute after:right-4 after:text-gray-500 group-hover:after:content-['â–¶'] false"><span><span class="inline-block space-x-1 leading-5"><span><!-- HTML_TAG_START -->Multimodal models<!-- HTML_TAG_END --></span>
						
					</span></span>
			</div></div>
		
		<div class="group flex cursor-pointer items-center pl-2 text-[0.8rem] font-semibold uppercase leading-9 hover:text-gray-700 dark:hover:text-gray-300 ml-4"><div class="flex after:absolute after:right-4 after:text-gray-500 group-hover:after:content-['â–¶'] false"><span><span class="inline-block space-x-1 leading-5"><span><!-- HTML_TAG_START -->Reinforcement learning models<!-- HTML_TAG_END --></span>
						
					</span></span>
			</div></div>
		
		<div class="group flex cursor-pointer items-center pl-2 text-[0.8rem] font-semibold uppercase leading-9 hover:text-gray-700 dark:hover:text-gray-300 ml-4"><div class="flex after:absolute after:right-4 after:text-gray-500 group-hover:after:content-['â–¶'] false"><span><span class="inline-block space-x-1 leading-5"><span><!-- HTML_TAG_START -->Time series models<!-- HTML_TAG_END --></span>
						
					</span></span>
			</div></div>
		
		<div class="group flex cursor-pointer items-center pl-2 text-[0.8rem] font-semibold uppercase leading-9 hover:text-gray-700 dark:hover:text-gray-300 ml-4"><div class="flex after:absolute after:right-4 after:text-gray-500 group-hover:after:content-['â–¶'] false"><span><span class="inline-block space-x-1 leading-5"><span><!-- HTML_TAG_START -->Graph models<!-- HTML_TAG_END --></span>
						
					</span></span>
			</div></div>
		
			</div>
		<div class="group flex cursor-pointer items-center pl-2 text-[0.8rem] font-semibold uppercase leading-9 hover:text-gray-700 dark:hover:text-gray-300 ml-2"><div class="flex after:absolute after:right-4 after:text-gray-500 group-hover:after:content-['â–¶'] after:rotate-90 after:transform"><span><span class="inline-block space-x-1 leading-5"><span><!-- HTML_TAG_START -->Internal Helpers<!-- HTML_TAG_END --></span>
						
					</span></span>
			</div></div>
		<div class="flex flex-col"><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/internal/modeling_utils" id="internal/modeling_utils"><!-- HTML_TAG_START -->Custom Layers and Utilities<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/internal/pipelines_utils" id="internal/pipelines_utils"><!-- HTML_TAG_START -->Utilities for pipelines<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/internal/tokenization_utils" id="internal/tokenization_utils"><!-- HTML_TAG_START -->Utilities for Tokenizers<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/internal/trainer_utils" id="internal/trainer_utils"><!-- HTML_TAG_START -->Utilities for Trainer<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/internal/generation_utils" id="internal/generation_utils"><!-- HTML_TAG_START -->Utilities for Generation<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/internal/image_processing_utils" id="internal/image_processing_utils"><!-- HTML_TAG_START -->Utilities for Image Processors<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/internal/audio_utils" id="internal/audio_utils"><!-- HTML_TAG_START -->Utilities for Audio processing<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/internal/file_utils" id="internal/file_utils"><!-- HTML_TAG_START -->General Utilities<!-- HTML_TAG_END -->
		</a><a class="transform py-1 pl-2 pr-2 text-gray-500 first:mt-1 last:mb-4 hover:translate-x-px hover:text-black dark:hover:text-gray-300 ml-4" href="/docs/transformers/internal/time_series_utils" id="internal/time_series_utils"><!-- HTML_TAG_START -->Utilities for Time Series<!-- HTML_TAG_END -->
		</a>
			</div>
			</div></nav></div></div></div>
		<div class="z-1 min-w-0 flex-1">
			<div class="px-6 pt-6 md:px-12 md:pb-16 md:pt-16">
				<div class="prose-doc prose relative mx-auto max-w-4xl break-words"><!-- HTML_TAG_START -->		<link href="/docs/transformers/v4.46.3/en/_app/immutable/assets/0.e3b0c442.css" rel="modulepreload">
		<link rel="modulepreload" href="/docs/transformers/v4.46.3/en/_app/immutable/entry/start.0a93d74d.js">
		<link rel="modulepreload" href="/docs/transformers/v4.46.3/en/_app/immutable/chunks/scheduler.25b97de1.js">
		<link rel="modulepreload" href="/docs/transformers/v4.46.3/en/_app/immutable/chunks/singletons.d92cc913.js">
		<link rel="modulepreload" href="/docs/transformers/v4.46.3/en/_app/immutable/chunks/index.e188933d.js">
		<link rel="modulepreload" href="/docs/transformers/v4.46.3/en/_app/immutable/chunks/paths.b2f3ca1e.js">
		<link rel="modulepreload" href="/docs/transformers/v4.46.3/en/_app/immutable/entry/app.123a6bc1.js">
		<link rel="modulepreload" href="/docs/transformers/v4.46.3/en/_app/immutable/chunks/index.d9030fc9.js">
		<link rel="modulepreload" href="/docs/transformers/v4.46.3/en/_app/immutable/nodes/0.f8c06a9e.js">
		<link rel="modulepreload" href="/docs/transformers/v4.46.3/en/_app/immutable/chunks/each.e59479a4.js">
		<link rel="modulepreload" href="/docs/transformers/v4.46.3/en/_app/immutable/nodes/22.2f04a294.js">
		<link rel="modulepreload" href="/docs/transformers/v4.46.3/en/_app/immutable/chunks/Tip.baa67368.js">
		<link rel="modulepreload" href="/docs/transformers/v4.46.3/en/_app/immutable/chunks/CodeBlock.e6cd0d95.js">
		<link rel="modulepreload" href="/docs/transformers/v4.46.3/en/_app/immutable/chunks/EditOnGithub.91d95064.js"><!-- HEAD_svelte-u9bgzb_START --><meta name="hf:doc:metadata" content="{&quot;title&quot;:&quot;Text generation strategies&quot;,&quot;local&quot;:&quot;text-generation-strategies&quot;,&quot;sections&quot;:[{&quot;title&quot;:&quot;Default text generation configuration&quot;,&quot;local&quot;:&quot;default-text-generation-configuration&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;Customize text generation&quot;,&quot;local&quot;:&quot;customize-text-generation&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;Save a custom decoding strategy with your model&quot;,&quot;local&quot;:&quot;save-a-custom-decoding-strategy-with-your-model&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;Streaming&quot;,&quot;local&quot;:&quot;streaming&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;Watermarking&quot;,&quot;local&quot;:&quot;watermarking&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;Decoding strategies&quot;,&quot;local&quot;:&quot;decoding-strategies&quot;,&quot;sections&quot;:[{&quot;title&quot;:&quot;Greedy Search&quot;,&quot;local&quot;:&quot;greedy-search&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3},{&quot;title&quot;:&quot;Contrastive search&quot;,&quot;local&quot;:&quot;contrastive-search&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3},{&quot;title&quot;:&quot;Multinomial sampling&quot;,&quot;local&quot;:&quot;multinomial-sampling&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3},{&quot;title&quot;:&quot;Beam-search decoding&quot;,&quot;local&quot;:&quot;beam-search-decoding&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3},{&quot;title&quot;:&quot;Beam-search multinomial sampling&quot;,&quot;local&quot;:&quot;beam-search-multinomial-sampling&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3},{&quot;title&quot;:&quot;Diverse beam search decoding&quot;,&quot;local&quot;:&quot;diverse-beam-search-decoding&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3},{&quot;title&quot;:&quot;Speculative Decoding&quot;,&quot;local&quot;:&quot;speculative-decoding&quot;,&quot;sections&quot;:[{&quot;title&quot;:&quot;Universal Assisted Decoding&quot;,&quot;local&quot;:&quot;universal-assisted-decoding&quot;,&quot;sections&quot;:[],&quot;depth&quot;:4}],&quot;depth&quot;:3},{&quot;title&quot;:&quot;DoLa Decoding&quot;,&quot;local&quot;:&quot;dola-decoding&quot;,&quot;sections&quot;:[{&quot;title&quot;:&quot;Understanding the dola_layers argument&quot;,&quot;local&quot;:&quot;understanding-the-dolalayers-argument&quot;,&quot;sections&quot;:[],&quot;depth&quot;:4}],&quot;depth&quot;:3}],&quot;depth&quot;:2}],&quot;depth&quot;:1}"><!-- HEAD_svelte-u9bgzb_END -->      <p></p>   <h1 class="relative group"><a id="text-generation-strategies" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#text-generation-strategies"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Text generation strategies</span></h1> <p data-svelte-h="svelte-1pq6r4w">Text generation is essential to many NLP tasks, such as open-ended text generation, summarization, translation, and
more. It also plays a role in a variety of mixed-modality applications that have text as an output like speech-to-text
and vision-to-text. Some of the models that can generate text include
GPT2, XLNet, OpenAI GPT, CTRL, TransformerXL, XLM, Bart, T5, GIT, Whisper.</p> <p data-svelte-h="svelte-9vq6co">Check out a few examples that use <a href="/docs/transformers/v4.46.3/en/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a> method to produce
text outputs for different tasks:</p> <ul data-svelte-h="svelte-18jzu0"><li><a href="./tasks/summarization#inference">Text summarization</a></li> <li><a href="./model_doc/git#transformers.GitForCausalLM.forward.example">Image captioning</a></li> <li><a href="./model_doc/whisper#transformers.WhisperForConditionalGeneration.forward.example">Audio transcription</a></li></ul> <p data-svelte-h="svelte-5iqkcx">Note that the inputs to the generate method depend on the modelâ€™s modality. They are returned by the modelâ€™s preprocessor
class, such as AutoTokenizer or AutoProcessor. If a modelâ€™s preprocessor creates more than one kind of input, pass all
the inputs to generate(). You can learn more about the individual modelâ€™s preprocessor in the corresponding modelâ€™s documentation.</p> <p data-svelte-h="svelte-agd87v">The process of selecting output tokens to generate text is known as decoding, and you can customize the decoding strategy
that the <code>generate()</code> method will use. Modifying a decoding strategy does not change the values of any trainable parameters.
However, it can have a noticeable impact on the quality of the generated output. It can help reduce repetition in the text
and make it more coherent.</p> <p data-svelte-h="svelte-1gun7m8">This guide describes:</p> <ul data-svelte-h="svelte-l1azua"><li>default generation configuration</li> <li>common decoding strategies and their main parameters</li> <li>saving and sharing custom generation configurations with your fine-tuned model on ðŸ¤— Hub</li></ul>  <h2 class="relative group"><a id="default-text-generation-configuration" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#default-text-generation-configuration"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Default text generation configuration</span></h2> <p data-svelte-h="svelte-11qhg0g">A decoding strategy for a model is defined in its generation configuration. When using pre-trained models for inference
within a <a href="/docs/transformers/v4.46.3/en/main_classes/pipelines#transformers.pipeline">pipeline()</a>, the models call the <code>PreTrainedModel.generate()</code> method that applies a default generation
configuration under the hood. The default configuration is also used when no custom configuration has been saved with
the model.</p> <p data-svelte-h="svelte-2o7gdz">When you load a model explicitly, you can inspect the generation configuration that comes with it through
<code>model.generation_config</code>:</p> <div class="code-block relative"><div class="absolute top-2.5 right-4"><button class="inline-flex items-center relative text-sm focus:text-green-500 cursor-pointer focus:outline-none transition duration-200 ease-in-out opacity-0 mx-0.5   text-gray-600 " title="code excerpt" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg> <div class="absolute pointer-events-none transition-opacity bg-black text-white py-1 px-2 leading-tight rounded font-normal shadow left-1/2 top-full transform -translate-x-1/2 translate-y-2 opacity-0"><div class="absolute bottom-full left-1/2 transform -translate-x-1/2 w-0 h-0 border-black border-4 border-t-0" style="border-left-color: transparent; border-right-color: transparent; "></div> Copied</div></button></div> <pre class=""><!-- HTML_TAG_START --><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;distilbert/distilgpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.generation_config
GenerationConfig {
  <span class="hljs-string">&quot;bos_token_id&quot;</span>: <span class="hljs-number">50256</span>,
  <span class="hljs-string">&quot;eos_token_id&quot;</span>: <span class="hljs-number">50256</span>
}
&lt;BLANKLINE&gt;<!-- HTML_TAG_END --></pre></div> <p data-svelte-h="svelte-18srzjs">Printing out the <code>model.generation_config</code> reveals only the values that are different from the default generation
configuration, and does not list any of the default values.</p> <p data-svelte-h="svelte-32rftl">The default generation configuration limits the size of the output combined with the input prompt to a maximum of 20
tokens to avoid running into resource limitations. The default decoding strategy is greedy search, which is the simplest decoding strategy that picks a token with the highest probability as the next token. For many tasks
and small output sizes this works well. However, when used to generate longer outputs, greedy search can start
producing highly repetitive results.</p>  <h2 class="relative group"><a id="customize-text-generation" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#customize-text-generation"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Customize text generation</span></h2> <p data-svelte-h="svelte-qprijs">You can override any <code>generation_config</code> by passing the parameters and their values directly to the <code>generate</code> method:</p> <div class="code-block relative"><div class="absolute top-2.5 right-4"><button class="inline-flex items-center relative text-sm focus:text-green-500 cursor-pointer focus:outline-none transition duration-200 ease-in-out opacity-0 mx-0.5   text-gray-600 " title="code excerpt" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg> <div class="absolute pointer-events-none transition-opacity bg-black text-white py-1 px-2 leading-tight rounded font-normal shadow left-1/2 top-full transform -translate-x-1/2 translate-y-2 opacity-0"><div class="absolute bottom-full left-1/2 transform -translate-x-1/2 w-0 h-0 border-black border-4 border-t-0" style="border-left-color: transparent; border-right-color: transparent; "></div> Copied</div></button></div> <pre class=""><!-- HTML_TAG_START --><span class="hljs-meta">&gt;&gt;&gt; </span>my_model.generate(**inputs, num_beams=<span class="hljs-number">4</span>, do_sample=<span class="hljs-literal">True</span>)<!-- HTML_TAG_END --></pre></div> <p data-svelte-h="svelte-t9z639">Even if the default decoding strategy mostly works for your task, you can still tweak a few things. Some of the
commonly adjusted parameters include:</p> <ul data-svelte-h="svelte-oqg148"><li><code>max_new_tokens</code>: the maximum number of tokens to generate. In other words, the size of the output sequence, not
including the tokens in the prompt. As an alternative to using the outputâ€™s length as a stopping criteria, you can choose
to stop generation whenever the full generation exceeds some amount of time. To learn more, check <a href="/docs/transformers/v4.46.3/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>.</li> <li><code>num_beams</code>: by specifying a number of beams higher than 1, you are effectively switching from greedy search to
beam search. This strategy evaluates several hypotheses at each time step and eventually chooses the hypothesis that
has the overall highest probability for the entire sequence. This has the advantage of identifying high-probability
sequences that start with a lower probability initial tokens and wouldâ€™ve been ignored by the greedy search. Visualize how it works <a href="https://huggingface.co/spaces/m-ric/beam_search_visualizer" rel="nofollow">here</a>.</li> <li><code>do_sample</code>: if set to <code>True</code>, this parameter enables decoding strategies such as multinomial sampling, beam-search
multinomial sampling, Top-K sampling and Top-p sampling. All these strategies select the next token from the probability
distribution over the entire vocabulary with various strategy-specific adjustments.</li> <li><code>num_return_sequences</code>: the number of sequence candidates to return for each input. This option is only available for
the decoding strategies that support multiple sequence candidates, e.g. variations of beam search and sampling. Decoding
strategies like greedy search and contrastive search return a single output sequence.</li></ul>  <h2 class="relative group"><a id="save-a-custom-decoding-strategy-with-your-model" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#save-a-custom-decoding-strategy-with-your-model"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Save a custom decoding strategy with your model</span></h2> <p data-svelte-h="svelte-1l1ri2h">If you would like to share your fine-tuned model with a specific generation configuration, you can:</p> <ul data-svelte-h="svelte-1iyi0ki"><li>Create a <a href="/docs/transformers/v4.46.3/en/main_classes/text_generation#transformers.GenerationConfig">GenerationConfig</a> class instance</li> <li>Specify the decoding strategy parameters</li> <li>Save your generation configuration with <a href="/docs/transformers/v4.46.3/en/main_classes/text_generation#transformers.GenerationConfig.save_pretrained">GenerationConfig.save_pretrained()</a>, making sure to leave its <code>config_file_name</code> argument empty</li> <li>Set <code>push_to_hub</code> to <code>True</code> to upload your config to the modelâ€™s repo</li></ul> <div class="code-block relative"><div class="absolute top-2.5 right-4"><button class="inline-flex items-center relative text-sm focus:text-green-500 cursor-pointer focus:outline-none transition duration-200 ease-in-out opacity-0 mx-0.5   text-gray-600 " title="code excerpt" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg> <div class="absolute pointer-events-none transition-opacity bg-black text-white py-1 px-2 leading-tight rounded font-normal shadow left-1/2 top-full transform -translate-x-1/2 translate-y-2 opacity-0"><div class="absolute bottom-full left-1/2 transform -translate-x-1/2 w-0 h-0 border-black border-4 border-t-0" style="border-left-color: transparent; border-right-color: transparent; "></div> Copied</div></button></div> <pre class=""><!-- HTML_TAG_START --><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, GenerationConfig

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;my_account/my_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>generation_config = GenerationConfig(
<span class="hljs-meta">... </span>    max_new_tokens=<span class="hljs-number">50</span>, do_sample=<span class="hljs-literal">True</span>, top_k=<span class="hljs-number">50</span>, eos_token_id=model.config.eos_token_id
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>generation_config.save_pretrained(<span class="hljs-string">&quot;my_account/my_model&quot;</span>, push_to_hub=<span class="hljs-literal">True</span>)<!-- HTML_TAG_END --></pre></div> <p data-svelte-h="svelte-1fak8zj">You can also store several generation configurations in a single directory, making use of the <code>config_file_name</code>
argument in <a href="/docs/transformers/v4.46.3/en/main_classes/text_generation#transformers.GenerationConfig.save_pretrained">GenerationConfig.save_pretrained()</a>. You can later instantiate them with <a href="/docs/transformers/v4.46.3/en/main_classes/text_generation#transformers.GenerationConfig.from_pretrained">GenerationConfig.from_pretrained()</a>. This is useful if you want to
store several generation configurations for a single model (e.g. one for creative text generation with sampling, and
one for summarization with beam search). You must have the right Hub permissions to add configuration files to a model.</p> <div class="code-block relative"><div class="absolute top-2.5 right-4"><button class="inline-flex items-center relative text-sm focus:text-green-500 cursor-pointer focus:outline-none transition duration-200 ease-in-out opacity-0 mx-0.5   text-gray-600 " title="code excerpt" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg> <div class="absolute pointer-events-none transition-opacity bg-black text-white py-1 px-2 leading-tight rounded font-normal shadow left-1/2 top-full transform -translate-x-1/2 translate-y-2 opacity-0"><div class="absolute bottom-full left-1/2 transform -translate-x-1/2 w-0 h-0 border-black border-4 border-t-0" style="border-left-color: transparent; border-right-color: transparent; "></div> Copied</div></button></div> <pre class=""><!-- HTML_TAG_START --><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;google-t5/t5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;google-t5/t5-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>translation_generation_config = GenerationConfig(
<span class="hljs-meta">... </span>    num_beams=<span class="hljs-number">4</span>,
<span class="hljs-meta">... </span>    early_stopping=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    decoder_start_token_id=<span class="hljs-number">0</span>,
<span class="hljs-meta">... </span>    eos_token_id=model.config.eos_token_id,
<span class="hljs-meta">... </span>    pad_token=model.config.pad_token_id,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Tip: add `push_to_hub=True` to push to the Hub</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>translation_generation_config.save_pretrained(<span class="hljs-string">&quot;/tmp&quot;</span>, <span class="hljs-string">&quot;translation_generation_config.json&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># You could then use the named generation config file to parameterize generation</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generation_config = GenerationConfig.from_pretrained(<span class="hljs-string">&quot;/tmp&quot;</span>, <span class="hljs-string">&quot;translation_generation_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;translate English to French: Configuration files are easy to use!&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(**inputs, generation_config=generation_config)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>))
[<span class="hljs-string">&#x27;Les fichiers de configuration sont faciles Ã  utiliser!&#x27;</span>]<!-- HTML_TAG_END --></pre></div>  <h2 class="relative group"><a id="streaming" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#streaming"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Streaming</span></h2> <p data-svelte-h="svelte-12rh7l9">The <code>generate()</code> supports streaming, through its <code>streamer</code> input. The <code>streamer</code> input is compatible with any instance
from a class that has the following methods: <code>put()</code> and <code>end()</code>. Internally, <code>put()</code> is used to push new tokens and
<code>end()</code> is used to flag the end of text generation.</p>  <div class="course-tip course-tip-orange bg-gradient-to-br dark:bg-gradient-to-r before:border-orange-500 dark:before:border-orange-800 from-orange-50 dark:from-gray-900 to-white dark:to-gray-950 border border-orange-50 text-orange-700 dark:text-gray-400"><p data-svelte-h="svelte-gv2g1g">The API for the streamer classes is still under development and may change in the future.</p></div> <p data-svelte-h="svelte-10qwvk7">In practice, you can craft your own streaming class for all sorts of purposes! We also have basic streaming classes
ready for you to use. For example, you can use the <a href="/docs/transformers/v4.46.3/en/internal/generation_utils#transformers.TextStreamer">TextStreamer</a> class to stream the output of <code>generate()</code> into
your screen, one word at a time:</p> <div class="code-block relative"><div class="absolute top-2.5 right-4"><button class="inline-flex items-center relative text-sm focus:text-green-500 cursor-pointer focus:outline-none transition duration-200 ease-in-out opacity-0 mx-0.5   text-gray-600 " title="code excerpt" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg> <div class="absolute pointer-events-none transition-opacity bg-black text-white py-1 px-2 leading-tight rounded font-normal shadow left-1/2 top-full transform -translate-x-1/2 translate-y-2 opacity-0"><div class="absolute bottom-full left-1/2 transform -translate-x-1/2 w-0 h-0 border-black border-4 border-t-0" style="border-left-color: transparent; border-right-color: transparent; "></div> Copied</div></button></div> <pre class=""><!-- HTML_TAG_START --><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, TextStreamer

<span class="hljs-meta">&gt;&gt;&gt; </span>tok = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;openai-community/gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;openai-community/gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tok([<span class="hljs-string">&quot;An increasing sequence: one,&quot;</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>streamer = TextStreamer(tok)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Despite returning the usual output, the streamer will also print the generated text to stdout.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>_ = model.generate(**inputs, streamer=streamer, max_new_tokens=<span class="hljs-number">20</span>)
An increasing sequence: one, two, three, four, five, six, seven, eight, nine, ten, eleven,<!-- HTML_TAG_END --></pre></div>  <h2 class="relative group"><a id="watermarking" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#watermarking"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Watermarking</span></h2> <p data-svelte-h="svelte-10ws557">The <code>generate()</code> supports watermarking the generated text by randomly marking a portion of tokens as â€œgreenâ€.
When generating the â€œgreenâ€ will have a small â€˜biasâ€™ value added to their logits, thus having a higher chance to be generated.
The watermarked text can be detected by calculating the proportion of â€œgreenâ€ tokens in the text and estimating how likely it is
statistically to obtain that amount of â€œgreenâ€ tokens for human-generated text. This watermarking strategy was proposed in the paper
<a href="https://arxiv.org/abs/2306.04634" rel="nofollow">â€œOn the Reliability of Watermarks for Large Language Modelsâ€</a>. For more information on
the inner functioning of watermarking, it is recommended to refer to the paper.</p> <p data-svelte-h="svelte-1kfn7ka">The watermarking can be used with any generative model in <code>tranformers</code> and does not require an extra classification model
to detect watermarked text. To trigger watermarking, pass in a <a href="/docs/transformers/v4.46.3/en/internal/generation_utils#transformers.WatermarkingConfig">WatermarkingConfig</a> with needed arguments directly to the
<code>.generate()</code> method or add it to the <a href="/docs/transformers/v4.46.3/en/main_classes/text_generation#transformers.GenerationConfig">GenerationConfig</a>. Watermarked text can be later detected with a <a href="/docs/transformers/v4.46.3/en/internal/generation_utils#transformers.WatermarkDetector">WatermarkDetector</a>.</p>  <div class="course-tip course-tip-orange bg-gradient-to-br dark:bg-gradient-to-r before:border-orange-500 dark:before:border-orange-800 from-orange-50 dark:from-gray-900 to-white dark:to-gray-950 border border-orange-50 text-orange-700 dark:text-gray-400"><p data-svelte-h="svelte-ge8sma">The WatermarkDetector internally relies on the proportion of â€œgreenâ€ tokens, and whether generated text follows the coloring pattern.
That is why it is recommended to strip off the prompt text, if it is much longer than the generated text.
This also can have an effect when one sequence in the batch is a lot longer causing other rows to be padded.
Additionally, the detector <strong>must</strong> be initiated with identical watermark configuration arguments used when generating.</p></div> <p data-svelte-h="svelte-ba6d9u">Letâ€™s generate some text with watermarking. In the below code snippet, we set the bias to 2.5 which is a value that
will be added to â€œgreenâ€ tokensâ€™ logits. After generating watermarked text, we can pass it directly to the <code>WatermarkDetector</code>
to check if the text is machine-generated (outputs <code>True</code> for machine-generated and <code>False</code> otherwise).</p> <div class="code-block relative"><div class="absolute top-2.5 right-4"><button class="inline-flex items-center relative text-sm focus:text-green-500 cursor-pointer focus:outline-none transition duration-200 ease-in-out opacity-0 mx-0.5   text-gray-600 " title="code excerpt" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg> <div class="absolute pointer-events-none transition-opacity bg-black text-white py-1 px-2 leading-tight rounded font-normal shadow left-1/2 top-full transform -translate-x-1/2 translate-y-2 opacity-0"><div class="absolute bottom-full left-1/2 transform -translate-x-1/2 w-0 h-0 border-black border-4 border-t-0" style="border-left-color: transparent; border-right-color: transparent; "></div> Copied</div></button></div> <pre class=""><!-- HTML_TAG_START --><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM, WatermarkDetector, WatermarkingConfig

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;openai-community/gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tok = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;openai-community/gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tok.pad_token_id = tok.eos_token_id
<span class="hljs-meta">&gt;&gt;&gt; </span>tok.padding_side = <span class="hljs-string">&quot;left&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tok([<span class="hljs-string">&quot;This is the beginning of a long story&quot;</span>, <span class="hljs-string">&quot;Alice and Bob are&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_len = inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].shape[-<span class="hljs-number">1</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>watermarking_config = WatermarkingConfig(bias=<span class="hljs-number">2.5</span>, seeding_scheme=<span class="hljs-string">&quot;selfhash&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>out = model.generate(**inputs, watermarking_config=watermarking_config, do_sample=<span class="hljs-literal">False</span>, max_length=<span class="hljs-number">20</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>detector = WatermarkDetector(model_config=model.config, device=<span class="hljs-string">&quot;cpu&quot;</span>, watermarking_config=watermarking_config)
<span class="hljs-meta">&gt;&gt;&gt; </span>detection_out = detector(out, return_dict=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>detection_out.prediction
array([<span class="hljs-literal">True</span>, <span class="hljs-literal">True</span>])<!-- HTML_TAG_END --></pre></div>  <h2 class="relative group"><a id="decoding-strategies" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#decoding-strategies"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Decoding strategies</span></h2> <p data-svelte-h="svelte-dhwbam">Certain combinations of the <code>generate()</code> parameters, and ultimately <code>generation_config</code>, can be used to enable specific
decoding strategies. If you are new to this concept, we recommend reading
<a href="https://huggingface.co/blog/how-to-generate" rel="nofollow">this blog post that illustrates how common decoding strategies work</a>.</p> <p data-svelte-h="svelte-nugt5b">Here, weâ€™ll show some of the parameters that control the decoding strategies and illustrate how you can use them.</p>  <div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400"><p data-svelte-h="svelte-1pbdq26">Selecting a given decoding strategy is not the only way you can influence the outcome of <code>generate()</code> with your model.
The decoding strategies act based (mostly) on the logits, the distribution of probabilities for the next token, and
thus selecting a good logits manipulation strategy can go a long way! In other words, manipulating the logits is another
dimension you can act upon, in addition to selecting a decoding strategy. Popular logits manipulation strategies include
<code>top_p</code>, <code>min_p</code>, and <code>repetition_penalty</code> â€” you can check the full list in the <a href="/docs/transformers/v4.46.3/en/main_classes/text_generation#transformers.GenerationConfig">GenerationConfig</a> class.</p></div>  <h3 class="relative group"><a id="greedy-search" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#greedy-search"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Greedy Search</span></h3> <p data-svelte-h="svelte-1m7rj88"><code>generate</code> uses greedy search decoding by default so you donâ€™t have to pass any parameters to enable it. This means the parameters <code>num_beams</code> is set to 1 and <code>do_sample=False</code>.</p> <div class="code-block relative"><div class="absolute top-2.5 right-4"><button class="inline-flex items-center relative text-sm focus:text-green-500 cursor-pointer focus:outline-none transition duration-200 ease-in-out opacity-0 mx-0.5   text-gray-600 " title="code excerpt" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg> <div class="absolute pointer-events-none transition-opacity bg-black text-white py-1 px-2 leading-tight rounded font-normal shadow left-1/2 top-full transform -translate-x-1/2 translate-y-2 opacity-0"><div class="absolute bottom-full left-1/2 transform -translate-x-1/2 w-0 h-0 border-black border-4 border-t-0" style="border-left-color: transparent; border-right-color: transparent; "></div> Copied</div></button></div> <pre class=""><!-- HTML_TAG_START --><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;I look forward to&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>checkpoint = <span class="hljs-string">&quot;distilbert/distilgpt2&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;I look forward to seeing you all again!\n\n\n\n\n\n\n\n\n\n\n&#x27;</span>]<!-- HTML_TAG_END --></pre></div>  <h3 class="relative group"><a id="contrastive-search" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#contrastive-search"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Contrastive search</span></h3> <p data-svelte-h="svelte-m0y9j6">The contrastive search decoding strategy was proposed in the 2022 paper <a href="https://arxiv.org/abs/2202.06417" rel="nofollow">A Contrastive Framework for Neural Text Generation</a>.
It demonstrates superior results for generating non-repetitive yet coherent long outputs. To learn how contrastive search
works, check out <a href="https://huggingface.co/blog/introducing-csearch" rel="nofollow">this blog post</a>.
The two main parameters that enable and control the behavior of contrastive search are <code>penalty_alpha</code> and <code>top_k</code>:</p> <div class="code-block relative"><div class="absolute top-2.5 right-4"><button class="inline-flex items-center relative text-sm focus:text-green-500 cursor-pointer focus:outline-none transition duration-200 ease-in-out opacity-0 mx-0.5   text-gray-600 " title="code excerpt" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg> <div class="absolute pointer-events-none transition-opacity bg-black text-white py-1 px-2 leading-tight rounded font-normal shadow left-1/2 top-full transform -translate-x-1/2 translate-y-2 opacity-0"><div class="absolute bottom-full left-1/2 transform -translate-x-1/2 w-0 h-0 border-black border-4 border-t-0" style="border-left-color: transparent; border-right-color: transparent; "></div> Copied</div></button></div> <pre class=""><!-- HTML_TAG_START --><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>checkpoint = <span class="hljs-string">&quot;openai-community/gpt2-large&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(checkpoint)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;Hugging Face Company is&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(**inputs, penalty_alpha=<span class="hljs-number">0.6</span>, top_k=<span class="hljs-number">4</span>, max_new_tokens=<span class="hljs-number">100</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Hugging Face Company is a family owned and operated business. We pride ourselves on being the best
in the business and our customer service is second to none.\n\nIf you have any questions about our
products or services, feel free to contact us at any time. We look forward to hearing from you!&#x27;</span>]<!-- HTML_TAG_END --></pre></div>  <h3 class="relative group"><a id="multinomial-sampling" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#multinomial-sampling"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Multinomial sampling</span></h3> <p data-svelte-h="svelte-vsvvis">As opposed to greedy search that always chooses a token with the highest probability as the
next token, multinomial sampling (also called ancestral sampling) randomly selects the next token based on the probability distribution over the entire
vocabulary given by the model. Every token with a non-zero probability has a chance of being selected, thus reducing the
risk of repetition.</p> <p data-svelte-h="svelte-ldtxsn">To enable multinomial sampling set <code>do_sample=True</code> and <code>num_beams=1</code>.</p> <div class="code-block relative"><div class="absolute top-2.5 right-4"><button class="inline-flex items-center relative text-sm focus:text-green-500 cursor-pointer focus:outline-none transition duration-200 ease-in-out opacity-0 mx-0.5   text-gray-600 " title="code excerpt" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg> <div class="absolute pointer-events-none transition-opacity bg-black text-white py-1 px-2 leading-tight rounded font-normal shadow left-1/2 top-full transform -translate-x-1/2 translate-y-2 opacity-0"><div class="absolute bottom-full left-1/2 transform -translate-x-1/2 w-0 h-0 border-black border-4 border-t-0" style="border-left-color: transparent; border-right-color: transparent; "></div> Copied</div></button></div> <pre class=""><!-- HTML_TAG_START --><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM, set_seed
<span class="hljs-meta">&gt;&gt;&gt; </span>set_seed(<span class="hljs-number">0</span>)  <span class="hljs-comment"># For reproducibility</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>checkpoint = <span class="hljs-string">&quot;openai-community/gpt2-large&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(checkpoint)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;Today was an amazing day because&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(**inputs, do_sample=<span class="hljs-literal">True</span>, num_beams=<span class="hljs-number">1</span>, max_new_tokens=<span class="hljs-number">100</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&quot;Today was an amazing day because we received these wonderful items by the way of a gift shop. The box arrived on a Thursday and I opened it on Monday afternoon to receive the gifts. Both bags featured pieces from all the previous years!\n\nThe box had lots of surprises in it, including some sweet little mini chocolate chips! I don&#x27;t think I&#x27;d eat all of these. This was definitely one of the most expensive presents I have ever got, I actually got most of them for free!\n\nThe first package came&quot;</span>]<!-- HTML_TAG_END --></pre></div>  <h3 class="relative group"><a id="beam-search-decoding" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#beam-search-decoding"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Beam-search decoding</span></h3> <p data-svelte-h="svelte-149ek3p">Unlike greedy search, beam-search decoding keeps several hypotheses at each time step and eventually chooses
the hypothesis that has the overall highest probability for the entire sequence. This has the advantage of identifying high-probability
sequences that start with lower probability initial tokens and wouldâ€™ve been ignored by the greedy search.</p> <a href="https://huggingface.co/spaces/m-ric/beam_search_visualizer" class="flex flex-col justify-center" data-svelte-h="svelte-1ck2qcm"><img style="max-width: 90%; margin: auto;" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/beam_search.png"></a> <p data-svelte-h="svelte-1ymxnsd">You can visualize how beam-search decoding works in <a href="https://huggingface.co/spaces/m-ric/beam_search_visualizer" rel="nofollow">this interactive demo</a>: type your input sentence, and play with the parameters to see how the decoding beams change.</p> <p data-svelte-h="svelte-krswod">To enable this decoding strategy, specify the <code>num_beams</code> (aka number of hypotheses to keep track of) that is greater than 1.</p> <div class="code-block relative"><div class="absolute top-2.5 right-4"><button class="inline-flex items-center relative text-sm focus:text-green-500 cursor-pointer focus:outline-none transition duration-200 ease-in-out opacity-0 mx-0.5   text-gray-600 " title="code excerpt" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg> <div class="absolute pointer-events-none transition-opacity bg-black text-white py-1 px-2 leading-tight rounded font-normal shadow left-1/2 top-full transform -translate-x-1/2 translate-y-2 opacity-0"><div class="absolute bottom-full left-1/2 transform -translate-x-1/2 w-0 h-0 border-black border-4 border-t-0" style="border-left-color: transparent; border-right-color: transparent; "></div> Copied</div></button></div> <pre class=""><!-- HTML_TAG_START --><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;It is astonishing how one can&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>checkpoint = <span class="hljs-string">&quot;openai-community/gpt2-medium&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(checkpoint)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(**inputs, num_beams=<span class="hljs-number">5</span>, max_new_tokens=<span class="hljs-number">50</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;It is astonishing how one can have such a profound impact on the lives of so many people in such a short period of
time.&quot;\n\nHe added: &quot;I am very proud of the work I have been able to do in the last few years.\n\n&quot;I have&#x27;</span>]<!-- HTML_TAG_END --></pre></div>  <h3 class="relative group"><a id="beam-search-multinomial-sampling" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#beam-search-multinomial-sampling"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Beam-search multinomial sampling</span></h3> <p data-svelte-h="svelte-zgjlvh">As the name implies, this decoding strategy combines beam search with multinomial sampling. You need to specify
the <code>num_beams</code> greater than 1, and set <code>do_sample=True</code> to use this decoding strategy.</p> <div class="code-block relative"><div class="absolute top-2.5 right-4"><button class="inline-flex items-center relative text-sm focus:text-green-500 cursor-pointer focus:outline-none transition duration-200 ease-in-out opacity-0 mx-0.5   text-gray-600 " title="code excerpt" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg> <div class="absolute pointer-events-none transition-opacity bg-black text-white py-1 px-2 leading-tight rounded font-normal shadow left-1/2 top-full transform -translate-x-1/2 translate-y-2 opacity-0"><div class="absolute bottom-full left-1/2 transform -translate-x-1/2 w-0 h-0 border-black border-4 border-t-0" style="border-left-color: transparent; border-right-color: transparent; "></div> Copied</div></button></div> <pre class=""><!-- HTML_TAG_START --><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSeq2SeqLM, set_seed
<span class="hljs-meta">&gt;&gt;&gt; </span>set_seed(<span class="hljs-number">0</span>)  <span class="hljs-comment"># For reproducibility</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;translate English to German: The house is wonderful.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>checkpoint = <span class="hljs-string">&quot;google-t5/t5-small&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(**inputs, num_beams=<span class="hljs-number">5</span>, do_sample=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(outputs[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-string">&#x27;Das Haus ist wunderbar.&#x27;</span><!-- HTML_TAG_END --></pre></div>  <h3 class="relative group"><a id="diverse-beam-search-decoding" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#diverse-beam-search-decoding"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Diverse beam search decoding</span></h3> <p data-svelte-h="svelte-zdhf65">The diverse beam search decoding strategy is an extension of the beam search strategy that allows for generating a more diverse
set of beam sequences to choose from. To learn how it works, refer to <a href="https://arxiv.org/pdf/1610.02424.pdf" rel="nofollow">Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models</a>.
This approach has three main parameters: <code>num_beams</code>, <code>num_beam_groups</code>, and <code>diversity_penalty</code>.
The diversity penalty ensures the outputs are distinct across groups, and beam search is used within each group.</p> <div class="code-block relative"><div class="absolute top-2.5 right-4"><button class="inline-flex items-center relative text-sm focus:text-green-500 cursor-pointer focus:outline-none transition duration-200 ease-in-out opacity-0 mx-0.5   text-gray-600 " title="code excerpt" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg> <div class="absolute pointer-events-none transition-opacity bg-black text-white py-1 px-2 leading-tight rounded font-normal shadow left-1/2 top-full transform -translate-x-1/2 translate-y-2 opacity-0"><div class="absolute bottom-full left-1/2 transform -translate-x-1/2 w-0 h-0 border-black border-4 border-t-0" style="border-left-color: transparent; border-right-color: transparent; "></div> Copied</div></button></div> <pre class=""><!-- HTML_TAG_START --><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span>checkpoint = <span class="hljs-string">&quot;google/pegasus-xsum&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = (
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;The Permaculture Design Principles are a set of universal design principles &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;that can be applied to any location, climate and culture, and they allow us to design &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;the most efficient and sustainable human habitation and food production systems. &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Permaculture is a design system that encompasses a wide variety of disciplines, such &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;as ecology, landscape design, environmental science and energy conservation, and the &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Permaculture design principles are drawn from these various disciplines. Each individual &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;design principle itself embodies a complete conceptual framework based on sound &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;scientific principles. When we bring all these separate  principles together, we can &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;create a design system that both looks at whole systems, the parts that these systems &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;consist of, and how those parts interact with each other to create a complex, dynamic, &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;living system. Each design principle serves as a tool that allows us to integrate all &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;the separate parts of a design, referred to as elements, into a functional, synergistic, &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;whole system, where the elements harmoniously interact and work together in the most &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;efficient way possible.&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(**inputs, num_beams=<span class="hljs-number">5</span>, num_beam_groups=<span class="hljs-number">5</span>, max_new_tokens=<span class="hljs-number">30</span>, diversity_penalty=<span class="hljs-number">1.0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(outputs[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-string">&#x27;The Design Principles are a set of universal design principles that can be applied to any location, climate and
culture, and they allow us to design the&#x27;</span><!-- HTML_TAG_END --></pre></div> <p data-svelte-h="svelte-1e7mt8t">This guide illustrates the main parameters that enable various decoding strategies. More advanced parameters exist for the
<code>generate</code> method, which gives you even further control over the <code>generate</code> methodâ€™s behavior.
For the complete list of the available parameters, refer to the <a href="./main_classes/text_generation.md">API documentation</a>.</p>  <h3 class="relative group"><a id="speculative-decoding" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#speculative-decoding"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Speculative Decoding</span></h3> <p data-svelte-h="svelte-1ddqaeo">Speculative decoding (also known as assisted decoding) is a modification of the decoding strategies above, that uses an
assistant model (ideally a much smaller one), to generate a few candidate tokens. The main model then validates the candidate
tokens in a single forward pass, which speeds up the decoding process. If <code>do_sample=True</code>, then the token validation with
resampling introduced in the <a href="https://arxiv.org/pdf/2211.17192.pdf" rel="nofollow">speculative decoding paper</a> is used.
Assisted decoding assumes the main and assistant models have the same tokenizer, otherwise, see Universal Assisted Decoding below.</p> <p data-svelte-h="svelte-xy91hv">Currently, only greedy search and sampling are supported with assisted decoding, and assisted decoding doesnâ€™t support batched inputs.
To learn more about assisted decoding, check <a href="https://huggingface.co/blog/assisted-generation" rel="nofollow">this blog post</a>.</p>  <h4 class="relative group"><a id="universal-assisted-decoding" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#universal-assisted-decoding"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Universal Assisted Decoding</span></h4> <p data-svelte-h="svelte-zp0z7a">Universal Assisted Decoding (UAD) adds support for main and assistant models with different tokenizers.
To use it, simply pass the tokenizers using the <code>tokenizer</code> and <code>assistant_tokenizer</code> arguments (see below).
Internally, the main model input tokens are re-encoded into assistant model tokens, then candidate tokens are generated in the assistant encoding, which are
in turn re-encoded into main model candidate tokens. Validation then proceeds as explained above.
The re-encoding steps involve decoding token ids into text and then encoding the text using a different tokenizer.
Since re-encoding the tokens may result in tokenization discrepancies, UAD finds the longest common subsequence between the source and target encodings,
to ensure the new tokens include the correct prompt suffix.</p> <p data-svelte-h="svelte-ebd3ly">To enable assisted decoding, set the <code>assistant_model</code> argument with a model.</p> <div class="code-block relative"><div class="absolute top-2.5 right-4"><button class="inline-flex items-center relative text-sm focus:text-green-500 cursor-pointer focus:outline-none transition duration-200 ease-in-out opacity-0 mx-0.5   text-gray-600 " title="code excerpt" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg> <div class="absolute pointer-events-none transition-opacity bg-black text-white py-1 px-2 leading-tight rounded font-normal shadow left-1/2 top-full transform -translate-x-1/2 translate-y-2 opacity-0"><div class="absolute bottom-full left-1/2 transform -translate-x-1/2 w-0 h-0 border-black border-4 border-t-0" style="border-left-color: transparent; border-right-color: transparent; "></div> Copied</div></button></div> <pre class=""><!-- HTML_TAG_START --><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;Alice and Bob&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>checkpoint = <span class="hljs-string">&quot;EleutherAI/pythia-1.4b-deduped&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>assistant_checkpoint = <span class="hljs-string">&quot;EleutherAI/pythia-160m-deduped&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(**inputs, assistant_model=assistant_model)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Alice and Bob are sitting in a bar. Alice is drinking a beer and Bob is drinking a&#x27;</span>]<!-- HTML_TAG_END --></pre></div> <p data-svelte-h="svelte-1c4jyzw">If the main and assistant models have different tokenizers, use Universal Assisted Decoding.</p> <div class="code-block relative"><div class="absolute top-2.5 right-4"><button class="inline-flex items-center relative text-sm focus:text-green-500 cursor-pointer focus:outline-none transition duration-200 ease-in-out opacity-0 mx-0.5   text-gray-600 " title="code excerpt" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg> <div class="absolute pointer-events-none transition-opacity bg-black text-white py-1 px-2 leading-tight rounded font-normal shadow left-1/2 top-full transform -translate-x-1/2 translate-y-2 opacity-0"><div class="absolute bottom-full left-1/2 transform -translate-x-1/2 w-0 h-0 border-black border-4 border-t-0" style="border-left-color: transparent; border-right-color: transparent; "></div> Copied</div></button></div> <pre class=""><!-- HTML_TAG_START --><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;Alice and Bob&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>checkpoint = <span class="hljs-string">&quot;google/gemma-2-9b&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>assistant_checkpoint = <span class="hljs-string">&quot;double7/vicuna-68m&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>assistant_tokenizer = AutoTokenizer.from_pretrained(assistant_checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(**inputs, assistant_model=assistant_model, tokenizer=tokenizer, assistant_tokenizer=assistant_tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Alice and Bob are sitting in a bar. Alice is drinking a beer and Bob is drinking a&#x27;</span>]<!-- HTML_TAG_END --></pre></div> <p data-svelte-h="svelte-nm0fxt">When using assisted decoding with sampling methods, you can use the <code>temperature</code> argument to control the randomness,
just like in multinomial sampling. However, in assisted decoding, reducing the temperature may help improve the latency.</p> <div class="code-block relative"><div class="absolute top-2.5 right-4"><button class="inline-flex items-center relative text-sm focus:text-green-500 cursor-pointer focus:outline-none transition duration-200 ease-in-out opacity-0 mx-0.5   text-gray-600 " title="code excerpt" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg> <div class="absolute pointer-events-none transition-opacity bg-black text-white py-1 px-2 leading-tight rounded font-normal shadow left-1/2 top-full transform -translate-x-1/2 translate-y-2 opacity-0"><div class="absolute bottom-full left-1/2 transform -translate-x-1/2 w-0 h-0 border-black border-4 border-t-0" style="border-left-color: transparent; border-right-color: transparent; "></div> Copied</div></button></div> <pre class=""><!-- HTML_TAG_START --><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, set_seed
<span class="hljs-meta">&gt;&gt;&gt; </span>set_seed(<span class="hljs-number">42</span>)  <span class="hljs-comment"># For reproducibility</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;Alice and Bob&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>checkpoint = <span class="hljs-string">&quot;EleutherAI/pythia-1.4b-deduped&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>assistant_checkpoint = <span class="hljs-string">&quot;EleutherAI/pythia-160m-deduped&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(**inputs, assistant_model=assistant_model, do_sample=<span class="hljs-literal">True</span>, temperature=<span class="hljs-number">0.5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Alice and Bob, a couple of friends of mine, who are both in the same office as&#x27;</span>]<!-- HTML_TAG_END --></pre></div> <p data-svelte-h="svelte-n7i3t9">Alternatively, you can also set the <code>prompt_lookup_num_tokens</code> to trigger n-gram based assisted decoding, as opposed
to model based assisted decoding. You can read more about it <a href="https://twitter.com/joao_gante/status/1747322413006643259" rel="nofollow">here</a>.</p>  <h3 class="relative group"><a id="dola-decoding" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#dola-decoding"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>DoLa Decoding</span></h3> <p data-svelte-h="svelte-c1qhny"><strong>D</strong>ecoding by C<strong>o</strong>ntrasting <strong>La</strong>yers (DoLa) is a contrastive decoding strategy to improve the factuality and reduce the
hallucinations of LLMs, as described in this paper of ICLR 2024 <a href="https://arxiv.org/abs/2309.03883" rel="nofollow">DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models</a>.</p> <p data-svelte-h="svelte-1fok5rt">DoLa is achieved by contrasting the differences in logits obtained from final
layers versus earlier layers, thus amplify the factual knowledge localized to particular part of transformer layers.</p> <p data-svelte-h="svelte-1xb5ka9">Do the following two steps to activate DoLa decoding when calling the <code>model.generate</code> function:</p> <ol data-svelte-h="svelte-1rdm431"><li>Set the <code>dola_layers</code> argument, which can be either a string or a list of integers.<ul><li>If set to a string, it can be one of <code>low</code>, <code>high</code>.</li> <li>If set to a list of integers, it should be a list of layer indices between 0 and the total number of layers in the model. The 0-th layer is word embedding, and the 1st layer is the first transformer layer, and so on.</li></ul></li> <li>Set <code>repetition_penalty = 1.2</code> is suggested to reduce repetition in DoLa decoding.</li></ol> <p data-svelte-h="svelte-17tp6io">See the following examples for DoLa decoding with the 32-layer LLaMA-7B model.</p> <div class="code-block relative"><div class="absolute top-2.5 right-4"><button class="inline-flex items-center relative text-sm focus:text-green-500 cursor-pointer focus:outline-none transition duration-200 ease-in-out opacity-0 mx-0.5   text-gray-600 " title="code excerpt" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg> <div class="absolute pointer-events-none transition-opacity bg-black text-white py-1 px-2 leading-tight rounded font-normal shadow left-1/2 top-full transform -translate-x-1/2 translate-y-2 opacity-0"><div class="absolute bottom-full left-1/2 transform -translate-x-1/2 w-0 h-0 border-black border-4 border-t-0" style="border-left-color: transparent; border-right-color: transparent; "></div> Copied</div></button></div> <pre class=""><!-- HTML_TAG_START --><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM, set_seed
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;huggyllama/llama-7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;huggyllama/llama-7b&quot;</span>, torch_dtype=torch.float16)
<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>set_seed(<span class="hljs-number">42</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>text = <span class="hljs-string">&quot;On what date was the Declaration of Independence officially signed?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device)

<span class="hljs-comment"># Vanilla greddy decoding</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vanilla_output = model.generate(**inputs, do_sample=<span class="hljs-literal">False</span>, max_new_tokens=<span class="hljs-number">50</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(vanilla_output[:, inputs.input_ids.shape[-<span class="hljs-number">1</span>]:], skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;\nThe Declaration of Independence was signed on July 4, 1776.\nWhat was the date of the signing of the Declaration of Independence?\nThe Declaration of Independence was signed on July 4,&#x27;</span>]

<span class="hljs-comment"># DoLa decoding with contrasting higher part of layers (layers 16,18,...,30)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dola_high_output = model.generate(**inputs, do_sample=<span class="hljs-literal">False</span>, max_new_tokens=<span class="hljs-number">50</span>, dola_layers=<span class="hljs-string">&#x27;high&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(dola_high_output[:, inputs.input_ids.shape[-<span class="hljs-number">1</span>]:], skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;\nJuly 4, 1776, when the Continental Congress voted to separate from Great Britain. The 56 delegates to the Continental Congress signed the Declaration on August 2, 1776.&#x27;</span>]

<span class="hljs-comment"># DoLa decoding with contrasting specific layers (layers 28 and 30)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dola_custom_output = model.generate(**inputs, do_sample=<span class="hljs-literal">False</span>, max_new_tokens=<span class="hljs-number">50</span>, dola_layers=[<span class="hljs-number">28</span>,<span class="hljs-number">30</span>], repetition_penalty=<span class="hljs-number">1.2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(dola_custom_output[:, inputs.input_ids.shape[-<span class="hljs-number">1</span>]:], skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;\nIt was officially signed on 2 August 1776, when 56 members of the Second Continental Congress, representing the original 13 American colonies, voted unanimously for the resolution for independence. The 2&#x27;</span>]<!-- HTML_TAG_END --></pre></div>  <h4 class="relative group"><a id="understanding-the-dolalayers-argument" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#understanding-the-dolalayers-argument"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Understanding the dola_layers argument</span></h4> <p data-svelte-h="svelte-qgkuwa"><code>dola_layers</code> stands for the candidate layers in premature layer selection, as described in the DoLa paper. The selected premature layer will be contrasted with the final layer.</p> <p data-svelte-h="svelte-13a3h6h">Setting <code>dola_layers</code> to <code>&#39;low&#39;</code> or <code>&#39;high&#39;</code> will select the lower or higher part of the layers to contrast, respectively.</p> <ul data-svelte-h="svelte-bhw0w6"><li>For <code>N</code>-layer models with <code>N &lt;= 40</code> layers, the layers of <code>range(0, N // 2, 2)</code> and <code>range(N // 2, N, 2)</code> are used for <code>&#39;low&#39;</code> and <code>&#39;high&#39;</code> layers, respectively.</li> <li>For models with <code>N &gt; 40</code> layers, the layers of <code>range(0, 20, 2)</code> and <code>range(N - 20, N, 2)</code> are used for <code>&#39;low&#39;</code> and <code>&#39;high&#39;</code> layers, respectively.</li> <li>If the model has tied word embeddings, we skip the word embeddings (0-th) layer and start from the 2nd layer, as the early exit from word embeddings will become identity function.</li> <li>Set the <code>dola_layers</code> to a list of integers for layer indices to contrast manually specified layers. For example, setting <code>dola_layers=[28,30]</code> will contrast the final layer (32-th layer) with the 28-th and 30-th layers.</li></ul> <p data-svelte-h="svelte-1v5aa4g">The paper suggested that contrasting <code>&#39;high&#39;</code> layers to improve short-answer tasks like TruthfulQA, and contrasting <code>&#39;low&#39;</code> layers to improve all the other long-answer reasoning tasks, such as GSM8K, StrategyQA, FACTOR, and VicunaQA. Applying DoLa to smaller models like GPT-2 is not recommended, as the results shown in the Appendix N of the paper.</p> <a class="!text-gray-400 !no-underline text-sm flex items-center not-prose mt-4" href="https://github.com/huggingface/transformers/blob/main/docs/source/en/generation_strategies.md" target="_blank"><span data-svelte-h="svelte-1kd6by1">&lt;</span> <span data-svelte-h="svelte-x0xyl0">&gt;</span> <span data-svelte-h="svelte-1dajgef"><span class="underline ml-1.5">Update</span> on GitHub</span></a>  <p></p> 
			
			<script>
				{
					__sveltekit_1haiaah = {
						assets: "/docs/transformers/v4.46.3/en",
						base: "/docs/transformers/v4.46.3/en",
						env: {}
					};

					const element = document.currentScript.parentElement;

					const data = [null,null];

					Promise.all([
						import("/docs/transformers/v4.46.3/en/_app/immutable/entry/start.0a93d74d.js"),
						import("/docs/transformers/v4.46.3/en/_app/immutable/entry/app.123a6bc1.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 22],
							data,
							form: null,
							error: null
						});
					});
				}
			</script>
		
<!-- HTML_TAG_END --></div>
				<div class="SVELTE_HYDRATER contents" data-target="DocFooterNav" data-props="{&quot;classNames&quot;:&quot;mx-auto mt-16 flex max-w-4xl items-center pb-8 font-sans font-medium leading-6 xl:mt-32&quot;,&quot;chapterPrev&quot;:{&quot;title&quot;:&quot;Video-text-to-text&quot;,&quot;id&quot;:&quot;tasks/video_text_to_text&quot;,&quot;url&quot;:&quot;/docs/transformers/tasks/video_text_to_text&quot;},&quot;chapterNext&quot;:{&quot;title&quot;:&quot;Best Practices for Generation with Cache&quot;,&quot;id&quot;:&quot;kv_cache&quot;,&quot;url&quot;:&quot;/docs/transformers/kv_cache&quot;},&quot;isCourse&quot;:false,&quot;isLoggedIn&quot;:true}"><div class="mx-auto mt-16 flex max-w-4xl items-center pb-8 font-sans font-medium leading-6 xl:mt-32"><a href="/docs/transformers/tasks/video_text_to_text" class="mr-8 flex transform items-center text-gray-600 transition-all hover:-translate-x-px hover:text-gray-900 dark:hover:text-gray-300"><span class="mr-2 translate-y-px">â†</span>Video-text-to-text</a>
	<a href="/docs/transformers/kv_cache" class="ml-auto flex transform items-center text-right text-gray-600 transition-all hover:translate-x-px hover:text-gray-900 dark:hover:text-gray-300">Best Practices for Generation with Cache<span class="ml-2 translate-y-px">â†’</span></a></div></div></div></div>
		<div class="sticky top-0 self-start"><div class="SVELTE_HYDRATER contents" data-target="SubSideMenu" data-props="{&quot;chapter&quot;:{&quot;title&quot;:&quot;Text generation strategies&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;text-generation-strategies&quot;,&quot;url&quot;:&quot;#text-generation-strategies&quot;,&quot;sections&quot;:[{&quot;title&quot;:&quot;Default text generation configuration&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;default-text-generation-configuration&quot;,&quot;url&quot;:&quot;#default-text-generation-configuration&quot;,&quot;sections&quot;:[]},{&quot;title&quot;:&quot;Customize text generation&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;customize-text-generation&quot;,&quot;url&quot;:&quot;#customize-text-generation&quot;,&quot;sections&quot;:[]},{&quot;title&quot;:&quot;Save a custom decoding strategy with your model&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;save-a-custom-decoding-strategy-with-your-model&quot;,&quot;url&quot;:&quot;#save-a-custom-decoding-strategy-with-your-model&quot;,&quot;sections&quot;:[]},{&quot;title&quot;:&quot;Streaming&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;streaming&quot;,&quot;url&quot;:&quot;#streaming&quot;,&quot;sections&quot;:[]},{&quot;title&quot;:&quot;Watermarking&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;watermarking&quot;,&quot;url&quot;:&quot;#watermarking&quot;,&quot;sections&quot;:[]},{&quot;title&quot;:&quot;Decoding strategies&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;decoding-strategies&quot;,&quot;url&quot;:&quot;#decoding-strategies&quot;,&quot;sections&quot;:[{&quot;title&quot;:&quot;Greedy Search&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;greedy-search&quot;,&quot;url&quot;:&quot;#greedy-search&quot;,&quot;sections&quot;:[]},{&quot;title&quot;:&quot;Contrastive search&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;contrastive-search&quot;,&quot;url&quot;:&quot;#contrastive-search&quot;,&quot;sections&quot;:[]},{&quot;title&quot;:&quot;Multinomial sampling&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;multinomial-sampling&quot;,&quot;url&quot;:&quot;#multinomial-sampling&quot;,&quot;sections&quot;:[]},{&quot;title&quot;:&quot;Beam-search decoding&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;beam-search-decoding&quot;,&quot;url&quot;:&quot;#beam-search-decoding&quot;,&quot;sections&quot;:[]},{&quot;title&quot;:&quot;Beam-search multinomial sampling&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;beam-search-multinomial-sampling&quot;,&quot;url&quot;:&quot;#beam-search-multinomial-sampling&quot;,&quot;sections&quot;:[]},{&quot;title&quot;:&quot;Diverse beam search decoding&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;diverse-beam-search-decoding&quot;,&quot;url&quot;:&quot;#diverse-beam-search-decoding&quot;,&quot;sections&quot;:[]},{&quot;title&quot;:&quot;Speculative Decoding&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;speculative-decoding&quot;,&quot;url&quot;:&quot;#speculative-decoding&quot;,&quot;sections&quot;:[{&quot;title&quot;:&quot;Universal Assisted Decoding&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;universal-assisted-decoding&quot;,&quot;url&quot;:&quot;#universal-assisted-decoding&quot;,&quot;sections&quot;:[]}]},{&quot;title&quot;:&quot;DoLa Decoding&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;dola-decoding&quot;,&quot;url&quot;:&quot;#dola-decoding&quot;,&quot;sections&quot;:[{&quot;title&quot;:&quot;Understanding the dola_layers argument&quot;,&quot;isExpanded&quot;:true,&quot;id&quot;:&quot;understanding-the-dolalayers-argument&quot;,&quot;url&quot;:&quot;#understanding-the-dolalayers-argument&quot;,&quot;sections&quot;:[]}]}]}]}}">

<nav class="hidden h-dvh w-[270px] flex-none flex-col space-y-3 overflow-y-auto break-words border-l pb-16 pl-6 pr-10 pt-24 text-sm lg:flex 2xl:w-[305px]">

<a href="#text-generation-strategies" class=" text-gray-400 transform hover:translate-x-px hover:text-gray-700 dark:hover:text-gray-300" id="nav-text-generation-strategies"><!-- HTML_TAG_START --><wbr>Text generation strategies<!-- HTML_TAG_END --></a>
	

<a href="#default-text-generation-configuration" class="pl-4 text-gray-400 transform hover:translate-x-px hover:text-gray-700 dark:hover:text-gray-300" id="nav-default-text-generation-configuration"><!-- HTML_TAG_START --><wbr>Default text generation configuration<!-- HTML_TAG_END --></a>
			

<a href="#customize-text-generation" class="pl-4 text-gray-400 transform hover:translate-x-px hover:text-gray-700 dark:hover:text-gray-300" id="nav-customize-text-generation"><!-- HTML_TAG_START --><wbr>Customize text generation<!-- HTML_TAG_END --></a>
			

<a href="#save-a-custom-decoding-strategy-with-your-model" class="pl-4 text-gray-400 transform hover:translate-x-px hover:text-gray-700 dark:hover:text-gray-300" id="nav-save-a-custom-decoding-strategy-with-your-model"><!-- HTML_TAG_START --><wbr>Save a custom decoding strategy with your model<!-- HTML_TAG_END --></a>
			

<a href="#streaming" class="pl-4 text-gray-400 transform hover:translate-x-px hover:text-gray-700 dark:hover:text-gray-300" id="nav-streaming"><!-- HTML_TAG_START --><wbr>Streaming<!-- HTML_TAG_END --></a>
			

<a href="#watermarking" class="pl-4 text-gray-400 transform hover:translate-x-px hover:text-gray-700 dark:hover:text-gray-300" id="nav-watermarking"><!-- HTML_TAG_START --><wbr>Watermarking<!-- HTML_TAG_END --></a>
			

<a href="#decoding-strategies" class="pl-4 text-gray-400 transform hover:translate-x-px hover:text-gray-700 dark:hover:text-gray-300" id="nav-decoding-strategies"><!-- HTML_TAG_START --><wbr>Decoding strategies<!-- HTML_TAG_END --></a>
			

<a href="#greedy-search" class="pl-8 text-gray-400 transform hover:translate-x-px hover:text-gray-700 dark:hover:text-gray-300" id="nav-greedy-search"><!-- HTML_TAG_START --><wbr>Greedy <wbr>Search<!-- HTML_TAG_END --></a>
					

<a href="#contrastive-search" class="pl-8 text-gray-400 transform hover:translate-x-px hover:text-gray-700 dark:hover:text-gray-300" id="nav-contrastive-search"><!-- HTML_TAG_START --><wbr>Contrastive search<!-- HTML_TAG_END --></a>
					

<a href="#multinomial-sampling" class="pl-8 text-gray-400 transform hover:translate-x-px hover:text-gray-700 dark:hover:text-gray-300" id="nav-multinomial-sampling"><!-- HTML_TAG_START --><wbr>Multinomial sampling<!-- HTML_TAG_END --></a>
					

<a href="#beam-search-decoding" class="pl-8 text-gray-400 transform hover:translate-x-px hover:text-gray-700 dark:hover:text-gray-300" id="nav-beam-search-decoding"><!-- HTML_TAG_START --><wbr>Beam-search decoding<!-- HTML_TAG_END --></a>
					

<a href="#beam-search-multinomial-sampling" class="pl-8 text-gray-400 transform hover:translate-x-px hover:text-gray-700 dark:hover:text-gray-300" id="nav-beam-search-multinomial-sampling"><!-- HTML_TAG_START --><wbr>Beam-search multinomial sampling<!-- HTML_TAG_END --></a>
					

<a href="#diverse-beam-search-decoding" class="pl-8 text-gray-400 transform hover:translate-x-px hover:text-gray-700 dark:hover:text-gray-300" id="nav-diverse-beam-search-decoding"><!-- HTML_TAG_START --><wbr>Diverse beam search decoding<!-- HTML_TAG_END --></a>
					

<a href="#speculative-decoding" class="pl-8 text-gray-400 transform hover:translate-x-px hover:text-gray-700 dark:hover:text-gray-300" id="nav-speculative-decoding"><!-- HTML_TAG_START --><wbr>Speculative <wbr>Decoding<!-- HTML_TAG_END --></a>
					

<a href="#universal-assisted-decoding" class="pl-12 text-xs text-gray-400 transform hover:translate-x-px hover:text-gray-700 dark:hover:text-gray-300" id="nav-universal-assisted-decoding"><!-- HTML_TAG_START --><wbr>Universal <wbr>Assisted <wbr>Decoding<!-- HTML_TAG_END --></a>

<a href="#dola-decoding" class="pl-8 text-gray-400 transform hover:translate-x-px hover:text-gray-700 dark:hover:text-gray-300" id="nav-dola-decoding"><!-- HTML_TAG_START --><wbr>Do<wbr>La <wbr>Decoding<!-- HTML_TAG_END --></a>
					

<a href="#understanding-the-dolalayers-argument" class="pl-12 text-xs text-gray-400 transform hover:translate-x-px hover:text-gray-700 dark:hover:text-gray-300" id="nav-understanding-the-dolalayers-argument"><!-- HTML_TAG_START --><wbr>Understanding the dola_layers argument<!-- HTML_TAG_END --></a></nav></div></div></div>
	<div id="doc-footer"></div></main>

	</div>

		<script>
			import("\/front\/build\/kube-8e721ee\/index.js");
			window.moonSha = "kube-8e721ee\/";
			window.__hf_deferred = {};
		</script>

		<!-- Stripe -->
		<script>
			if (["hf.co", "huggingface.co"].includes(window.location.hostname)) {
				const script = document.createElement("script");
				script.src = "https://js.stripe.com/v3/";
				script.async = true;
				document.head.appendChild(script);
			}
		</script>
	</body>
</html>
