<!doctype html><html lang="en"><head><title data-rh="true">The FLOPs Calculus of Language Model Training | by Dzmitry Bahdanau | Medium</title><meta data-rh="true" charset="utf-8"/><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1,maximum-scale=1"/><meta data-rh="true" name="theme-color" content="#000000"/><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"/><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"/><meta data-rh="true" property="al:ios:app_name" content="Medium"/><meta data-rh="true" property="al:ios:app_store_id" content="828256236"/><meta data-rh="true" property="al:android:package" content="com.medium.reader"/><meta data-rh="true" property="fb:app_id" content="542599432471018"/><meta data-rh="true" property="og:site_name" content="Medium"/><meta data-rh="true" property="og:type" content="article"/><meta data-rh="true" property="article:published_time" content="2022-12-19T21:57:56.039Z"/><meta data-rh="true" name="title" content="The FLOPs Calculus of Language Model Training | by Dzmitry Bahdanau | Medium"/><meta data-rh="true" property="og:title" content="The FLOPs Calculus of Language Model Training"/><meta data-rh="true" property="al:android:url" content="medium://p/3b19c1f025e4"/><meta data-rh="true" property="al:ios:url" content="medium://p/3b19c1f025e4"/><meta data-rh="true" property="al:android:app_name" content="Medium"/><meta data-rh="true" name="description" content="Extremely large language models like the famous GPT-3 by OpenAI are all the rage. Many of us are now trying to get a sense of scale of the compute that goes into training them. In this article, I…"/><meta data-rh="true" property="og:description" content="Extremely large language models like the famous GPT-3 by OpenAI are all the rage. Many of us are now trying to get a sense of scale of the…"/><meta data-rh="true" property="og:url" content="https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4"/><meta data-rh="true" property="al:web:url" content="https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4"/><meta data-rh="true" property="og:image" content="https://miro.medium.com/v2/resize:fit:1024/0*sG59WF5K7qLZr307.jpg"/><meta data-rh="true" property="article:author" content="https://medium.com/@dzmitrybahdanau"/><meta data-rh="true" name="author" content="Dzmitry Bahdanau"/><meta data-rh="true" name="robots" content="index,noarchive,follow,max-image-preview:large"/><meta data-rh="true" name="referrer" content="unsafe-url"/><meta data-rh="true" property="twitter:title" content="The FLOPs Calculus of Language Model Training"/><meta data-rh="true" name="twitter:site" content="@Medium"/><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/3b19c1f025e4"/><meta data-rh="true" property="twitter:description" content="Extremely large language models like the famous GPT-3 by OpenAI are all the rage. Many of us are now trying to get a sense of scale of the…"/><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/v2/resize:fit:1024/0*sG59WF5K7qLZr307.jpg"/><meta data-rh="true" name="twitter:card" content="summary_large_image"/><meta data-rh="true" name="twitter:label1" content="Reading time"/><meta data-rh="true" name="twitter:data1" content="9 min read"/><link data-rh="true" rel="icon" href="https://miro.medium.com/v2/5d8de952517e8160e40ef9841c781cdc14a5db313057fa3c3de41c6f5b494b19"/><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="/osd.xml"/><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://miro.medium.com/v2/resize:fill:304:304/10fd5c419ac61637245384e7099e131627900034828f4f386bdaa47a74eae156"/><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://miro.medium.com/v2/resize:fill:240:240/10fd5c419ac61637245384e7099e131627900034828f4f386bdaa47a74eae156"/><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://miro.medium.com/v2/resize:fill:152:152/10fd5c419ac61637245384e7099e131627900034828f4f386bdaa47a74eae156"/><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://miro.medium.com/v2/resize:fill:120:120/10fd5c419ac61637245384e7099e131627900034828f4f386bdaa47a74eae156"/><link data-rh="true" rel="mask-icon" href="https://miro.medium.com/v2/resize:fill:1000:1000/7*GAOKVe--MXbEJmV9230oOQ.png" color="#171717"/><link data-rh="true" rel="preconnect" href="https://glyph.medium.com" crossOrigin=""/><link data-rh="true" id="glyph_preload_link" rel="preload" as="style" type="text/css" href="https://glyph.medium.com/css/unbound.css"/><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="https://glyph.medium.com/css/unbound.css"/><link data-rh="true" rel="author" href="https://medium.com/@dzmitrybahdanau"/><link data-rh="true" rel="canonical" href="https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4"/><link data-rh="true" rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/3b19c1f025e4"/><script data-rh="true" type="application/ld+json">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":["https:\u002F\u002Fmiro.medium.com\u002Fv2\u002Fresize:fit:1200\u002F0*sG59WF5K7qLZr307.jpg"],"url":"https:\u002F\u002Fmedium.com\u002F@dzmitrybahdanau\u002Fthe-flops-calculus-of-language-model-training-3b19c1f025e4","dateCreated":"2022-01-09T19:20:09.419Z","datePublished":"2022-01-09T19:20:09.419Z","dateModified":"2022-12-19T21:58:05.469Z","headline":"The FLOPs Calculus of Language Model Training - Dzmitry Bahdanau - Medium","name":"The FLOPs Calculus of Language Model Training - Dzmitry Bahdanau - Medium","description":"Extremely large language models like the famous GPT-3 by OpenAI are all the rage. Many of us are now trying to get a sense of scale of the compute that goes into training them. In this article, I…","identifier":"3b19c1f025e4","author":{"@type":"Person","name":"Dzmitry Bahdanau","url":"https:\u002F\u002Fmedium.com\u002F@dzmitrybahdanau"},"creator":["Dzmitry Bahdanau"],"publisher":{"@type":"Organization","name":"Medium","url":"https:\u002F\u002Fmedium.com\u002F","logo":{"@type":"ImageObject","width":272,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fv2\u002Fresize:fit:544\u002F7*V1_7XP4snlmqrc_0Njontw.png"}},"mainEntityOfPage":"https:\u002F\u002Fmedium.com\u002F@dzmitrybahdanau\u002Fthe-flops-calculus-of-language-model-training-3b19c1f025e4"}</script><style type="text/css" data-fela-rehydration="567" data-fela-type="STATIC">html{box-sizing:border-box;-webkit-text-size-adjust:100%}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}:root{--reach-tabs:1;--reach-menu-button:1}#speechify-root{font-family:Sohne, sans-serif}div[data-popper-reference-hidden="true"]{visibility:hidden;pointer-events:none}.grecaptcha-badge{visibility:hidden}
/*XCode style (c) Angel Garcia <angelgarcia.mail@gmail.com>*/.hljs {background: #fff;color: black;
}/* Gray DOCTYPE selectors like WebKit */
.xml .hljs-meta {color: #c0c0c0;
}.hljs-comment,
.hljs-quote {color: #007400;
}.hljs-tag,
.hljs-attribute,
.hljs-keyword,
.hljs-selector-tag,
.hljs-literal,
.hljs-name {color: #aa0d91;
}.hljs-variable,
.hljs-template-variable {color: #3F6E74;
}.hljs-code,
.hljs-string,
.hljs-meta .hljs-string {color: #c41a16;
}.hljs-regexp,
.hljs-link {color: #0E0EFF;
}.hljs-title,
.hljs-symbol,
.hljs-bullet,
.hljs-number {color: #1c00cf;
}.hljs-section,
.hljs-meta {color: #643820;
}.hljs-title.class_,
.hljs-class .hljs-title,
.hljs-type,
.hljs-built_in,
.hljs-params {color: #5c2699;
}.hljs-attr {color: #836C28;
}.hljs-subst {color: #000;
}.hljs-formula {background-color: #eee;font-style: italic;
}.hljs-addition {background-color: #baeeba;
}.hljs-deletion {background-color: #ffc8bd;
}.hljs-selector-id,
.hljs-selector-class {color: #9b703f;
}.hljs-doctag,
.hljs-strong {font-weight: bold;
}.hljs-emphasis {font-style: italic;
}
</style><style type="text/css" data-fela-rehydration="567" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@-moz-keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}</style><style type="text/css" data-fela-rehydration="567" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{display:block}.m{position:sticky}.n{top:0}.o{z-index:500}.p{padding:0 24px}.q{align-items:center}.r{border-bottom:solid 1px #F2F2F2}.y{height:41px}.z{line-height:20px}.ab{display:flex}.ac{height:57px}.ae{flex:1 0 auto}.af{color:inherit}.ag{fill:inherit}.ah{font-size:inherit}.ai{border:inherit}.aj{font-family:inherit}.ak{letter-spacing:inherit}.al{font-weight:inherit}.am{padding:0}.an{margin:0}.ao{cursor:pointer}.ap:disabled{cursor:not-allowed}.aq:disabled{color:#6B6B6B}.ar:disabled{fill:#6B6B6B}.au{width:auto}.av path{fill:#242424}.aw{height:25px}.ax{margin-left:16px}.ay{border:none}.az{border-radius:20px}.ba{width:240px}.bb{background:#F9F9F9}.bc path{fill:#6B6B6B}.be{outline:none}.bf{font-family:sohne, "Helvetica Neue", Helvetica, Arial, sans-serif}.bg{font-size:14px}.bh{width:100%}.bi{padding:10px 20px 10px 0}.bj{background-color:transparent}.bk{color:#242424}.bl::placeholder{color:#6B6B6B}.bm{display:inline-block}.bn{margin-left:12px}.bo{margin-right:12px}.bp{border-radius:4px}.bq{margin-left:24px}.br{height:24px}.bx{background-color:#F9F9F9}.by{border-radius:50%}.bz{height:32px}.ca{width:32px}.cb{justify-content:center}.ch{max-width:680px}.ci{min-width:0}.cj{animation:k1 1.2s ease-in-out infinite}.ck{height:100vh}.cl{margin-bottom:16px}.cm{margin-top:48px}.cn{align-items:flex-start}.co{flex-direction:column}.cp{justify-content:space-between}.cq{margin-bottom:24px}.cw{width:80%}.cx{background-color:#F2F2F2}.dd{height:44px}.de{width:44px}.df{margin:auto 0}.dg{margin-bottom:4px}.dh{height:16px}.di{width:120px}.dj{width:80px}.dp{margin-bottom:8px}.dq{width:96%}.dr{width:98%}.ds{width:81%}.dt{margin-left:8px}.du{color:#6B6B6B}.dv{font-size:13px}.dw{height:100%}.ep{color:#FFFFFF}.eq{fill:#FFFFFF}.er{background:#1A8917}.es{border-color:#1A8917}.ew:disabled{cursor:inherit !important}.ex:disabled{opacity:0.3}.ey:disabled:hover{background:#1A8917}.ez:disabled:hover{border-color:#1A8917}.fa{border-radius:99em}.fb{border-width:1px}.fc{border-style:solid}.fd{box-sizing:border-box}.fe{text-decoration:none}.ff{text-align:center}.fi{margin-right:32px}.fj{position:relative}.fk{fill:#6B6B6B}.fn{background:transparent}.fo svg{margin-left:4px}.fp svg{fill:#6B6B6B}.fr{box-shadow:inset 0 0 0 1px rgba(0, 0, 0, 0.05)}.fs{position:absolute}.fz{margin:0 24px}.gd{background:rgba(255, 255, 255, 1)}.ge{border:1px solid #F2F2F2}.gf{box-shadow:0 1px 4px #F2F2F2}.gg{max-height:100vh}.gh{overflow-y:auto}.gi{left:0}.gj{top:calc(100vh + 100px)}.gk{bottom:calc(100vh + 100px)}.gl{width:10px}.gm{pointer-events:none}.gn{word-break:break-word}.go{word-wrap:break-word}.gp:after{display:block}.gq:after{content:""}.gr:after{clear:both}.gs{line-height:1.23}.gt{letter-spacing:0}.gu{font-style:normal}.gv{font-weight:700}.ia{align-items:baseline}.ib{width:48px}.ic{height:48px}.id{border:2px solid rgba(255, 255, 255, 1)}.ie{z-index:0}.if{box-shadow:none}.ig{border:1px solid rgba(0, 0, 0, 0.05)}.ih{margin-bottom:2px}.ii{flex-wrap:nowrap}.ij{font-size:16px}.ik{line-height:24px}.im{margin:0 8px}.in{display:inline}.io{color:#1A8917}.ip{fill:#1A8917}.is{flex:0 0 auto}.iv{flex-wrap:wrap}.iw{padding-left:8px}.ix{padding-right:8px}.jy> *{flex-shrink:0}.jz{overflow-x:scroll}.ka::-webkit-scrollbar{display:none}.kb{scrollbar-width:none}.kc{-ms-overflow-style:none}.kd{width:74px}.ke{flex-direction:row}.kf{z-index:2}.kg{margin-right:4px}.kj{-webkit-user-select:none}.kk{border:0}.kl{fill:rgba(117, 117, 117, 1)}.ko{outline:0}.kp{user-select:none}.kq> svg{pointer-events:none}.kz{cursor:progress}.la{margin-left:4px}.lb{margin-top:0px}.lc{opacity:1}.ld{padding:4px 0}.lg{width:16px}.li{display:inline-flex}.lo{max-width:100%}.lp{padding:8px 2px}.lq svg{color:#6B6B6B}.mh{margin-left:auto}.mi{margin-right:auto}.mj{max-width:1024px}.mp{clear:both}.mr{cursor:zoom-in}.ms{z-index:auto}.mu{height:auto}.mv{margin-top:10px}.mw{max-width:728px}.mz{line-height:1.58}.na{letter-spacing:-0.004em}.nb{font-family:source-serif-pro, Georgia, Cambria, "Times New Roman", Times, serif}.nw{margin-bottom:-0.46em}.nx{text-decoration:underline}.ny{list-style-type:disc}.nz{margin-left:30px}.oa{padding-left:0px}.og{font-style:italic}.oh{line-height:1.12}.oi{letter-spacing:-0.022em}.oj{font-weight:600}.pe{margin-bottom:-0.28em}.pk{margin-top:32px}.pl{margin-bottom:14px}.pm{padding-top:24px}.pn{padding-bottom:10px}.po{background-color:#000000}.pp{height:3px}.pq{width:3px}.pr{margin-right:20px}.px{line-height:1.18}.ql{margin-bottom:-0.31em}.qm{max-width:1920px}.qs{list-style-type:decimal}.qt{margin:auto}.qu{overflow:hidden}.qv{padding-bottom:100%}.qw{height:0}.qx{margin-bottom:26px}.qy{margin-top:6px}.qz{margin-top:8px}.ra{margin-right:8px}.rb{padding:8px 16px}.rc{border-radius:100px}.rd{transition:background 300ms ease}.rf{white-space:nowrap}.rg{border-top:none}.rh{height:52px}.ri{max-height:52px}.rj{box-sizing:content-box}.rk{position:static}.rl{z-index:1}.rn{max-width:155px}.ry{height:0px}.rz{margin-bottom:40px}.so{height:64px}.sp{width:64px}.sq{align-self:flex-end}.sr{color:rgba(255, 255, 255, 1)}.ss{fill:rgba(255, 255, 255, 1)}.st{background:rgba(25, 25, 25, 1)}.su{border-color:rgba(25, 25, 25, 1)}.sx:disabled{opacity:0.1}.sy:disabled:hover{background:rgba(25, 25, 25, 1)}.sz:disabled:hover{border-color:rgba(25, 25, 25, 1)}.ta{flex:1 1 auto}.tg{padding-right:4px}.th{font-weight:500}.to{white-space:pre-wrap}.tp{margin-top:16px}.ty{gap:18px}.tz{fill:rgba(61, 61, 61, 1)}.ub{fill:#242424}.uc{background:0}.ud{border-color:#242424}.ue:disabled:hover{color:#242424}.uf:disabled:hover{fill:#242424}.ug:disabled:hover{border-color:#242424}.ur{border-bottom:solid 1px #E5E5E5}.us{margin-top:72px}.ut{padding:24px 0}.uu{margin-bottom:0px}.uv{margin-right:16px}.as:hover:not(:disabled){color:rgba(25, 25, 25, 1)}.at:hover:not(:disabled){fill:rgba(25, 25, 25, 1)}.et:hover{background:#156D12}.eu:hover{border-color:#156D12}.ev:hover{cursor:pointer}.fl:hover{color:#242424}.fm:hover{fill:#242424}.fq:hover svg{fill:#242424}.ft:hover{background-color:rgba(0, 0, 0, 0.1)}.il:hover{text-decoration:underline}.iq:hover:not(:disabled){color:#156D12}.ir:hover:not(:disabled){fill:#156D12}.kn:hover{fill:rgba(8, 8, 8, 1)}.le:hover{fill:#000000}.lf:hover p{color:#000000}.lh:hover{color:#000000}.lr:hover svg{color:#000000}.re:hover{background-color:#F2F2F2}.sn:hover{background-color:none}.sv:hover{background:#000000}.sw:hover{border-color:#242424}.ua:hover{fill:rgba(25, 25, 25, 1)}.bd:focus-within path{fill:#242424}.km:focus{fill:rgba(8, 8, 8, 1)}.ls:focus svg{color:#000000}.mt:focus{transform:scale(1.01)}.kr:active{border-style:none}</style><style type="text/css" data-fela-rehydration="567" data-fela-type="RULE" media="all and (min-width: 1080px)">.d{display:none}.bw{width:64px}.cg{margin:0 64px}.cv{height:48px}.dc{margin-bottom:52px}.do{margin-bottom:48px}.ef{font-size:14px}.eg{line-height:20px}.em{font-size:13px}.eo{padding:5px 12px}.fh{display:flex}.fy{margin-bottom:68px}.gc{max-width:680px}.hq{font-size:42px}.hr{margin-top:1.19em}.hs{margin-bottom:32px}.ht{line-height:52px}.hu{letter-spacing:-0.011em}.hz{align-items:center}.jk{border-top:solid 1px #F2F2F2}.jl{border-bottom:solid 1px #F2F2F2}.jm{margin:32px 0 0}.jn{padding:3px 8px}.jw> *{margin-right:24px}.jx> :last-child{margin-right:0}.ky{margin-top:0px}.ln{margin:0}.mo{margin-top:40px}.ns{font-size:20px}.nt{margin-top:2.14em}.nu{line-height:32px}.nv{letter-spacing:-0.003em}.of{margin-top:1.14em}.pa{font-size:24px}.pb{margin-top:1.95em}.pc{line-height:30px}.pd{letter-spacing:-0.016em}.pj{margin-top:0.94em}.pw{margin-top:1.25em}.qi{margin-top:1.72em}.qj{line-height:24px}.qk{letter-spacing:0}.qr{margin-top:56px}.rs{display:inline-block}.rx{margin-bottom:104px}.sa{flex-direction:row}.sd{margin-bottom:0}.se{margin-right:20px}.tb{max-width:500px}.tu{margin-bottom:88px}.tx{margin-bottom:72px}.ul{width:min-width}.uq{padding-top:72px}</style><style type="text/css" data-fela-rehydration="567" data-fela-type="RULE" media="all and (max-width: 1079.98px)">.e{display:none}.kx{margin-top:0px}.mx{margin-left:auto}.my{text-align:center}.rr{display:inline-block}</style><style type="text/css" data-fela-rehydration="567" data-fela-type="RULE" media="all and (max-width: 903.98px)">.f{display:none}.kw{margin-top:0px}.rq{display:inline-block}</style><style type="text/css" data-fela-rehydration="567" data-fela-type="RULE" media="all and (max-width: 727.98px)">.g{display:none}.ku{margin-top:0px}.kv{margin-right:0px}.rp{display:inline-block}</style><style type="text/css" data-fela-rehydration="567" data-fela-type="RULE" media="all and (max-width: 551.98px)">.h{display:none}.s{display:flex}.t{justify-content:space-between}.bs{width:24px}.cc{margin:0 24px}.cr{height:40px}.cy{margin-bottom:44px}.dk{margin-bottom:32px}.dx{font-size:13px}.dy{line-height:20px}.eh{padding:0px 8px 1px}.fu{margin-bottom:4px}.gw{font-size:32px}.gx{margin-top:1.01em}.gy{margin-bottom:24px}.gz{line-height:38px}.ha{letter-spacing:-0.014em}.hv{align-items:flex-start}.it{flex-direction:column}.iy{margin:24px -24px 0}.iz{padding:0}.jo> *{margin-right:8px}.jp> :last-child{margin-right:24px}.kh{margin-left:0px}.ks{margin-top:0px}.kt{margin-right:0px}.lj{margin:0}.lt{border:1px solid #F2F2F2}.lu{border-radius:99em}.lv{padding:0px 16px 0px 12px}.lw{height:38px}.lx{align-items:center}.lz svg{margin-right:8px}.mk{margin-top:32px}.nc{font-size:18px}.nd{margin-top:1.56em}.ne{line-height:28px}.nf{letter-spacing:-0.003em}.ob{margin-top:1.34em}.ok{font-size:20px}.ol{margin-top:1.2em}.om{line-height:24px}.on{letter-spacing:0}.pf{margin-top:0.67em}.ps{margin-top:0.93em}.py{font-size:16px}.pz{margin-top:1.23em}.qn{margin-top:40px}.ro{display:inline-block}.rt{margin-bottom:96px}.sl{margin-bottom:20px}.sm{margin-right:0}.tf{max-width:100%}.ti{font-size:24px}.tj{line-height:30px}.tk{letter-spacing:-0.016em}.tq{margin-bottom:64px}.uh{width:100%}.um{padding-top:48px}.ly:hover{border-color:#E5E5E5}</style><style type="text/css" data-fela-rehydration="567" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.i{display:none}.bv{width:64px}.cf{margin:0 64px}.cu{height:48px}.db{margin-bottom:52px}.dn{margin-bottom:48px}.ed{font-size:14px}.ee{line-height:20px}.ek{font-size:13px}.el{padding:5px 12px}.fg{display:flex}.fx{margin-bottom:68px}.gb{max-width:680px}.hl{font-size:42px}.hm{margin-top:1.19em}.hn{margin-bottom:32px}.ho{line-height:52px}.hp{letter-spacing:-0.011em}.hy{align-items:center}.jg{border-top:solid 1px #F2F2F2}.jh{border-bottom:solid 1px #F2F2F2}.ji{margin:32px 0 0}.jj{padding:3px 8px}.ju> *{margin-right:24px}.jv> :last-child{margin-right:0}.lm{margin:0}.mn{margin-top:40px}.no{font-size:20px}.np{margin-top:2.14em}.nq{line-height:32px}.nr{letter-spacing:-0.003em}.oe{margin-top:1.14em}.ow{font-size:24px}.ox{margin-top:1.95em}.oy{line-height:30px}.oz{letter-spacing:-0.016em}.pi{margin-top:0.94em}.pv{margin-top:1.25em}.qf{margin-top:1.72em}.qg{line-height:24px}.qh{letter-spacing:0}.qq{margin-top:56px}.rw{margin-bottom:104px}.sb{flex-direction:row}.sf{margin-bottom:0}.sg{margin-right:20px}.tc{max-width:500px}.tt{margin-bottom:88px}.tw{margin-bottom:72px}.uk{width:min-width}.up{padding-top:72px}</style><style type="text/css" data-fela-rehydration="567" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.j{display:none}.w{display:flex}.x{justify-content:space-between}.bu{width:64px}.ce{margin:0 48px}.ct{height:48px}.da{margin-bottom:52px}.dm{margin-bottom:48px}.eb{font-size:13px}.ec{line-height:20px}.ej{padding:0px 8px 1px}.fw{margin-bottom:68px}.ga{max-width:680px}.hg{font-size:42px}.hh{margin-top:1.19em}.hi{margin-bottom:32px}.hj{line-height:52px}.hk{letter-spacing:-0.011em}.hx{align-items:center}.jc{border-top:solid 1px #F2F2F2}.jd{border-bottom:solid 1px #F2F2F2}.je{margin:32px 0 0}.jf{padding:3px 8px}.js> *{margin-right:24px}.jt> :last-child{margin-right:0}.ll{margin:0}.mm{margin-top:40px}.nk{font-size:20px}.nl{margin-top:2.14em}.nm{line-height:32px}.nn{letter-spacing:-0.003em}.od{margin-top:1.14em}.os{font-size:24px}.ot{margin-top:1.95em}.ou{line-height:30px}.ov{letter-spacing:-0.016em}.ph{margin-top:0.94em}.pu{margin-top:1.25em}.qc{margin-top:1.72em}.qd{line-height:24px}.qe{letter-spacing:0}.qp{margin-top:56px}.rv{margin-bottom:104px}.sc{flex-direction:row}.sh{margin-bottom:0}.si{margin-right:20px}.td{max-width:500px}.ts{margin-bottom:88px}.tv{margin-bottom:72px}.uj{width:min-width}.uo{padding-top:72px}</style><style type="text/css" data-fela-rehydration="567" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.k{display:none}.u{display:flex}.v{justify-content:space-between}.bt{width:24px}.cd{margin:0 24px}.cs{height:40px}.cz{margin-bottom:44px}.dl{margin-bottom:32px}.dz{font-size:13px}.ea{line-height:20px}.ei{padding:0px 8px 1px}.fv{margin-bottom:4px}.hb{font-size:32px}.hc{margin-top:1.01em}.hd{margin-bottom:24px}.he{line-height:38px}.hf{letter-spacing:-0.014em}.hw{align-items:flex-start}.iu{flex-direction:column}.ja{margin:24px 0 0}.jb{padding:0}.jq> *{margin-right:8px}.jr> :last-child{margin-right:8px}.ki{margin-left:0px}.lk{margin:0}.ma{border:1px solid #F2F2F2}.mb{border-radius:99em}.mc{padding:0px 16px 0px 12px}.md{height:38px}.me{align-items:center}.mg svg{margin-right:8px}.ml{margin-top:32px}.ng{font-size:18px}.nh{margin-top:1.56em}.ni{line-height:28px}.nj{letter-spacing:-0.003em}.oc{margin-top:1.34em}.oo{font-size:20px}.op{margin-top:1.2em}.oq{line-height:24px}.or{letter-spacing:0}.pg{margin-top:0.67em}.pt{margin-top:0.93em}.qa{font-size:16px}.qb{margin-top:1.23em}.qo{margin-top:40px}.ru{margin-bottom:96px}.sj{margin-bottom:20px}.sk{margin-right:0}.te{max-width:100%}.tl{font-size:24px}.tm{line-height:30px}.tn{letter-spacing:-0.016em}.tr{margin-bottom:64px}.ui{width:100%}.un{padding-top:48px}.mf:hover{border-color:#E5E5E5}</style><style type="text/css" data-fela-rehydration="567" data-fela-type="RULE" media="print">.rm{display:none}</style><style type="text/css" data-fela-rehydration="567" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)">.mq{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}</style></head><body><div id="root"><div class="a b c"><div class="d e f g h i j k"></div><script>document.domain = document.domain;</script><div class="l c"><div class="l m n o c"><div class="p q r s t u v w x i d y z"><a class="du ag dv bf ak b am an ao ap aq ar as at s u w i d q dw z" href="https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3b19c1f025e4&amp;%7Efeature=LoOpenInAppButton&amp;%7Echannel=ShowPostUnderUser&amp;source=---top_nav_layout_nav----------------------------------" rel="noopener follow">Open in app<svg xmlns="http://www.w3.org/2000/svg" width="10" height="10" fill="none" viewBox="0 0 10 10" class="dt"><path fill="currentColor" d="M.985 8.485a.375.375 0 1 0 .53.53zM8.75 1.25h.375A.375.375 0 0 0 8.75.875zM8.375 6.5a.375.375 0 1 0 .75 0zM3.5.875a.375.375 0 1 0 0 .75zm-1.985 8.14 7.5-7.5-.53-.53-7.5 7.5zm6.86-7.765V6.5h.75V1.25zM3.5 1.625h5.25v-.75H3.5z"></path></svg></a><div class="ab q"><p class="bf b dx dy dz ea eb ec ed ee ef eg du"><span><button class="bf b dx dy eh dz ea ei eb ec ej ek ee el em eg eo ep eq er es et eu ev ew ex ey ez fa fb fc fd bm fe ff" data-testid="headerSignUpButton">Sign up</button></span></p><div class="ax l"><p class="bf b dx dy dz ea eb ec ed ee ef eg du"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerSignInButton" rel="noopener follow" href="/m/signin?operation=login&amp;redirect=https%3A%2F%2Fmedium.com%2F%40dzmitrybahdanau%2Fthe-flops-calculus-of-language-model-training-3b19c1f025e4&amp;source=post_page---top_nav_layout_nav-----------------------global_nav-----------">Sign in</a></span></p></div></div></div><div class="p q r ab ac"><div class="ab q ae"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab" aria-label="Homepage" data-testid="headerMediumLogo" rel="noopener follow" href="/?source=---top_nav_layout_nav----------------------------------"><svg xmlns="http://www.w3.org/2000/svg" width="719" height="160" fill="none" viewBox="0 0 719 160" class="au av aw"><path fill="#242424" d="m174.104 9.734.215-.047V8.02H130.39L89.6 103.89 48.81 8.021H1.472v1.666l.212.047c8.018 1.81 12.09 4.509 12.09 14.242V137.93c0 9.734-4.087 12.433-12.106 14.243l-.212.047v1.671h32.118v-1.665l-.213-.048c-8.018-1.809-12.089-4.509-12.089-14.242V30.586l52.399 123.305h2.972l53.925-126.743V140.75c-.687 7.688-4.721 10.062-11.982 11.701l-.215.05v1.652h55.948v-1.652l-.215-.05c-7.269-1.639-11.4-4.013-12.087-11.701l-.037-116.774h.037c0-9.733 4.071-12.432 12.087-14.242m25.555 75.488c.915-20.474 8.268-35.252 20.606-35.507 3.806.063 6.998 1.312 9.479 3.714 5.272 5.118 7.751 15.812 7.368 31.793zm-.553 5.77h65.573v-.275c-.186-15.656-4.721-27.834-13.466-36.196-7.559-7.227-18.751-11.203-30.507-11.203h-.263c-6.101 0-13.584 1.48-18.909 4.16-6.061 2.807-11.407 7.003-15.855 12.511-7.161 8.874-11.499 20.866-12.554 34.343q-.05.606-.092 1.212a50 50 0 0 0-.065 1.151 85.807 85.807 0 0 0-.094 5.689c.71 30.524 17.198 54.917 46.483 54.917 25.705 0 40.675-18.791 44.407-44.013l-1.886-.664c-6.557 13.556-18.334 21.771-31.738 20.769-18.297-1.369-32.314-19.922-31.042-42.395m139.722 41.359c-2.151 5.101-6.639 7.908-12.653 7.908s-11.513-4.129-15.418-11.63c-4.197-8.053-6.405-19.436-6.405-32.92 0-28.067 8.729-46.22 22.24-46.22 5.657 0 10.111 2.807 12.236 7.704zm43.499 20.008c-8.019-1.897-12.089-4.722-12.089-14.951V1.309l-48.716 14.353v1.757l.299-.024c6.72-.543 11.278.386 13.925 2.83 2.072 1.915 3.082 4.853 3.082 8.987v18.66c-4.803-3.067-10.516-4.56-17.448-4.56-14.059 0-26.909 5.92-36.176 16.672-9.66 11.205-14.767 26.518-14.767 44.278-.003 31.72 15.612 53.039 38.851 53.039 13.595 0 24.533-7.449 29.54-20.013v16.865h43.711v-1.746zM424.1 19.819c0-9.904-7.468-17.374-17.375-17.374-9.859 0-17.573 7.632-17.573 17.374s7.721 17.374 17.573 17.374c9.907 0 17.375-7.47 17.375-17.374m11.499 132.546c-8.019-1.897-12.089-4.722-12.089-14.951h-.035V43.635l-43.714 12.551v1.705l.263.024c9.458.842 12.047 4.1 12.047 15.152v81.086h43.751v-1.746zm112.013 0c-8.018-1.897-12.089-4.722-12.089-14.951V43.635l-41.621 12.137v1.71l.246.026c7.733.813 9.967 4.257 9.967 15.36v59.279c-2.578 5.102-7.415 8.131-13.274 8.336-9.503 0-14.736-6.419-14.736-18.073V43.638l-43.714 12.55v1.703l.262.024c9.459.84 12.05 4.097 12.05 15.152v50.17a56.3 56.3 0 0 0 .91 10.444l.787 3.423c3.701 13.262 13.398 20.197 28.59 20.197 12.868 0 24.147-7.966 29.115-20.43v17.311h43.714v-1.747zm169.818 1.788v-1.749l-.213-.05c-8.7-2.006-12.089-5.789-12.089-13.49v-63.79c0-19.89-11.171-31.761-29.883-31.761-13.64 0-25.141 7.882-29.569 20.16-3.517-13.01-13.639-20.16-28.606-20.16-13.146 0-23.449 6.938-27.869 18.657V43.643L545.487 55.68v1.715l.263.024c9.345.829 12.047 4.181 12.047 14.95v81.784h40.787v-1.746l-.215-.053c-6.941-1.631-9.181-4.606-9.181-12.239V66.998c1.836-4.289 5.537-9.37 12.853-9.37 9.086 0 13.692 6.296 13.692 18.697v77.828h40.797v-1.746l-.215-.053c-6.94-1.631-9.18-4.606-9.18-12.239V75.066a42 42 0 0 0-.578-7.26c1.947-4.661 5.86-10.177 13.475-10.177 9.214 0 13.691 6.114 13.691 18.696v77.828z"></path></svg></a><div class="ax h"><div class="ab ay az ba bb q bc bd"><div class="bm" aria-hidden="false" aria-describedby="searchResults" aria-labelledby="searchResults"></div><div class="bn bo ab"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.092 11.06a6.95 6.95 0 1 1 13.9 0 6.95 6.95 0 0 1-13.9 0m6.95-8.05a8.05 8.05 0 1 0 5.13 14.26l3.75 3.75a.56.56 0 1 0 .79-.79l-3.73-3.73A8.05 8.05 0 0 0 11.042 3z" clip-rule="evenodd"></path></svg></div><input role="combobox" aria-controls="searchResults" aria-expanded="false" aria-label="search" data-testid="headerSearchInput" tabindex="0" class="ay be bf bg z bh bi bj bk bl" placeholder="Search" value=""/></div></div></div><div class="h k w fg fh"><div class="fi ab"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerWriteButton" rel="noopener follow" href="/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fnew-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav-----------"><div class="bf b bg z du fj fk ab q fl fm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" aria-label="Write"><path fill="currentColor" d="M14 4a.5.5 0 0 0 0-1zm7 6a.5.5 0 0 0-1 0zm-7-7H4v1h10zM3 4v16h1V4zm1 17h16v-1H4zm17-1V10h-1v10zm-1 1a1 1 0 0 0 1-1h-1zM3 20a1 1 0 0 0 1 1v-1zM4 3a1 1 0 0 0-1 1h1z"></path><path stroke="currentColor" d="m17.5 4.5-8.458 8.458a.25.25 0 0 0-.06.098l-.824 2.47a.25.25 0 0 0 .316.316l2.47-.823a.25.25 0 0 0 .098-.06L19.5 6.5m-2-2 2.323-2.323a.25.25 0 0 1 .354 0l1.646 1.646a.25.25 0 0 1 0 .354L19.5 6.5m-2-2 2 2"></path></svg><div class="dt l">Write</div></div></a></span></div></div><div class="k j i d"><div class="fi ab"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerSearchButton" rel="noopener follow" href="/search?source=---top_nav_layout_nav----------------------------------"><div class="bf b bg z du fj fk ab q fl fm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" aria-label="Search"><path fill="currentColor" fill-rule="evenodd" d="M4.092 11.06a6.95 6.95 0 1 1 13.9 0 6.95 6.95 0 0 1-13.9 0m6.95-8.05a8.05 8.05 0 1 0 5.13 14.26l3.75 3.75a.56.56 0 1 0 .79-.79l-3.73-3.73A8.05 8.05 0 0 0 11.042 3z" clip-rule="evenodd"></path></svg></div></a></div></div><div class="fi h k j"><div class="ab q"><p class="bf b dx dy dz ea eb ec ed ee ef eg du"><span><button class="bf b dx dy eh dz ea ei eb ec ej ek ee el em eg eo ep eq er es et eu ev ew ex ey ez fa fb fc fd bm fe ff" data-testid="headerSignUpButton">Sign up</button></span></p><div class="ax l"><p class="bf b dx dy dz ea eb ec ed ee ef eg du"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerSignInButton" rel="noopener follow" href="/m/signin?operation=login&amp;redirect=https%3A%2F%2Fmedium.com%2F%40dzmitrybahdanau%2Fthe-flops-calculus-of-language-model-training-3b19c1f025e4&amp;source=post_page---top_nav_layout_nav-----------------------global_nav-----------">Sign in</a></span></p></div></div></div><div class="l" aria-hidden="false"><button class="ay fn am ab q ao fo fp fq" aria-label="user options menu" data-testid="headerUserIcon"><div class="l fj"><img alt="" class="l fd by bz ca cx" src="https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png" width="32" height="32" loading="lazy" role="presentation"/><div class="fr by l bz ca fs n ay ft"></div></div></button></div></div></div><div class="l"><div class="fu fv fw fx fy l"><div class="ab cb"><div class="ci bh fz ga gb gc"></div></div><article><div class="l"><div class="l"><span class="l"></span><section><div><div class="fs gi gj gk gl gm"></div><div class="gn go gp gq gr"><div class="ab cb"><div class="ci bh fz ga gb gc"><div><h1 id="64a6" class="pw-post-title gs gt gu bf gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu bk" data-testid="storyTitle">The FLOPs Calculus of Language Model Training</h1><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hv hw hx hy hz ab"><div><div class="ab ia"><div><div class="bm" aria-hidden="false"><a rel="noopener follow" href="/@dzmitrybahdanau?source=post_page---byline--3b19c1f025e4--------------------------------"><div class="l ib ic by id ie"><div class="l fj"><img alt="Dzmitry Bahdanau" class="l fd by dd de cx" src="https://miro.medium.com/v2/resize:fill:88:88/1*4ycUaU0RGJDPwaSfQrfmgg.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"/><div class="if by l dd de fs n ig ft"></div></div></div></a></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="ih ab q"><div class="ab q ii"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ij ik bk"><a class="af ag ah ai aj ak al am an ao ap aq ar il" data-testid="authorName" rel="noopener follow" href="/@dzmitrybahdanau?source=post_page---byline--3b19c1f025e4--------------------------------">Dzmitry Bahdanau</a></p></div></div></div><span class="im in" aria-hidden="true"><span class="bf b bg z du">·</span></span><p class="bf b ij ik du"><span><a class="io ip ah ai aj ak al am an ao ap aq ar ex iq ir" rel="noopener follow" href="/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe663600ab469&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40dzmitrybahdanau%2Fthe-flops-calculus-of-language-model-training-3b19c1f025e4&amp;user=Dzmitry+Bahdanau&amp;userId=e663600ab469&amp;source=post_page-e663600ab469--byline--3b19c1f025e4---------------------post_header-----------">Follow</a></span></p></div></div></span></div></div><div class="l is"><span class="bf b bg z du"><div class="ab cn it iu iv"><span class="bf b bg z du"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z du">·</span></span></div><span data-testid="storyPublishDate">Jan 9, 2022</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w fg fh q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon fj kg kh ki kj"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerClapButton" rel="noopener follow" href="/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F3b19c1f025e4&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40dzmitrybahdanau%2Fthe-flops-calculus-of-language-model-training-3b19c1f025e4&amp;user=Dzmitry+Bahdanau&amp;userId=e663600ab469&amp;source=---header_actions--3b19c1f025e4---------------------clap_footer-----------"><div><div class="bm" aria-hidden="false"><div class="kk ao kl km kn ko am kp kq kr kj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM13.916 3.953l1.523-2.112-1.184-.39zM8.589 1.84l1.522 2.112-.337-2.501zM18.523 18.92c-.86.86-1.75 1.246-2.62 1.33a6 6 0 0 0 .407-.372c2.388-2.389 2.86-4.951 1.399-7.623l-.912-1.603-.79-1.672c-.26-.56-.194-.98.203-1.288a.7.7 0 0 1 .546-.132c.283.046.546.231.728.5l2.363 4.157c.976 1.624 1.141 4.237-1.324 6.702m-10.999-.438L3.37 14.328a.828.828 0 0 1 .585-1.408.83.83 0 0 1 .585.242l2.158 2.157a.365.365 0 0 0 .516-.516l-2.157-2.158-1.449-1.449a.826.826 0 0 1 1.167-1.17l3.438 3.44a.363.363 0 0 0 .516 0 .364.364 0 0 0 0-.516L5.293 9.513l-.97-.97a.826.826 0 0 1 0-1.166.84.84 0 0 1 1.167 0l.97.968 3.437 3.436a.36.36 0 0 0 .517 0 .366.366 0 0 0 0-.516L6.977 7.83a.82.82 0 0 1-.241-.584.82.82 0 0 1 .824-.826c.219 0 .43.087.584.242l5.787 5.787a.366.366 0 0 0 .587-.415l-1.117-2.363c-.26-.56-.194-.98.204-1.289a.7.7 0 0 1 .546-.132c.283.046.545.232.727.501l2.193 3.86c1.302 2.38.883 4.59-1.277 6.75-1.156 1.156-2.602 1.627-4.19 1.367-1.418-.236-2.866-1.033-4.079-2.246M10.75 5.971l2.12 2.12c-.41.502-.465 1.17-.128 1.89l.22.465-3.523-3.523a.8.8 0 0 1-.097-.368c0-.22.086-.428.241-.584a.847.847 0 0 1 1.167 0m7.355 1.705c-.31-.461-.746-.758-1.23-.837a1.44 1.44 0 0 0-1.11.275c-.312.24-.505.543-.59.881a1.74 1.74 0 0 0-.906-.465 1.47 1.47 0 0 0-.82.106l-2.182-2.182a1.56 1.56 0 0 0-2.2 0 1.54 1.54 0 0 0-.396.701 1.56 1.56 0 0 0-2.21-.01 1.55 1.55 0 0 0-.416.753c-.624-.624-1.649-.624-2.237-.037a1.557 1.557 0 0 0 0 2.2c-.239.1-.501.238-.715.453a1.56 1.56 0 0 0 0 2.2l.516.515a1.556 1.556 0 0 0-.753 2.615L7.01 19c1.32 1.319 2.909 2.189 4.475 2.449q.482.08.971.08c.85 0 1.653-.198 2.393-.579.231.033.46.054.686.054 1.266 0 2.457-.52 3.505-1.567 2.763-2.763 2.552-5.734 1.439-7.586z" clip-rule="evenodd"></path></svg></div></div></div></a></span></div><div class="pw-multi-vote-count l ks kt ku kv kw kx ky"><p class="bf b dv z du"><span class="kz">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kk lc ld ab q fk le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"></path></svg><p class="bf b dv z du"><span class="pw-responses-count la lb">3</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"></div><div class="h k"><div><div class="bm" aria-hidden="false"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerBookmarkButton" rel="noopener follow" href="/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3b19c1f025e4&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40dzmitrybahdanau%2Fthe-flops-calculus-of-language-model-training-3b19c1f025e4&amp;source=---header_actions--3b19c1f025e4---------------------bookmark_footer-----------"><svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" fill="none" viewBox="0 0 25 25" class="du lh" aria-label="Add to list bookmark button"><path fill="currentColor" d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .805.396L12.5 17l5.695 4.396A.5.5 0 0 0 19 21v-8.5a.5.5 0 0 0-1 0v7.485l-5.195-4.012a.5.5 0 0 0-.61 0L7 19.985z"></path></svg></a></span></div></div></div><div class="fd li cn"><div class="l ae"><div class="ab cb"><div class="lj lk ll lm ln lo ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af fk ah ai aj ak al lp an ao ap ex lq lr lf ls lt lu lv lw s lx ly lz ma mb mc md u me mf mg"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"></path></svg><div class="j i d"><p class="bf b bg z du">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af fk ah ai aj ak al lp an ao ap ex lq lr lf ls lt lu lv lw s lx ly lz ma mb mc md u me mf mg"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"></path></svg><div class="j i d"><p class="bf b bg z du">Share</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mk ml mm mn mo mp mh mi paragraph-image"><div role="button" tabindex="0" class="mq mr fj ms bh mt"><div class="mh mi mj"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/0*sG59WF5K7qLZr307.jpg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*sG59WF5K7qLZr307.jpg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*sG59WF5K7qLZr307.jpg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*sG59WF5K7qLZr307.jpg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*sG59WF5K7qLZr307.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*sG59WF5K7qLZr307.jpg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sG59WF5K7qLZr307.jpg 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/0*sG59WF5K7qLZr307.jpg 640w, https://miro.medium.com/v2/resize:fit:720/0*sG59WF5K7qLZr307.jpg 720w, https://miro.medium.com/v2/resize:fit:750/0*sG59WF5K7qLZr307.jpg 750w, https://miro.medium.com/v2/resize:fit:786/0*sG59WF5K7qLZr307.jpg 786w, https://miro.medium.com/v2/resize:fit:828/0*sG59WF5K7qLZr307.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/0*sG59WF5K7qLZr307.jpg 1100w, https://miro.medium.com/v2/resize:fit:1400/0*sG59WF5K7qLZr307.jpg 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" class="bh lo mu c" width="700" height="525" loading="eager" role="presentation"/></picture></div></div><figcaption class="mv ff mw mh mi mx my bf b bg z du">Training a large Transformer requires many [flip-]FLOPs</figcaption></figure><p id="eb0f" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk">Extremely large language models like the famous <a class="af nx" href="https://arxiv.org/abs/2005.14165" rel="noopener ugc nofollow" target="_blank">GPT-3 by OpenAI</a> are all the rage. Many of us are now trying to get a sense of scale of the compute that goes into training them.</p><p id="58a3" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk">In this article, I will offer you a very useful tool to reason about large Transformer LMs. This tool will help you roughly answer questions like</p><ul class=""><li id="2fb5" class="mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw ny nz oa bk">How much does it cost to train GPT-3?</li><li id="19f8" class="mz na gu nb b nc ob ne nf ng oc ni nj nk od nm nn no oe nq nr ns of nu nv nw ny nz oa bk">How long will training this big model take me?</li></ul><p id="3915" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk">Turns out quick back-of-the-envelope calculations can be sufficient to answer these questions if you use a simple equation that ties together</p><ul class=""><li id="be48" class="mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw ny nz oa bk">the compute required to train a Transformer model ( <em class="og">C )</em></li><li id="704f" class="mz na gu nb b nc ob ne nf ng oc ni nj nk od nm nn no oe nq nr ns of nu nv nw ny nz oa bk">its number of parameters, or model size ( <em class="og">N</em> )</li><li id="6bf0" class="mz na gu nb b nc ob ne nf ng oc ni nj nk od nm nn no oe nq nr ns of nu nv nw ny nz oa bk">the number of tokens that the model is trained on ( <em class="og">D</em> )</li></ul><p id="d027" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk">Without further ado, meet the <strong class="nb gv">Transformer FLOPs Equation</strong>:</p><p id="8f3d" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk"><em class="og">C ≈ 6ND.</em></p><p id="b9b0" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk">A slightly more sophisticated version of the equation expresses the compute C as the product of cluster’s throughput 𝜏 and training time <em class="og">T</em>:</p><p id="3b25" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk">𝜏<em class="og">T = 6ND.</em></p><h1 id="5044" class="oh oi gu bf oj ok ol om on oo op oq or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Example</h1><p id="e960" class="pw-post-body-paragraph mz na gu nb b nc pf ne nf ng pg ni nj nk ph nm nn no pi nq nr ns pj nu nv nw gn bk">Let’s apply the Transformer FLOPs Equation to some middle-school style problem solving:</p><p id="57a3" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk"><strong class="nb gv">Problem</strong> An 82B parameter Korean variant of GPT-3 called <a class="af nx" href="https://arxiv.org/abs/2109.04650" rel="noopener ugc nofollow" target="_blank">HyperCLOVA</a> was trained on 150B tokens using a cluster of 1024 Nvidia A100 GPUs. How long could that take?</p><p id="d7b7" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk"><strong class="nb gv">Solution</strong> The <a class="af nx" href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet.pdf" rel="noopener ugc nofollow" target="_blank">peak float16 FLOPs throughput of A100</a> is 𝜏 = 312 teraFLOPs = 3.12e14 FLOPs. The total compute is <em class="og">C</em> = 6 ∙ 8.2e10 ∙ 1.5e11 = 7.38e22. The training must have taken at least<em class="og"> T</em> = <em class="og">C</em> / 1024𝜏 / 86400 = 2.67 days.</p><p id="a33d" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk"><strong class="nb gv">Answer Validation </strong>According to the <a class="af nx" href="https://arxiv.org/abs/2109.04650" rel="noopener ugc nofollow" target="_blank">white paper</a>, training took 13.4 days. Our estimate is 5 times off, but we did get the order of magnitude right.</p><p id="1249" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk">As I explain later in this post, the error is due to us naively plugging the theoretical peak throughput 𝜏 that is not achievable with distributed training and when models do anything but large matrix multiplications. If you correct 𝜏 accordingly (I will discuss this later), the FLOPs equation will get much more accurate. The other correction is that with checkpointing² that is a must for the largest models the required compute <em class="og">C </em>goes up to <em class="og">≈ 8ND.</em></p><p id="e68d" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk">This equation is not something I came up with¹: you can find it in both the <a class="af nx" href="https://arxiv.org/abs/2001.08361" rel="noopener ugc nofollow" target="_blank">scaling laws</a> and the <a class="af nx" href="https://arxiv.org/abs/2005.14165" rel="noopener ugc nofollow" target="_blank">GPT-3</a> papers by OpenAI. Yet I think it is not as widely known as it should be. In the rest of the article I will derive the equation and discuss what realistic throughput 𝜏 you can expect. I will assume you are familiar with Transformers (check out <a class="af nx" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">the paper</a> or a blog post like e.g. <a class="af nx" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">this one</a> if you are not).</p></div></div></div><div class="ab cb pk pl pm pn" role="separator"><span class="po by bm pp pq pr"></span><span class="po by bm pp pq pr"></span><span class="po by bm pp pq"></span></div><div class="gn go gp gq gr"><div class="ab cb"><div class="ci bh fz ga gb gc"><h1 id="d4a4" class="oh oi gu bf oj ok ps om on oo pt oq or os pu ou ov ow pv oy oz pa pw pc pd pe bk">Derivation of Transformer FLOPs Equation</h1><p id="4943" class="pw-post-body-paragraph mz na gu nb b nc pf ne nf ng pg ni nj nk ph nm nn no pi nq nr ns pj nu nv nw gn bk">To derive the Transformer FLOPs equation we will have to make a key assumption.</p><h2 id="cd82" class="px oi gu bf oj py pz dy on qa qb ea or nk qc qd qe no qf qg qh ns qi qj qk ql bk">The Weight FLOPs Assumption</h2><p id="a94f" class="pw-post-body-paragraph mz na gu nb b nc pf ne nf ng pg ni nj nk ph nm nn no pi nq nr ns pj nu nv nw gn bk"><em class="og">The FLOPs that matter the most are weight FLOPs, that is ones performed when intermediate states are multiplied by weight matrices.</em></p><p id="2592" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk">The weight FLOPs are the majority of Transformer FLOPs, meaning that we can put aside FLOPs required for bias vector addition, layer normalization, residual connections, non-linearities, softmax and even attention. If you do not believe this, you are not wrong: while other FLOPs are less numerous, they also requires a lot of memory access and will in practice matter quite a bit. I will return to this later.</p><p id="055c" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk">The beauty of matrix multiplications is that each of them adds a predictable and easy to compute number of FLOPs to the training total:</p><p id="76c1" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk"><em class="og">weight FLOPs for multiplying by a matrix W = 6</em> <strong class="nb gv">times</strong> <em class="og">batch size </em><strong class="nb gv"><em class="og">times </em></strong><em class="og">size of W</em></p><p id="9b90" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk">This <strong class="nb gv">Weight FLOPs Equation </strong>can take some time to wrap one’s head around. To understand where it comes from, consider a weight <em class="og">w </em>that connects an input unit <em class="og">i</em> to an output unit <em class="og">j:</em></p><figure class="qn qo qp qq qr mp mh mi paragraph-image"><div role="button" tabindex="0" class="mq mr fj ms bh mt"><div class="mh mi qm"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*VC9y_dHhCKFPXj90Qshj3w.gif 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*VC9y_dHhCKFPXj90Qshj3w.gif 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*VC9y_dHhCKFPXj90Qshj3w.gif 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*VC9y_dHhCKFPXj90Qshj3w.gif 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*VC9y_dHhCKFPXj90Qshj3w.gif 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*VC9y_dHhCKFPXj90Qshj3w.gif 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VC9y_dHhCKFPXj90Qshj3w.gif 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*VC9y_dHhCKFPXj90Qshj3w.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*VC9y_dHhCKFPXj90Qshj3w.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*VC9y_dHhCKFPXj90Qshj3w.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*VC9y_dHhCKFPXj90Qshj3w.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*VC9y_dHhCKFPXj90Qshj3w.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*VC9y_dHhCKFPXj90Qshj3w.gif 1100w, https://miro.medium.com/v2/resize:fit:1400/1*VC9y_dHhCKFPXj90Qshj3w.gif 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" class="bh lo mu c" width="700" height="394" loading="lazy" role="presentation"/></picture></div></div></figure><p id="7e9f" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk">For each example in the batch, the weight <em class="og">w</em> generates exactly 6 FLOPs combined in the forward and backward pass:</p><ol class=""><li id="23d7" class="mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw qs nz oa bk">The unit <em class="og">i</em> multiplies its output <em class="og">h(i)</em> by <em class="og">w</em> to send it to the unit <em class="og">j</em>.</li><li id="d708" class="mz na gu nb b nc ob ne nf ng oc ni nj nk od nm nn no oe nq nr ns of nu nv nw qs nz oa bk">The unit <em class="og">j</em> adds the unit <em class="og">i</em>’s contribution to its total input <em class="og">a(j)</em>.</li><li id="c0c6" class="mz na gu nb b nc ob ne nf ng oc ni nj nk od nm nn no oe nq nr ns of nu nv nw qs nz oa bk">The unit <em class="og">j</em> multiplies the incoming loss gradient <em class="og">dL/da(j)</em> by <em class="og">w</em> to send it back to the unit i.</li><li id="19c4" class="mz na gu nb b nc ob ne nf ng oc ni nj nk od nm nn no oe nq nr ns of nu nv nw qs nz oa bk">The unit <em class="og">i</em> adds the unit <em class="og">j</em>’s contribution to its total loss gradient <em class="og">dL/dh(i)</em>.</li><li id="cdaa" class="mz na gu nb b nc ob ne nf ng oc ni nj nk od nm nn no oe nq nr ns of nu nv nw qs nz oa bk">The unit j multiplies its loss gradient <em class="og">dL/da(j) </em>by the unit i’s output <em class="og">h(i)</em> to compute the loss gradient <em class="og">dL/dw</em> <strong class="nb gv">for the given example</strong>.</li><li id="7aba" class="mz na gu nb b nc ob ne nf ng oc ni nj nk od nm nn no oe nq nr ns of nu nv nw qs nz oa bk">(The sneakiest FLOP, IMHO) The weight <em class="og">w</em> adds the contribution from step 5 to its loss gradient accumulator <em class="og">dL/dw </em>that aggregates gradients <strong class="nb gv">for all examples</strong>.</li></ol><p id="40be" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk">The Weight FLOPs Equation directly follows from the fact that we need 6 FLOPs per example per weight. And from this equation follows the Transformer FLOPs Equation. To understand this, think about how many weight matrix multiplication Transformer performs for each input token, no matter how many input sequences the batch consists of. The answer is <strong class="nb gv">exactly 1 for each weight matrix</strong>! So the total number of FLOPs for each token is 6 times the model size <em class="og">N</em>, Q.E.D.</p><h2 id="c44d" class="px oi gu bf oj py pz dy on qa qb ea or nk qc qd qe no qf qg qh ns qi qj qk ql bk">Why only weight FLOPs matter</h2><p id="b763" class="pw-post-body-paragraph mz na gu nb b nc pf ne nf ng pg ni nj nk ph nm nn no pi nq nr ns pj nu nv nw gn bk">I have asked you to swallow the assumption that only FLOPs from mulplications by weight matrices, and you might be wondering now if the assumption is too strong. It is a fair question to ask; it is commonly thought that attention is the bottleneck in Transformers, and here I am boldly brushing it aside. The reason we can do this is because attention only adds <em class="og">O(dL) </em>FLOPs³ per token per layer, whereas matrix multiplications add <em class="og">O(d²)</em> (for more precise counting check out <a class="af nx" href="https://arxiv.org/abs/1909.08053" rel="noopener ugc nofollow" target="_blank">the Megatron paper by Nvidia</a>). And when we talk about <em class="og">really</em> large models, <em class="og">d</em> tends to be considerable larger than <em class="og">L</em>, and hence weight FLOPs dwarf attention FLOPs!</p><p id="d877" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk">The other FLOPs (softmax, layer norm, activations and etc), should be even more negligible, but there is a catch — the GPU memory bandwidth becomes the bottleneck when these operations are performed. In practice these elementwise operations can take non-negligible time. I find it thus helpful to think about the <strong class="nb gv">weight FLOPs (WFLOPs) throughput</strong> that a particular implementation can deliver on a particular hardware.</p><h1 id="fffd" class="oh oi gu bf oj ok ol om on oo op oq or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Estimating the WFLOPs Throughput</h1><p id="0381" class="pw-post-body-paragraph mz na gu nb b nc pf ne nf ng pg ni nj nk ph nm nn no pi nq nr ns pj nu nv nw gn bk">The theory of counting Transformer FLOPs is elegant, but as seen in the HyperCLOVA example, naive application results in significant underestimation of time required to training the language model. For more precise reasoning, we need a better of idea of what actual WFLOPs throughput can be like.</p><p id="dd65" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk">I have done a little case study on a A100 GPU. According to Nvidia documentation, it can deliver up to 312 bfloat16 teraFLOPs — that’s 3.12e14 operations per second! Nvidia docs also show how ~250 teraFLOPs can be actually achieved by doing 4096 x 8192 x 4096 matrix multiplications, and I was able to reproduce that. But what practical throughput can we get when training neural networks?</p><p id="fff6" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk">I have experimented with the <a class="af nx" href="https://huggingface.co/gpt2" rel="noopener ugc nofollow" target="_blank">GPT-2 implementation from Huggingface Transformers</a> and with a <a class="af nx" href="https://gist.github.com/rizar/be08b9b5b2a8aed1a24cc316ba6a6a96" rel="noopener ugc nofollow" target="_blank">bespoke MLP implementation</a> (note that WFLOPs calculus for an MLP is the same as for Transformer). Both models have states of <em class="og">d=</em>1600 dimensions for each input and <em class="og">d_ff</em>=6400 intermediate units. I train MLP on batches of 8192 input vectors; GPT-2 receives 8192 input tokens as <em class="og">B=</em>32 contexts of length <em class="og">L=</em>256 to get an upper bound on the throughput that is less affected by attention (which should become cheaper for larger models). To fit in the single GPU memory, I use only 8 GPT-2 layers.</p><p id="86a4" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk">Here are the throughputs:</p><figure class="qn qo qp qq qr mp"><div class="qt qu l fj"><div class="qv qw l"></div></div></figure><p id="383d" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk">A training step (including backprop) for a linear float16 MLP with no activations yields a throughput of 230 teraWFLOP/s, very close to 237 teraWFLOP/s that I registered for pure matrix multiplications. With float32 weights and <a class="af nx" href="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html" rel="noopener ugc nofollow" target="_blank">mixed precision training</a> (which AFAIK is the standard in training big Transformers these days) the throughput drops to 207.6 teraWFLOPs due to the float32 -&gt; bfloat16 conversion that mixed precision training involves. Adding a ReLU activation and a residual connection causes a further throughput drops to 185.7 teraWFLOPs. This might be surpising but is very much in line with <a class="af nx" href="https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html" rel="noopener ugc nofollow" target="_blank">Nvidia performance documentation</a>, which explains how throughput for activations and elementwise operations is bounded by the memory bandwidth.</p><p id="a728" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk">The MLP throughput looks encouraging, but <strong class="nb gv">for the actual GPT-2 implementation from HuggingFace Transformers the throughput was merely 68 teraWFLOP/s.</strong> I have not looked deeper into the exact breakdown, but a likely explanation is that the memory-intensive computations, such as residual connections, activations, layer normalization, attention masking and attention softmax do cost a lot when combined together.</p><h2 id="a8f6" class="px oi gu bf oj py pz dy on qa qb ea or nk qc qd qe no qf qg qh ns qi qj qk ql bk">WFLOPs throughput in the literature</h2><p id="0e29" class="pw-post-body-paragraph mz na gu nb b nc pf ne nf ng pg ni nj nk ph nm nn no pi nq nr ns pj nu nv nw gn bk">Throughput estimates can also be obtained by looking at white papers. For example, from <a class="af nx" href="https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/" rel="noopener ugc nofollow" target="_blank">here</a>, <a class="af nx" href="http://arxiv.org/abs/2109.04650" rel="noopener ugc nofollow" target="_blank">here</a> and <a class="af nx" href="https://github.com/NVIDIA/Megatron-LM#gpt-3-example" rel="noopener ugc nofollow" target="_blank">here</a> we can estimate the throughput achieved with various forks of Megatron-LM by Nvidia²:</p><figure class="qn qo qp qq qr mp"><div class="qt qu l fj"><div class="qv qw l"></div></div></figure><p id="093f" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk">Note that these numbers are for highly distributed setups, single GPU throughput for Megatron is likely to be much higher, thanks to the extensive used of<a class="af nx" href="https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/fused_bias_gelu.py" rel="noopener ugc nofollow" target="_blank"> fused operations</a>.</p></div></div></div><div class="ab cb pk pl pm pn" role="separator"><span class="po by bm pp pq pr"></span><span class="po by bm pp pq pr"></span><span class="po by bm pp pq"></span></div><div class="gn go gp gq gr"><div class="ab cb"><div class="ci bh fz ga gb gc"><h1 id="d816" class="oh oi gu bf oj ok ps om on oo pt oq or os pu ou ov ow pv oy oz pa pw pc pd pe bk">To Sum It Up</h1><p id="dfd9" class="pw-post-body-paragraph mz na gu nb b nc pf ne nf ng pg ni nj nk ph nm nn no pi nq nr ns pj nu nv nw gn bk">In this article I have shared with you the Transformer FLOPs equation that makes reasoning about extremely large language models easy. The equation ties together the throughput 𝜏, the training time <em class="og">T</em>, the model size <em class="og">N</em> and the number of training tokens <em class="og">T:</em></p><p id="06ff" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk">𝜏<em class="og">T = 6ND.</em></p><p id="1746" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk">Looking at publicly available white papers, the throughput 𝜏 is likely to be anywhere between 50 and 150 teraWFLOP/s per A100 GPU.</p><p id="2ff9" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk">My favorite corollary of this equation is that assuming constant throughput, the training time grows linearly with the model size. So if you want to increase the model size by 2, you have to either use 2 times as many GPUs, or wait 2 times longer. Easy mathematics that you can do in your head!</p><h1 id="c0c4" class="oh oi gu bf oj ok ol om on oo op oq or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Beyond Transformers</h1><p id="ddaa" class="pw-post-body-paragraph mz na gu nb b nc pf ne nf ng pg ni nj nk ph nm nn no pi nq nr ns pj nu nv nw gn bk">As the art of compressing the internet in matrices (a.k.a. language modeling) develops further, the FLOPs calculus might get hairier. For example, the math is different for Mixture of Experts (MoE) models trained by researchers at <a class="af nx" href="https://arxiv.org/pdf/2112.06905.pdf" rel="noopener ugc nofollow" target="_blank">Google</a> and <a class="af nx" href="http://arxiv.org/abs/2112.10684" rel="noopener ugc nofollow" target="_blank">Meta</a>. In these models, only a fraction of model’s weights is active for every input token. The equation can thus be fixed by replacing the total model size <em class="og">N</em> with the number of active weights.</p><p id="a703" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk">The FLOPs calculus for LSTMs would look very similar to that of Transformer, which is a key factor explaining their demise. The total number of FLOPs grows linearly with the model size. But to train LSTMs on long sequences while fitting in GPU memory one has to reduce the batch size. And with a small batch size the GPU throughput for sequential LSTM computations falls dramatically. For example, for 32x1600x6400 matrix multiplication the throughput is below 20 teraFLOP/s, more than 10 times slower than for 8192x1600x6400! Recurrence comes at a price: the computation for further tokens must wait before computations for previous tokens are done, making computations less parallel and thus less GPU-friendly.</p><h1 id="9050" class="oh oi gu bf oj ok ol om on oo op oq or os ot ou ov ow ox oy oz pa pb pc pd pe bk">The End</h1><p id="4e85" class="pw-post-body-paragraph mz na gu nb b nc pf ne nf ng pg ni nj nk ph nm nn no pi nq nr ns pj nu nv nw gn bk">I hope you found this article useful! Many thanks to Harm de Vries, Amine el Hattami, Torsten Scholak, Nicolas Chapados, Sebastien Paquet, and my other fabulous colleagues at ServiceNow Research for discussions that greatly helped me in researching this topic.</p></div></div></div><div class="ab cb pk pl pm pn" role="separator"><span class="po by bm pp pq pr"></span><span class="po by bm pp pq pr"></span><span class="po by bm pp pq"></span></div><div class="gn go gp gq gr"><div class="ab cb"><div class="ci bh fz ga gb gc"><h1 id="a6e5" class="oh oi gu bf oj ok ps om on oo pt oq or os pu ou ov ow pv oy oz pa pw pc pd pe bk"><strong class="al">Footnotes</strong></h1><p id="4f80" class="pw-post-body-paragraph mz na gu nb b nc pf ne nf ng pg ni nj nk ph nm nn no pi nq nr ns pj nu nv nw gn bk">[1] Disclaimer: this is meant to be a popular article, not an academic contribution. While I’m trying to give credit where the credit is due, please don’t freak out if you find this not sufficiently rigorous and just get in touch — I will to try to address the issue.</p><p id="4d32" class="pw-post-body-paragraph mz na gu nb b nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw gn bk">[2] Note that Nvidia reports different teraFLOP/s numbers, namely 138 teraFLOP/s vs 100.8 teraWFLOPs that I calculated. The major source of the difference is that they include the FLOPs needed for the extra forward pass that recomputes the activations. Activation recomputation (checkpointing) allows back-propagation without storing all intermediate states in memory. It is now routinely used to train the largest models. The extra forward pass requires <em class="og">2ND </em>FLOPs. The rest of the difference comes from them including attention and output layer FLOPs. In this article I view that activation recomputation FLOPs do not directly contribute to learning and thus reduce the system’s effective WFLOPs throughput. But if you are an HPC person and what you want to showcase is optimal device utilization, it makes sense to include these FLOPs in the total.</p></div></div></div></div></section></div></div></article></div><div class="ab cb"><div class="ci bh fz ga gb gc"><div class="qx qy ab iv"><div class="qz ab"><a class="ra ay am ao" rel="noopener follow" href="/tag/language-model?source=post_page-----3b19c1f025e4--------------------------------"><div class="rb fj cx rc ge rd re bf b bg z bk rf">Language Model</div></a></div><div class="qz ab"><a class="ra ay am ao" rel="noopener follow" href="/tag/flip-flops?source=post_page-----3b19c1f025e4--------------------------------"><div class="rb fj cx rc ge rd re bf b bg z bk rf">Flip Flops</div></a></div><div class="qz ab"><a class="ra ay am ao" rel="noopener follow" href="/tag/deep-learning?source=post_page-----3b19c1f025e4--------------------------------"><div class="rb fj cx rc ge rd re bf b bg z bk rf">Deep Learning</div></a></div><div class="qz ab"><a class="ra ay am ao" rel="noopener follow" href="/tag/artificial-intelligence?source=post_page-----3b19c1f025e4--------------------------------"><div class="rb fj cx rc ge rd re bf b bg z bk rf">Artificial Intelligence</div></a></div><div class="qz ab"><a class="ra ay am ao" rel="noopener follow" href="/tag/neural-networks?source=post_page-----3b19c1f025e4--------------------------------"><div class="rb fj cx rc ge rd re bf b bg z bk rf">Neural Networks</div></a></div></div></div></div><div class="l"></div><footer class="rg pl rh ri rj ab q rk rl c"><div class="l ae"><div class="ab cb"><div class="ci bh fz ga gb gc"><div class="ab cp rm"><div class="ab q ke"><div class="rn l"><span class="l ro rp rq e d"><div class="ab q ke kf"><div class="pw-multi-vote-icon fj kg kh ki kj"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="footerClapButton" rel="noopener follow" href="/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F3b19c1f025e4&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40dzmitrybahdanau%2Fthe-flops-calculus-of-language-model-training-3b19c1f025e4&amp;user=Dzmitry+Bahdanau&amp;userId=e663600ab469&amp;source=---footer_actions--3b19c1f025e4---------------------clap_footer-----------"><div><div class="bm" aria-hidden="false"><div class="kk ao kl km kn ko am kp kq kr kj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM13.916 3.953l1.523-2.112-1.184-.39zM8.589 1.84l1.522 2.112-.337-2.501zM18.523 18.92c-.86.86-1.75 1.246-2.62 1.33a6 6 0 0 0 .407-.372c2.388-2.389 2.86-4.951 1.399-7.623l-.912-1.603-.79-1.672c-.26-.56-.194-.98.203-1.288a.7.7 0 0 1 .546-.132c.283.046.546.231.728.5l2.363 4.157c.976 1.624 1.141 4.237-1.324 6.702m-10.999-.438L3.37 14.328a.828.828 0 0 1 .585-1.408.83.83 0 0 1 .585.242l2.158 2.157a.365.365 0 0 0 .516-.516l-2.157-2.158-1.449-1.449a.826.826 0 0 1 1.167-1.17l3.438 3.44a.363.363 0 0 0 .516 0 .364.364 0 0 0 0-.516L5.293 9.513l-.97-.97a.826.826 0 0 1 0-1.166.84.84 0 0 1 1.167 0l.97.968 3.437 3.436a.36.36 0 0 0 .517 0 .366.366 0 0 0 0-.516L6.977 7.83a.82.82 0 0 1-.241-.584.82.82 0 0 1 .824-.826c.219 0 .43.087.584.242l5.787 5.787a.366.366 0 0 0 .587-.415l-1.117-2.363c-.26-.56-.194-.98.204-1.289a.7.7 0 0 1 .546-.132c.283.046.545.232.727.501l2.193 3.86c1.302 2.38.883 4.59-1.277 6.75-1.156 1.156-2.602 1.627-4.19 1.367-1.418-.236-2.866-1.033-4.079-2.246M10.75 5.971l2.12 2.12c-.41.502-.465 1.17-.128 1.89l.22.465-3.523-3.523a.8.8 0 0 1-.097-.368c0-.22.086-.428.241-.584a.847.847 0 0 1 1.167 0m7.355 1.705c-.31-.461-.746-.758-1.23-.837a1.44 1.44 0 0 0-1.11.275c-.312.24-.505.543-.59.881a1.74 1.74 0 0 0-.906-.465 1.47 1.47 0 0 0-.82.106l-2.182-2.182a1.56 1.56 0 0 0-2.2 0 1.54 1.54 0 0 0-.396.701 1.56 1.56 0 0 0-2.21-.01 1.55 1.55 0 0 0-.416.753c-.624-.624-1.649-.624-2.237-.037a1.557 1.557 0 0 0 0 2.2c-.239.1-.501.238-.715.453a1.56 1.56 0 0 0 0 2.2l.516.515a1.556 1.556 0 0 0-.753 2.615L7.01 19c1.32 1.319 2.909 2.189 4.475 2.449q.482.08.971.08c.85 0 1.653-.198 2.393-.579.231.033.46.054.686.054 1.266 0 2.457-.52 3.505-1.567 2.763-2.763 2.552-5.734 1.439-7.586z" clip-rule="evenodd"></path></svg></div></div></div></a></span></div><div class="pw-multi-vote-count l ks kt ku kv kw kx ky"><p class="bf b dv z du"><span class="kz">--</span></p></div></div></span><span class="l h g f rr rs"><div class="ab q ke kf"><div class="pw-multi-vote-icon fj kg kh ki kj"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="footerClapButton" rel="noopener follow" href="/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F3b19c1f025e4&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40dzmitrybahdanau%2Fthe-flops-calculus-of-language-model-training-3b19c1f025e4&amp;user=Dzmitry+Bahdanau&amp;userId=e663600ab469&amp;source=---footer_actions--3b19c1f025e4---------------------clap_footer-----------"><div><div class="bm" aria-hidden="false"><div class="kk ao kl km kn ko am kp kq kr kj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM13.916 3.953l1.523-2.112-1.184-.39zM8.589 1.84l1.522 2.112-.337-2.501zM18.523 18.92c-.86.86-1.75 1.246-2.62 1.33a6 6 0 0 0 .407-.372c2.388-2.389 2.86-4.951 1.399-7.623l-.912-1.603-.79-1.672c-.26-.56-.194-.98.203-1.288a.7.7 0 0 1 .546-.132c.283.046.546.231.728.5l2.363 4.157c.976 1.624 1.141 4.237-1.324 6.702m-10.999-.438L3.37 14.328a.828.828 0 0 1 .585-1.408.83.83 0 0 1 .585.242l2.158 2.157a.365.365 0 0 0 .516-.516l-2.157-2.158-1.449-1.449a.826.826 0 0 1 1.167-1.17l3.438 3.44a.363.363 0 0 0 .516 0 .364.364 0 0 0 0-.516L5.293 9.513l-.97-.97a.826.826 0 0 1 0-1.166.84.84 0 0 1 1.167 0l.97.968 3.437 3.436a.36.36 0 0 0 .517 0 .366.366 0 0 0 0-.516L6.977 7.83a.82.82 0 0 1-.241-.584.82.82 0 0 1 .824-.826c.219 0 .43.087.584.242l5.787 5.787a.366.366 0 0 0 .587-.415l-1.117-2.363c-.26-.56-.194-.98.204-1.289a.7.7 0 0 1 .546-.132c.283.046.545.232.727.501l2.193 3.86c1.302 2.38.883 4.59-1.277 6.75-1.156 1.156-2.602 1.627-4.19 1.367-1.418-.236-2.866-1.033-4.079-2.246M10.75 5.971l2.12 2.12c-.41.502-.465 1.17-.128 1.89l.22.465-3.523-3.523a.8.8 0 0 1-.097-.368c0-.22.086-.428.241-.584a.847.847 0 0 1 1.167 0m7.355 1.705c-.31-.461-.746-.758-1.23-.837a1.44 1.44 0 0 0-1.11.275c-.312.24-.505.543-.59.881a1.74 1.74 0 0 0-.906-.465 1.47 1.47 0 0 0-.82.106l-2.182-2.182a1.56 1.56 0 0 0-2.2 0 1.54 1.54 0 0 0-.396.701 1.56 1.56 0 0 0-2.21-.01 1.55 1.55 0 0 0-.416.753c-.624-.624-1.649-.624-2.237-.037a1.557 1.557 0 0 0 0 2.2c-.239.1-.501.238-.715.453a1.56 1.56 0 0 0 0 2.2l.516.515a1.556 1.556 0 0 0-.753 2.615L7.01 19c1.32 1.319 2.909 2.189 4.475 2.449q.482.08.971.08c.85 0 1.653-.198 2.393-.579.231.033.46.054.686.054 1.266 0 2.457-.52 3.505-1.567 2.763-2.763 2.552-5.734 1.439-7.586z" clip-rule="evenodd"></path></svg></div></div></div></a></span></div><div class="pw-multi-vote-count l ks kt ku kv kw kx ky"><p class="bf b dv z du"><span class="kz">--</span></p></div></div></span></div><div class="bq ab"><div><div class="bm" aria-hidden="false"><button class="ao kk lc ld ab q fk le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"></path></svg><p class="bf b bg z du"><span class="pw-responses-count la lb">3</span></p></button></div></div></div></div><div class="ab q"><div class="pr l is"><div><div class="bm" aria-hidden="false"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="footerBookmarkButton" rel="noopener follow" href="/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3b19c1f025e4&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40dzmitrybahdanau%2Fthe-flops-calculus-of-language-model-training-3b19c1f025e4&amp;source=---footer_actions--3b19c1f025e4---------------------bookmark_footer-----------"><svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" fill="none" viewBox="0 0 25 25" class="du lh" aria-label="Add to list bookmark button"><path fill="currentColor" d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .805.396L12.5 17l5.695 4.396A.5.5 0 0 0 19 21v-8.5a.5.5 0 0 0-1 0v7.485l-5.195-4.012a.5.5 0 0 0-.61 0L7 19.985z"></path></svg></a></span></div></div></div><div class="pr l is"><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="footerSocialShareButton" class="af fk ah ai aj ak al lp an ao ap ex lq lr lf ls"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div></footer><div class="rt ru rv rw rx l"><div class="ab cb"><div class="ci bh fz ga gb gc"><div class="ry bh r rz"></div><div class="ab sa sb sc iu it"><div class="sd se sf sg sh si sj sk sl sm ab cp"><div class="h k"><a tabindex="0" rel="noopener follow" href="/@dzmitrybahdanau?source=post_page---post_author_info--3b19c1f025e4--------------------------------"><div class="l fj"><img alt="Dzmitry Bahdanau" class="l fd by ic ib cx" src="https://miro.medium.com/v2/resize:fill:96:96/1*4ycUaU0RGJDPwaSfQrfmgg.jpeg" width="48" height="48" loading="lazy"/><div class="fr by l ic ib fs n ay sn"></div></div></a></div><div class="j i d"><a tabindex="0" rel="noopener follow" href="/@dzmitrybahdanau?source=post_page---post_author_info--3b19c1f025e4--------------------------------"><div class="l fj"><img alt="Dzmitry Bahdanau" class="l fd by so sp cx" src="https://miro.medium.com/v2/resize:fill:128:128/1*4ycUaU0RGJDPwaSfQrfmgg.jpeg" width="64" height="64" loading="lazy"/><div class="fr by l so sp fs n ay sn"></div></div></a></div><div class="j i d sq is"><div class="ab"><span><button class="bf b bg z sr rb ss st su sv sw ev ew sx sy sz fa fb fc fd bm fe ff">Follow</button></span></div></div></div><div class="ab co ta"><div class="tb tc td te tf l"><a class="af ag ah aj ak al am an ao ap aq ar as at ab q" rel="noopener follow" href="/@dzmitrybahdanau?source=post_page---post_author_info--3b19c1f025e4--------------------------------"><h2 class="pw-author-name bf th ti tj tk tl tm tn nk qd qe no qg qh ns qj qk bk"><span class="gn tg">Written by <!-- -->Dzmitry Bahdanau</span></h2></a><div class="qz ab ia"><div class="l is"><span class="pw-follower-count bf b bg z du"><a class="af ag ah ai aj ak al am an ao ap aq ar il" rel="noopener follow" href="/@dzmitrybahdanau/followers?source=post_page---post_author_info--3b19c1f025e4--------------------------------">180 Followers</a></span></div><div class="bf b bg z du ab to"><span class="im l" aria-hidden="true"><span class="bf b bg z du">·</span></span><a class="af ag ah ai aj ak al am an ao ap aq ar il" rel="noopener follow" href="/@dzmitrybahdanau/following?source=post_page---post_author_info--3b19c1f025e4--------------------------------">36 Following</a></div></div><div class="tp l"><p class="bf b bg z bk"><span class="gn">Research Scientist at ServiceNow Research</span></p></div></div></div><div class="h k"><div class="ab"><span><button class="bf b bg z sr rb ss st su sv sw ev ew sx sy sz fa fb fc fd bm fe ff">Follow</button></span></div></div></div></div></div></div><div class="tq tr ts tt tu l"><div class="ry bh r tq tr tv tw tx"></div><div class="ab cb"><div class="ci bh fz ga gb gc"><div class="ab q cp"><h2 class="bf th ok om on oo oq or os ou ov ow oy oz pa pc pd bk">Responses (<!-- -->3<!-- -->)</h2><div class="ab ty"><div><div class="bm" aria-hidden="false"><a class="tz ua" href="https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--3b19c1f025e4--------------------------------" rel="noopener follow" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" viewBox="0 0 25 25"><path fill-rule="evenodd" d="M11.987 5.036a.754.754 0 0 1 .914-.01c.972.721 1.767 1.218 2.6 1.543.828.322 1.719.485 2.887.505a.755.755 0 0 1 .741.757c-.018 3.623-.43 6.256-1.449 8.21-1.034 1.984-2.662 3.209-4.966 4.083a.75.75 0 0 1-.537-.003c-2.243-.874-3.858-2.095-4.897-4.074-1.024-1.951-1.457-4.583-1.476-8.216a.755.755 0 0 1 .741-.757c1.195-.02 2.1-.182 2.923-.503.827-.322 1.6-.815 2.519-1.535m.468.903c-.897.69-1.717 1.21-2.623 1.564-.898.35-1.856.527-3.026.565.037 3.45.469 5.817 1.36 7.515.884 1.684 2.25 2.762 4.284 3.571 2.092-.81 3.465-1.89 4.344-3.575.886-1.698 1.299-4.065 1.334-7.512-1.149-.039-2.091-.217-2.99-.567-.906-.353-1.745-.873-2.683-1.561m-.009 9.155a2.672 2.672 0 1 0 0-5.344 2.672 2.672 0 0 0 0 5.344m0 1a3.672 3.672 0 1 0 0-7.344 3.672 3.672 0 0 0 0 7.344m-1.813-3.777.525-.526.916.917 1.623-1.625.526.526-2.149 2.152z" clip-rule="evenodd"></path></svg></a></div></div></div></div><div class="pk l"><button class="bf b bg z bk rb ub uc ud lh le sw ev ew ex ue uf ug fa uh ui uj uk ul fb fc fd bm fe ff">See all responses</button></div></div></div></div><div class="um un uo up uq l bx"><div class="h k j"><div class="ry bh ur us"></div><div class="ab cb"><div class="ci bh fz ga gb gc"><div class="ut ab ke iv"><div class="uu uv l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://help.medium.com/hc/en-us?source=post_page-----3b19c1f025e4--------------------------------" rel="noopener follow"><p class="bf b dv z du">Help</p></a></div><div class="uu uv l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.statuspage.io/?source=post_page-----3b19c1f025e4--------------------------------" rel="noopener follow"><p class="bf b dv z du">Status</p></a></div><div class="uu uv l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="/about?autoplay=1&amp;source=post_page-----3b19c1f025e4--------------------------------"><p class="bf b dv z du">About</p></a></div><div class="uu uv l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----3b19c1f025e4--------------------------------"><p class="bf b dv z du">Careers</p></a></div><div class="uu uv l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="pressinquiries@medium.com?source=post_page-----3b19c1f025e4--------------------------------" rel="noopener follow"><p class="bf b dv z du">Press</p></a></div><div class="uu uv l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://blog.medium.com/?source=post_page-----3b19c1f025e4--------------------------------" rel="noopener follow"><p class="bf b dv z du">Blog</p></a></div><div class="uu uv l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3b19c1f025e4--------------------------------" rel="noopener follow"><p class="bf b dv z du">Privacy</p></a></div><div class="uu uv l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3b19c1f025e4--------------------------------" rel="noopener follow"><p class="bf b dv z du">Terms</p></a></div><div class="uu uv l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://speechify.com/medium?source=post_page-----3b19c1f025e4--------------------------------" rel="noopener follow"><p class="bf b dv z du">Text to speech</p></a></div><div class="uu l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="/business?source=post_page-----3b19c1f025e4--------------------------------"><p class="bf b dv z du">Teams</p></a></div></div></div></div></div></div></div></div></div></div><script>window.__BUILD_ID__="main-20241202-170510-51d39927f0"</script><script>window.__GRAPHQL_URI__ = "https://medium.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"algolia":{"queries":{}},"cache":{"experimentGroupSet":true,"reason":"","group":"enabled","tags":["group-edgeCachePosts","post-3b19c1f025e4","user-e663600ab469"],"serverVariantState":"44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a","middlewareEnabled":true,"cacheStatus":"DYNAMIC","shouldUseCache":true,"vary":[],"lohpSummerUpsellEnabled":false,"publicationHierarchyEnabledWeb":false,"postBottomResponsesEnabled":false},"client":{"hydrated":false,"isUs":false,"isNativeMedium":false,"isSafariMobile":false,"isSafari":false,"isFirefox":true,"routingEntity":{"type":"DEFAULT","explicit":false},"viewerIsBot":false},"debug":{"requestId":"96bca4c8-c8eb-46cb-ac8f-1a0987bda107","hybridDevServices":[],"originalSpanCarrier":{"traceparent":"00-d0250edae22d24df8e0cfed0b2aefdcc-b7233704872cd42e-01"}},"multiVote":{"clapsPerPost":{}},"navigation":{"branch":{"show":null,"hasRendered":null,"blockedByCTA":false},"hideGoogleOneTap":false,"hasRenderedAlternateUserBanner":null,"currentLocation":"https:\u002F\u002Fmedium.com\u002F@dzmitrybahdanau\u002Fthe-flops-calculus-of-language-model-training-3b19c1f025e4","host":"medium.com","hostname":"medium.com","referrer":"","hasSetReferrer":false,"susiModal":{"step":null,"operation":"register"},"postRead":false,"partnerProgram":{"selectedCountryCode":null},"queryString":"","currentHash":""},"config":{"nodeEnv":"production","version":"main-20241202-170510-51d39927f0","target":"production","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","recaptcha3Key":"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5","recaptchaEnterpriseKeyId":"6Le-uGgpAAAAAPprRaokM8AKthQ9KNGdoxaGUvVp","datadog":{"applicationId":"6702d87d-a7e0-42fe-bbcb-95b469547ea0","clientToken":"pub853ea8d17ad6821d9f8f11861d23dfed","rumToken":"pubf9cc52896502b9413b68ba36fc0c7162","context":{"deployment":{"target":"production","tag":"main-20241202-170510-51d39927f0","commit":"51d39927f0c2bd5e6674cfac7ea9a5bb687a10a8"}},"datacenter":"us"},"googleAnalyticsCode":"G-7JY7T788PK","googlePay":{"apiVersion":"2","apiVersionMinor":"0","merchantId":"BCR2DN6TV7EMTGBM","merchantName":"Medium","instanceMerchantId":"13685562959212738550"},"applePay":{"version":3},"signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumMastodonDomainName":"me.dm","mediumOwnedAndOperatedCollectionIds":["8a9336e5bb4","b7e45b22fec3","193b68bd4fba","8d6b8a439e32","54c98c43354d","3f6ecf56618","d944778ce714","92d2092dc598","ae2a65f35510","1285ba81cada","544c7006046e","fc8964313712","40187e704f1c","88d9857e584e","7b6769f2748b","bcc38c8f6edf","cef6983b292","cb8577c9149e","444d13b52878","713d7dbc99b0","ef8e90590e66","191186aaafa0","55760f21cdc5","9dc80918cc93","bdc4052bbdba","8ccfed20cbb2"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"topicsToFollow":["d61cf867d93f","8a146bc21b28","1eca0103fff3","4d562ee63426","aef1078a3ef5","e15e46793f8d","6158eb913466","55f1c20aba7a","3d18b94f6858","4861fee224fd","63c6f1f93ee","1d98b3a9a871","decb52b64abf","ae5d4995e225","830cded25262"],"topicToTagMappings":{"accessibility":"accessibility","addiction":"addiction","android-development":"android-development","art":"art","artificial-intelligence":"artificial-intelligence","astrology":"astrology","basic-income":"basic-income","beauty":"beauty","biotech":"biotech","blockchain":"blockchain","books":"books","business":"business","cannabis":"cannabis","cities":"cities","climate-change":"climate-change","comics":"comics","coronavirus":"coronavirus","creativity":"creativity","cryptocurrency":"cryptocurrency","culture":"culture","cybersecurity":"cybersecurity","data-science":"data-science","design":"design","digital-life":"digital-life","disability":"disability","economy":"economy","education":"education","equality":"equality","family":"family","feminism":"feminism","fiction":"fiction","film":"film","fitness":"fitness","food":"food","freelancing":"freelancing","future":"future","gadgets":"gadgets","gaming":"gaming","gun-control":"gun-control","health":"health","history":"history","humor":"humor","immigration":"immigration","ios-development":"ios-development","javascript":"javascript","justice":"justice","language":"language","leadership":"leadership","lgbtqia":"lgbtqia","lifestyle":"lifestyle","machine-learning":"machine-learning","makers":"makers","marketing":"marketing","math":"math","media":"media","mental-health":"mental-health","mindfulness":"mindfulness","money":"money","music":"music","neuroscience":"neuroscience","nonfiction":"nonfiction","outdoors":"outdoors","parenting":"parenting","pets":"pets","philosophy":"philosophy","photography":"photography","podcasts":"podcast","poetry":"poetry","politics":"politics","privacy":"privacy","product-management":"product-management","productivity":"productivity","programming":"programming","psychedelics":"psychedelics","psychology":"psychology","race":"race","relationships":"relationships","religion":"religion","remote-work":"remote-work","san-francisco":"san-francisco","science":"science","self":"self","self-driving-cars":"self-driving-cars","sexuality":"sexuality","social-media":"social-media","society":"society","software-engineering":"software-engineering","space":"space","spirituality":"spirituality","sports":"sports","startups":"startup","style":"style","technology":"technology","transportation":"transportation","travel":"travel","true-crime":"true-crime","tv":"tv","ux":"ux","venture-capital":"venture-capital","visual-design":"visual-design","work":"work","world":"world","writing":"writing"},"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"7*V1_7XP4snlmqrc_0Njontw.png","height":110,"width":500},"postLogo":{"imageId":"bd978bb536350a710e8efb012513429cabdc4c28700604261aeda246d0f980b7","height":810,"width":1440},"postPreviewImage":{"imageId":"1*hn4v1tCaJy7cWMyb0bpNpQ.png","height":386,"width":579}},"collectionStructuredData":{"8d6b8a439e32":{"name":"Elemental","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F980\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png","width":980,"height":159}}},"3f6ecf56618":{"name":"Forge","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F596\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png","width":596,"height":183}}},"ae2a65f35510":{"name":"GEN","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F264\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png","width":264,"height":140}}},"88d9857e584e":{"name":"LEVEL","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png","width":540,"height":108}}},"7b6769f2748b":{"name":"Marker","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F383\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png","width":383,"height":92}}},"444d13b52878":{"name":"OneZero","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*cw32fIqCbRWzwJaoQw6BUg.png","width":540,"height":123}}},"8ccfed20cbb2":{"name":"Zora","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png","width":540,"height":106}}}},"embeddedPostIds":{"coronavirus":"cd3010f9d81f"},"sharedCdcMessaging":{"COVID_APPLICABLE_TAG_SLUGS":[],"COVID_APPLICABLE_TOPIC_NAMES":[],"COVID_APPLICABLE_TOPIC_NAMES_FOR_TOPIC_PAGE":[],"COVID_MESSAGES":{"tierA":{"text":"For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":66,"end":73,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"tierB":{"text":"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.","markups":[{"start":37,"end":45,"href":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Fcategories\u002F201931128-Policies-Safety"},{"start":125,"end":132,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"paywall":{"text":"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":56,"end":70,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":138,"end":145,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"unbound":{"text":"This article is free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":45,"end":59,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":127,"end":134,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]}},"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST":["3b31a67bff4a"]},"sharedVoteMessaging":{"TAGS":["politics","election-2020","government","us-politics","election","2020-presidential-race","trump","donald-trump","democrats","republicans","congress","republican-party","democratic-party","biden","joe-biden","maga"],"TOPICS":["politics","election"],"MESSAGE":{"text":"Find out more about the U.S. election results here.","markups":[{"start":46,"end":50,"href":"https:\u002F\u002Fcookpolitical.com\u002F2020-national-popular-vote-tracker"}]},"EXCLUDE_POSTS":["397ef29e3ca5"]},"embedPostRules":[],"recircOptions":{"v1":{"limit":3},"v2":{"limit":8}},"braintreeClientKey":"production_zjkj96jm_m56f8fqpf7ngnrd4","braintree":{"enabled":true,"merchantId":"m56f8fqpf7ngnrd4","merchantAccountId":{"usd":"AMediumCorporation_instant","eur":"amediumcorporation_EUR","cad":"amediumcorporation_CAD"},"publicKey":"ds2nn34bg2z7j5gd","braintreeEnvironment":"production","dashboardUrl":"https:\u002F\u002Fwww.braintreegateway.com\u002Fmerchants","gracePeriodDurationInDays":14,"mediumMembershipPlanId":{"monthly":"ce105f8c57a3","monthlyV2":"e8a5e126-792b-4ee6-8fba-d574c1b02fc5","monthlyWithTrial":"d5ee3dbe3db8","monthlyPremium":"fa741a9b47a2","yearly":"a40ad4a43185","yearlyV2":"3815d7d6-b8ca-4224-9b8c-182f9047866e","yearlyStaff":"d74fb811198a","yearlyWithTrial":"b3bc7350e5c7","yearlyPremium":"e21bd2c12166","monthlyOneYearFree":"e6c0637a-2bad-4171-ab4f-3c268633d83c","monthly25PercentOffFirstYear":"235ecc62-0cdb-49ae-9378-726cd21c504b","monthly20PercentOffFirstYear":"ba518864-9c13-4a99-91ca-411bf0cac756","monthly15PercentOffFirstYear":"594c029b-9f89-43d5-88f8-8173af4e070e","monthly10PercentOffFirstYear":"c6c7bc9a-40f2-4b51-8126-e28511d5bdb0","monthlyForStudents":"629ebe51-da7d-41fd-8293-34cd2f2030a8","yearlyOneYearFree":"78ba7be9-0d9f-4ece-aa3e-b54b826f2bf1","yearly25PercentOffFirstYear":"2dbb010d-bb8f-4eeb-ad5c-a08509f42d34","yearly20PercentOffFirstYear":"47565488-435b-47f8-bf93-40d5fbe0ebc8","yearly15PercentOffFirstYear":"8259809b-0881-47d9-acf7-6c001c7f720f","yearly10PercentOffFirstYear":"9dd694fb-96e1-472c-8d9e-3c868d5c1506","yearlyForStudents":"e29345ef-ab1c-4234-95c5-70e50fe6bc23","monthlyCad":"p52orjkaceei","yearlyCad":"h4q9g2up9ktt"},"braintreeDiscountId":{"oneMonthFree":"MONTHS_FREE_01","threeMonthsFree":"MONTHS_FREE_03","sixMonthsFree":"MONTHS_FREE_06","fiftyPercentOffOneYear":"FIFTY_PERCENT_OFF_ONE_YEAR"},"3DSecureVersion":"2","defaultCurrency":"usd","providerPlanIdCurrency":{"4ycw":"usd","rz3b":"usd","3kqm":"usd","jzw6":"usd","c2q2":"usd","nnsw":"usd","q8qw":"usd","d9y6":"usd","fx7w":"cad","nwf2":"cad"}},"paypalClientId":"AXj1G4fotC2GE8KzWX9mSxCH1wmPE3nJglf4Z2ig_amnhvlMVX87otaq58niAg9iuLktVNF_1WCMnN7v","paypal":{"host":"https:\u002F\u002Fapi.paypal.com:443","clientMode":"production","serverMode":"live","webhookId":"4G466076A0294510S","monthlyPlan":{"planId":"P-9WR0658853113943TMU5FDQA","name":"Medium Membership (Monthly) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"yearlyPlan":{"planId":"P-7N8963881P8875835MU5JOPQ","name":"Medium Membership (Annual) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"oneYearGift":{"name":"Medium Membership (1 Year, Digital Gift Code)","description":"Unlimited access to the best and brightest stories on Medium. Gift codes can be redeemed at medium.com\u002Fredeem.","price":"50.00","currency":"USD","sku":"membership-gift-1-yr"},"oldMonthlyPlan":{"planId":"P-96U02458LM656772MJZUVH2Y","name":"Medium Membership (Monthly)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"oldYearlyPlan":{"planId":"P-59P80963JF186412JJZU3SMI","name":"Medium Membership (Annual)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"monthlyPlanWithTrial":{"planId":"P-66C21969LR178604GJPVKUKY","name":"Medium Membership (Monthly) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"yearlyPlanWithTrial":{"planId":"P-6XW32684EX226940VKCT2MFA","name":"Medium Membership (Annual) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"oldMonthlyPlanNoSetupFee":{"planId":"P-4N046520HR188054PCJC7LJI","name":"Medium Membership (Monthly)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"oldYearlyPlanNoSetupFee":{"planId":"P-7A4913502Y5181304CJEJMXQ","name":"Medium Membership (Annual)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"sdkUrl":"https:\u002F\u002Fwww.paypal.com\u002Fsdk\u002Fjs"},"stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl","log":{"json":true,"level":"info"},"imageUploadMaxSizeMb":25,"staffPicks":{"title":"Staff Picks","catalogId":"c7bc6e1ee00f"}},"session":{"xsrf":""}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY":{"__typename":"Query","viewer":null,"collectionByDomainOrSlug({\"domainOrSlug\":\"medium.com\"})":null,"postResult({\"id\":\"3b19c1f025e4\"})":{"__ref":"Post:3b19c1f025e4"}},"LinkedAccounts:e663600ab469":{"__typename":"LinkedAccounts","mastodon":null,"id":"e663600ab469"},"UserViewerEdge:userId:e663600ab469-viewerId:lo_7fabea0c44e5":{"__typename":"UserViewerEdge","id":"userId:e663600ab469-viewerId:lo_7fabea0c44e5","isFollowing":false,"isUser":false,"isMuting":false},"NewsletterV3:c6bef16ab630":{"__typename":"NewsletterV3","id":"c6bef16ab630","type":"NEWSLETTER_TYPE_AUTHOR","slug":"e663600ab469","name":"e663600ab469","collection":null,"user":{"__ref":"User:e663600ab469"}},"User:e663600ab469":{"__typename":"User","id":"e663600ab469","name":"Dzmitry Bahdanau","username":"dzmitrybahdanau","newsletterV3":{"__ref":"NewsletterV3:c6bef16ab630"},"linkedAccounts":{"__ref":"LinkedAccounts:e663600ab469"},"isSuspended":false,"imageId":"1*4ycUaU0RGJDPwaSfQrfmgg.jpeg","mediumMemberAt":1640381107000,"verifications":{"__typename":"VerifiedInfo","isBookAuthor":false},"socialStats":{"__typename":"SocialStats","followerCount":180,"followingCount":36,"collectionFollowingCount":0},"customDomainState":null,"hasSubdomain":false,"bio":"Research Scientist at ServiceNow Research","isPartnerProgramEnrolled":false,"viewerEdge":{"__ref":"UserViewerEdge:userId:e663600ab469-viewerId:lo_7fabea0c44e5"},"viewerIsUser":false,"postSubscribeMembershipUpsellShownAt":0,"membership":{"__ref":"Membership:66fc56b15d61"},"allowNotes":true,"twitterScreenName":""},"Membership:66fc56b15d61":{"__typename":"Membership","tier":"MEMBER","id":"66fc56b15d61"},"Topic:1eca0103fff3":{"__typename":"Topic","slug":"machine-learning","id":"1eca0103fff3","name":"Machine Learning"},"Paragraph:578c2017870b_0":{"__typename":"Paragraph","id":"578c2017870b_0","name":"64a6","type":"H3","href":null,"layout":null,"metadata":null,"text":"The FLOPs Calculus of Language Model Training","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*sG59WF5K7qLZr307.jpg":{"__typename":"ImageMetadata","id":"0*sG59WF5K7qLZr307.jpg","originalHeight":768,"originalWidth":1024,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:578c2017870b_1":{"__typename":"Paragraph","id":"578c2017870b_1","name":"1289","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:0*sG59WF5K7qLZr307.jpg"},"text":"Training a large Transformer requires many [flip-]FLOPs","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_2":{"__typename":"Paragraph","id":"578c2017870b_2","name":"eb0f","type":"P","href":null,"layout":null,"metadata":null,"text":"Extremely large language models like the famous GPT-3 by OpenAI are all the rage. Many of us are now trying to get a sense of scale of the compute that goes into training them.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":48,"end":63,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F2005.14165","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_3":{"__typename":"Paragraph","id":"578c2017870b_3","name":"58a3","type":"P","href":null,"layout":null,"metadata":null,"text":"In this article, I will offer you a very useful tool to reason about large Transformer LMs. This tool will help you roughly answer questions like","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_4":{"__typename":"Paragraph","id":"578c2017870b_4","name":"2fb5","type":"ULI","href":null,"layout":null,"metadata":null,"text":"How much does it cost to train GPT-3?","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_5":{"__typename":"Paragraph","id":"578c2017870b_5","name":"19f8","type":"ULI","href":null,"layout":null,"metadata":null,"text":"How long will training this big model take me?","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_6":{"__typename":"Paragraph","id":"578c2017870b_6","name":"3915","type":"P","href":null,"layout":null,"metadata":null,"text":"Turns out quick back-of-the-envelope calculations can be sufficient to answer these questions if you use a simple equation that ties together","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_7":{"__typename":"Paragraph","id":"578c2017870b_7","name":"be48","type":"ULI","href":null,"layout":null,"metadata":null,"text":"the compute required to train a Transformer model ( C )","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":52,"end":55,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_8":{"__typename":"Paragraph","id":"578c2017870b_8","name":"704f","type":"ULI","href":null,"layout":null,"metadata":null,"text":"its number of parameters, or model size ( N )","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":42,"end":43,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_9":{"__typename":"Paragraph","id":"578c2017870b_9","name":"6bf0","type":"ULI","href":null,"layout":null,"metadata":null,"text":"the number of tokens that the model is trained on ( D )","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":52,"end":53,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_10":{"__typename":"Paragraph","id":"578c2017870b_10","name":"d027","type":"P","href":null,"layout":null,"metadata":null,"text":"Without further ado, meet the Transformer FLOPs Equation:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":30,"end":56,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_11":{"__typename":"Paragraph","id":"578c2017870b_11","name":"8f3d","type":"P","href":null,"layout":null,"metadata":null,"text":"C ≈ 6ND.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":0,"end":8,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_12":{"__typename":"Paragraph","id":"578c2017870b_12","name":"b9b0","type":"P","href":null,"layout":null,"metadata":null,"text":"A slightly more sophisticated version of the equation expresses the compute C as the product of cluster’s throughput 𝜏 and training time T:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":138,"end":139,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_13":{"__typename":"Paragraph","id":"578c2017870b_13","name":"3b25","type":"P","href":null,"layout":null,"metadata":null,"text":"𝜏T = 6ND.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":2,"end":10,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_14":{"__typename":"Paragraph","id":"578c2017870b_14","name":"5044","type":"H3","href":null,"layout":null,"metadata":null,"text":"Example","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_15":{"__typename":"Paragraph","id":"578c2017870b_15","name":"e960","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s apply the Transformer FLOPs Equation to some middle-school style problem solving:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_16":{"__typename":"Paragraph","id":"578c2017870b_16","name":"57a3","type":"P","href":null,"layout":null,"metadata":null,"text":"Problem An 82B parameter Korean variant of GPT-3 called HyperCLOVA was trained on 150B tokens using a cluster of 1024 Nvidia A100 GPUs. How long could that take?","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":56,"end":66,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F2109.04650","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":0,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_17":{"__typename":"Paragraph","id":"578c2017870b_17","name":"d7b7","type":"P","href":null,"layout":null,"metadata":null,"text":"Solution The peak float16 FLOPs throughput of A100 is 𝜏 = 312 teraFLOPs = 3.12e14 FLOPs. The total compute is C = 6 ∙ 8.2e10 ∙ 1.5e11 = 7.38e22. The training must have taken at least T = C \u002F 1024𝜏 \u002F 86400 = 2.67 days.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":13,"end":50,"href":"https:\u002F\u002Fwww.nvidia.com\u002Fcontent\u002Fdam\u002Fen-zz\u002FSolutions\u002FData-Center\u002Fa100\u002Fpdf\u002Fnvidia-a100-datasheet.pdf","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":0,"end":8,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":111,"end":112,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":183,"end":185,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":188,"end":189,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_18":{"__typename":"Paragraph","id":"578c2017870b_18","name":"a33d","type":"P","href":null,"layout":null,"metadata":null,"text":"Answer Validation According to the white paper, training took 13.4 days. Our estimate is 5 times off, but we did get the order of magnitude right.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":35,"end":46,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F2109.04650","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":0,"end":18,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_19":{"__typename":"Paragraph","id":"578c2017870b_19","name":"1249","type":"P","href":null,"layout":null,"metadata":null,"text":"As I explain later in this post, the error is due to us naively plugging the theoretical peak throughput 𝜏 that is not achievable with distributed training and when models do anything but large matrix multiplications. If you correct 𝜏 accordingly (I will discuss this later), the FLOPs equation will get much more accurate. The other correction is that with checkpointing² that is a must for the largest models the required compute C goes up to ≈ 8ND.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":434,"end":436,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":447,"end":453,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_20":{"__typename":"Paragraph","id":"578c2017870b_20","name":"e68d","type":"P","href":null,"layout":null,"metadata":null,"text":"This equation is not something I came up with¹: you can find it in both the scaling laws and the GPT-3 papers by OpenAI. Yet I think it is not as widely known as it should be. In the rest of the article I will derive the equation and discuss what realistic throughput 𝜏 you can expect. I will assume you are familiar with Transformers (check out the paper or a blog post like e.g. this one if you are not).","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":76,"end":88,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F2001.08361","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":97,"end":102,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F2005.14165","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":347,"end":356,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1706.03762","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":382,"end":390,"href":"https:\u002F\u002Fjalammar.github.io\u002Fillustrated-transformer\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_21":{"__typename":"Paragraph","id":"578c2017870b_21","name":"d4a4","type":"H3","href":null,"layout":null,"metadata":null,"text":"Derivation of Transformer FLOPs Equation","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_22":{"__typename":"Paragraph","id":"578c2017870b_22","name":"4943","type":"P","href":null,"layout":null,"metadata":null,"text":"To derive the Transformer FLOPs equation we will have to make a key assumption.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_23":{"__typename":"Paragraph","id":"578c2017870b_23","name":"cd82","type":"H4","href":null,"layout":null,"metadata":null,"text":"The Weight FLOPs Assumption","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_24":{"__typename":"Paragraph","id":"578c2017870b_24","name":"a94f","type":"P","href":null,"layout":null,"metadata":null,"text":"The FLOPs that matter the most are weight FLOPs, that is ones performed when intermediate states are multiplied by weight matrices.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":0,"end":131,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_25":{"__typename":"Paragraph","id":"578c2017870b_25","name":"2592","type":"P","href":null,"layout":null,"metadata":null,"text":"The weight FLOPs are the majority of Transformer FLOPs, meaning that we can put aside FLOPs required for bias vector addition, layer normalization, residual connections, non-linearities, softmax and even attention. If you do not believe this, you are not wrong: while other FLOPs are less numerous, they also requires a lot of memory access and will in practice matter quite a bit. I will return to this later.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_26":{"__typename":"Paragraph","id":"578c2017870b_26","name":"055c","type":"P","href":null,"layout":null,"metadata":null,"text":"The beauty of matrix multiplications is that each of them adds a predictable and easy to compute number of FLOPs to the training total:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_27":{"__typename":"Paragraph","id":"578c2017870b_27","name":"76c1","type":"P","href":null,"layout":null,"metadata":null,"text":"weight FLOPs for multiplying by a matrix W = 6 times batch size times size of W","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":47,"end":52,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":64,"end":70,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":0,"end":46,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":53,"end":79,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_28":{"__typename":"Paragraph","id":"578c2017870b_28","name":"9b90","type":"P","href":null,"layout":null,"metadata":null,"text":"This Weight FLOPs Equation can take some time to wrap one’s head around. To understand where it comes from, consider a weight w that connects an input unit i to an output unit j:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":5,"end":27,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":126,"end":128,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":156,"end":157,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":176,"end":178,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*VC9y_dHhCKFPXj90Qshj3w.gif":{"__typename":"ImageMetadata","id":"1*VC9y_dHhCKFPXj90Qshj3w.gif","originalHeight":1080,"originalWidth":1920,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:578c2017870b_29":{"__typename":"Paragraph","id":"578c2017870b_29","name":"8e15","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*VC9y_dHhCKFPXj90Qshj3w.gif"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_30":{"__typename":"Paragraph","id":"578c2017870b_30","name":"7e9f","type":"P","href":null,"layout":null,"metadata":null,"text":"For each example in the batch, the weight w generates exactly 6 FLOPs combined in the forward and backward pass:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":42,"end":43,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_31":{"__typename":"Paragraph","id":"578c2017870b_31","name":"23d7","type":"OLI","href":null,"layout":null,"metadata":null,"text":"The unit i multiplies its output h(i) by w to send it to the unit j.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":9,"end":10,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":33,"end":37,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":41,"end":42,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":66,"end":67,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_32":{"__typename":"Paragraph","id":"578c2017870b_32","name":"d708","type":"OLI","href":null,"layout":null,"metadata":null,"text":"The unit j adds the unit i’s contribution to its total input a(j).","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":9,"end":10,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":25,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":61,"end":65,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_33":{"__typename":"Paragraph","id":"578c2017870b_33","name":"c0c6","type":"OLI","href":null,"layout":null,"metadata":null,"text":"The unit j multiplies the incoming loss gradient dL\u002Fda(j) by w to send it back to the unit i.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":9,"end":10,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":49,"end":57,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":61,"end":62,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_34":{"__typename":"Paragraph","id":"578c2017870b_34","name":"19c4","type":"OLI","href":null,"layout":null,"metadata":null,"text":"The unit i adds the unit j’s contribution to its total loss gradient dL\u002Fdh(i).","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":9,"end":10,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":25,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":69,"end":77,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_35":{"__typename":"Paragraph","id":"578c2017870b_35","name":"cdaa","type":"OLI","href":null,"layout":null,"metadata":null,"text":"The unit j multiplies its loss gradient dL\u002Fda(j) by the unit i’s output h(i) to compute the loss gradient dL\u002Fdw for the given example.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":112,"end":133,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":40,"end":49,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":72,"end":76,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":106,"end":111,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_36":{"__typename":"Paragraph","id":"578c2017870b_36","name":"7aba","type":"OLI","href":null,"layout":null,"metadata":null,"text":"(The sneakiest FLOP, IMHO) The weight w adds the contribution from step 5 to its loss gradient accumulator dL\u002Fdw that aggregates gradients for all examples.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":139,"end":155,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":38,"end":39,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":107,"end":113,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_37":{"__typename":"Paragraph","id":"578c2017870b_37","name":"40be","type":"P","href":null,"layout":null,"metadata":null,"text":"The Weight FLOPs Equation directly follows from the fact that we need 6 FLOPs per example per weight. And from this equation follows the Transformer FLOPs Equation. To understand this, think about how many weight matrix multiplication Transformer performs for each input token, no matter how many input sequences the batch consists of. The answer is exactly 1 for each weight matrix! So the total number of FLOPs for each token is 6 times the model size N, Q.E.D.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":350,"end":382,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":454,"end":455,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_38":{"__typename":"Paragraph","id":"578c2017870b_38","name":"c44d","type":"H4","href":null,"layout":null,"metadata":null,"text":"Why only weight FLOPs matter","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_39":{"__typename":"Paragraph","id":"578c2017870b_39","name":"b763","type":"P","href":null,"layout":null,"metadata":null,"text":"I have asked you to swallow the assumption that only FLOPs from mulplications by weight matrices, and you might be wondering now if the assumption is too strong. It is a fair question to ask; it is commonly thought that attention is the bottleneck in Transformers, and here I am boldly brushing it aside. The reason we can do this is because attention only adds O(dL) FLOPs³ per token per layer, whereas matrix multiplications add O(d²) (for more precise counting check out the Megatron paper by Nvidia). And when we talk about really large models, d tends to be considerable larger than L, and hence weight FLOPs dwarf attention FLOPs!","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":474,"end":502,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1909.08053","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":362,"end":368,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":431,"end":436,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":528,"end":534,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":549,"end":550,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":588,"end":589,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_40":{"__typename":"Paragraph","id":"578c2017870b_40","name":"d877","type":"P","href":null,"layout":null,"metadata":null,"text":"The other FLOPs (softmax, layer norm, activations and etc), should be even more negligible, but there is a catch — the GPU memory bandwidth becomes the bottleneck when these operations are performed. In practice these elementwise operations can take non-negligible time. I find it thus helpful to think about the weight FLOPs (WFLOPs) throughput that a particular implementation can deliver on a particular hardware.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":313,"end":345,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_41":{"__typename":"Paragraph","id":"578c2017870b_41","name":"fffd","type":"H3","href":null,"layout":null,"metadata":null,"text":"Estimating the WFLOPs Throughput","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_42":{"__typename":"Paragraph","id":"578c2017870b_42","name":"0381","type":"P","href":null,"layout":null,"metadata":null,"text":"The theory of counting Transformer FLOPs is elegant, but as seen in the HyperCLOVA example, naive application results in significant underestimation of time required to training the language model. For more precise reasoning, we need a better of idea of what actual WFLOPs throughput can be like.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_43":{"__typename":"Paragraph","id":"578c2017870b_43","name":"dd65","type":"P","href":null,"layout":null,"metadata":null,"text":"I have done a little case study on a A100 GPU. According to Nvidia documentation, it can deliver up to 312 bfloat16 teraFLOPs — that’s 3.12e14 operations per second! Nvidia docs also show how ~250 teraFLOPs can be actually achieved by doing 4096 x 8192 x 4096 matrix multiplications, and I was able to reproduce that. But what practical throughput can we get when training neural networks?","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_44":{"__typename":"Paragraph","id":"578c2017870b_44","name":"fff6","type":"P","href":null,"layout":null,"metadata":null,"text":"I have experimented with the GPT-2 implementation from Huggingface Transformers and with a bespoke MLP implementation (note that WFLOPs calculus for an MLP is the same as for Transformer). Both models have states of d=1600 dimensions for each input and d_ff=6400 intermediate units. I train MLP on batches of 8192 input vectors; GPT-2 receives 8192 input tokens as B=32 contexts of length L=256 to get an upper bound on the throughput that is less affected by attention (which should become cheaper for larger models). To fit in the single GPU memory, I use only 8 GPT-2 layers.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":29,"end":79,"href":"https:\u002F\u002Fhuggingface.co\u002Fgpt2","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":91,"end":117,"href":"https:\u002F\u002Fgist.github.com\u002Frizar\u002Fbe08b9b5b2a8aed1a24cc316ba6a6a96","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":216,"end":218,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":253,"end":257,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":365,"end":367,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":389,"end":391,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_45":{"__typename":"Paragraph","id":"578c2017870b_45","name":"86a4","type":"P","href":null,"layout":null,"metadata":null,"text":"Here are the throughputs:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:9edf6d5cf4f8d7152f981a1abf2931fd":{"__typename":"MediaResource","id":"9edf6d5cf4f8d7152f981a1abf2931fd","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"Throughput table"},"Paragraph:578c2017870b_46":{"__typename":"Paragraph","id":"578c2017870b_46","name":"8942","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:9edf6d5cf4f8d7152f981a1abf2931fd"}},"mixtapeMetadata":null},"Paragraph:578c2017870b_47":{"__typename":"Paragraph","id":"578c2017870b_47","name":"383d","type":"P","href":null,"layout":null,"metadata":null,"text":"A training step (including backprop) for a linear float16 MLP with no activations yields a throughput of 230 teraWFLOP\u002Fs, very close to 237 teraWFLOP\u002Fs that I registered for pure matrix multiplications. With float32 weights and mixed precision training (which AFAIK is the standard in training big Transformers these days) the throughput drops to 207.6 teraWFLOPs due to the float32 -\u003E bfloat16 conversion that mixed precision training involves. Adding a ReLU activation and a residual connection causes a further throughput drops to 185.7 teraWFLOPs. This might be surpising but is very much in line with Nvidia performance documentation, which explains how throughput for activations and elementwise operations is bounded by the memory bandwidth.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":228,"end":252,"href":"https:\u002F\u002Fdocs.nvidia.com\u002Fdeeplearning\u002Fperformance\u002Fmixed-precision-training\u002Findex.html","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":606,"end":638,"href":"https:\u002F\u002Fdocs.nvidia.com\u002Fdeeplearning\u002Fperformance\u002Fdl-performance-gpu-background\u002Findex.html","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_48":{"__typename":"Paragraph","id":"578c2017870b_48","name":"a728","type":"P","href":null,"layout":null,"metadata":null,"text":"The MLP throughput looks encouraging, but for the actual GPT-2 implementation from HuggingFace Transformers the throughput was merely 68 teraWFLOP\u002Fs. I have not looked deeper into the exact breakdown, but a likely explanation is that the memory-intensive computations, such as residual connections, activations, layer normalization, attention masking and attention softmax do cost a lot when combined together.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":42,"end":149,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_49":{"__typename":"Paragraph","id":"578c2017870b_49","name":"a8f6","type":"H4","href":null,"layout":null,"metadata":null,"text":"WFLOPs throughput in the literature","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_50":{"__typename":"Paragraph","id":"578c2017870b_50","name":"0e29","type":"P","href":null,"layout":null,"metadata":null,"text":"Throughput estimates can also be obtained by looking at white papers. For example, from here, here and here we can estimate the throughput achieved with various forks of Megatron-LM by Nvidia²:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":88,"end":92,"href":"https:\u002F\u002Fwww.microsoft.com\u002Fen-us\u002Fresearch\u002Fblog\u002Fusing-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":94,"end":98,"href":"http:\u002F\u002Farxiv.org\u002Fabs\u002F2109.04650","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":103,"end":107,"href":"https:\u002F\u002Fgithub.com\u002FNVIDIA\u002FMegatron-LM#gpt-3-example","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:65195fb6d3e1663fa79a9c03a6d892f0":{"__typename":"MediaResource","id":"65195fb6d3e1663fa79a9c03a6d892f0","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"MegaTron throughput"},"Paragraph:578c2017870b_51":{"__typename":"Paragraph","id":"578c2017870b_51","name":"1087","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:65195fb6d3e1663fa79a9c03a6d892f0"}},"mixtapeMetadata":null},"Paragraph:578c2017870b_52":{"__typename":"Paragraph","id":"578c2017870b_52","name":"093f","type":"P","href":null,"layout":null,"metadata":null,"text":"Note that these numbers are for highly distributed setups, single GPU throughput for Megatron is likely to be much higher, thanks to the extensive used of fused operations.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":154,"end":171,"href":"https:\u002F\u002Fgithub.com\u002FNVIDIA\u002FMegatron-LM\u002Fblob\u002Fmain\u002Fmegatron\u002Fmodel\u002Ffused_bias_gelu.py","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_53":{"__typename":"Paragraph","id":"578c2017870b_53","name":"d816","type":"H3","href":null,"layout":null,"metadata":null,"text":"To Sum It Up","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_54":{"__typename":"Paragraph","id":"578c2017870b_54","name":"dfd9","type":"P","href":null,"layout":null,"metadata":null,"text":"In this article I have shared with you the Transformer FLOPs equation that makes reasoning about extremely large language models easy. The equation ties together the throughput 𝜏, the training time T, the model size N and the number of training tokens T:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":199,"end":200,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":217,"end":218,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":253,"end":255,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_55":{"__typename":"Paragraph","id":"578c2017870b_55","name":"06ff","type":"P","href":null,"layout":null,"metadata":null,"text":"𝜏T = 6ND.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":2,"end":10,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_56":{"__typename":"Paragraph","id":"578c2017870b_56","name":"1746","type":"P","href":null,"layout":null,"metadata":null,"text":"Looking at publicly available white papers, the throughput 𝜏 is likely to be anywhere between 50 and 150 teraWFLOP\u002Fs per A100 GPU.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_57":{"__typename":"Paragraph","id":"578c2017870b_57","name":"2ff9","type":"P","href":null,"layout":null,"metadata":null,"text":"My favorite corollary of this equation is that assuming constant throughput, the training time grows linearly with the model size. So if you want to increase the model size by 2, you have to either use 2 times as many GPUs, or wait 2 times longer. Easy mathematics that you can do in your head!","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_58":{"__typename":"Paragraph","id":"578c2017870b_58","name":"c0c4","type":"H3","href":null,"layout":null,"metadata":null,"text":"Beyond Transformers","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_59":{"__typename":"Paragraph","id":"578c2017870b_59","name":"ddaa","type":"P","href":null,"layout":null,"metadata":null,"text":"As the art of compressing the internet in matrices (a.k.a. language modeling) develops further, the FLOPs calculus might get hairier. For example, the math is different for Mixture of Experts (MoE) models trained by researchers at Google and Meta. In these models, only a fraction of model’s weights is active for every input token. The equation can thus be fixed by replacing the total model size N with the number of active weights.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":231,"end":237,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2112.06905.pdf","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":242,"end":246,"href":"http:\u002F\u002Farxiv.org\u002Fabs\u002F2112.10684","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":398,"end":399,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_60":{"__typename":"Paragraph","id":"578c2017870b_60","name":"a703","type":"P","href":null,"layout":null,"metadata":null,"text":"The FLOPs calculus for LSTMs would look very similar to that of Transformer, which is a key factor explaining their demise. The total number of FLOPs grows linearly with the model size. But to train LSTMs on long sequences while fitting in GPU memory one has to reduce the batch size. And with a small batch size the GPU throughput for sequential LSTM computations falls dramatically. For example, for 32x1600x6400 matrix multiplication the throughput is below 20 teraFLOP\u002Fs, more than 10 times slower than for 8192x1600x6400! Recurrence comes at a price: the computation for further tokens must wait before computations for previous tokens are done, making computations less parallel and thus less GPU-friendly.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_61":{"__typename":"Paragraph","id":"578c2017870b_61","name":"9050","type":"H3","href":null,"layout":null,"metadata":null,"text":"The End","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_62":{"__typename":"Paragraph","id":"578c2017870b_62","name":"4e85","type":"P","href":null,"layout":null,"metadata":null,"text":"I hope you found this article useful! Many thanks to Harm de Vries, Amine el Hattami, Torsten Scholak, Nicolas Chapados, Sebastien Paquet, and my other fabulous colleagues at ServiceNow Research for discussions that greatly helped me in researching this topic.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_63":{"__typename":"Paragraph","id":"578c2017870b_63","name":"a6e5","type":"H3","href":null,"layout":null,"metadata":null,"text":"Footnotes","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_64":{"__typename":"Paragraph","id":"578c2017870b_64","name":"4f80","type":"P","href":null,"layout":null,"metadata":null,"text":"[1] Disclaimer: this is meant to be a popular article, not an academic contribution. While I’m trying to give credit where the credit is due, please don’t freak out if you find this not sufficiently rigorous and just get in touch — I will to try to address the issue.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:578c2017870b_65":{"__typename":"Paragraph","id":"578c2017870b_65","name":"4d32","type":"P","href":null,"layout":null,"metadata":null,"text":"[2] Note that Nvidia reports different teraFLOP\u002Fs numbers, namely 138 teraFLOP\u002Fs vs 100.8 teraWFLOPs that I calculated. The major source of the difference is that they include the FLOPs needed for the extra forward pass that recomputes the activations. Activation recomputation (checkpointing) allows back-propagation without storing all intermediate states in memory. It is now routinely used to train the largest models. The extra forward pass requires 2ND FLOPs. The rest of the difference comes from them including attention and output layer FLOPs. In this article I view that activation recomputation FLOPs do not directly contribute to learning and thus reduce the system’s effective WFLOPs throughput. But if you are an HPC person and what you want to showcase is optimal device utilization, it makes sense to include these FLOPs in the total.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":455,"end":459,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"PostViewerEdge:postId:3b19c1f025e4-viewerId:lo_7fabea0c44e5":{"__typename":"PostViewerEdge","shouldIndexPostForExternalSearch":true,"id":"postId:3b19c1f025e4-viewerId:lo_7fabea0c44e5"},"Tag:language-model":{"__typename":"Tag","id":"language-model","displayTitle":"Language Model","normalizedTagSlug":"language-model"},"Tag:flip-flops":{"__typename":"Tag","id":"flip-flops","displayTitle":"Flip Flops","normalizedTagSlug":"flip-flops"},"Tag:deep-learning":{"__typename":"Tag","id":"deep-learning","displayTitle":"Deep Learning","normalizedTagSlug":"deep-learning"},"Tag:artificial-intelligence":{"__typename":"Tag","id":"artificial-intelligence","displayTitle":"Artificial Intelligence","normalizedTagSlug":"artificial-intelligence"},"Tag:neural-networks":{"__typename":"Tag","id":"neural-networks","displayTitle":"Neural Networks","normalizedTagSlug":"neural-networks"},"Post:3b19c1f025e4":{"__typename":"Post","id":"3b19c1f025e4","collection":null,"content({\"postMeteringOptions\":{}})":{"__typename":"PostContent","isLockedPreviewOnly":false,"bodyModel":{"__typename":"RichText","sections":[{"__typename":"Section","name":"571c","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"3f9d","startIndex":21,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"adbc","startIndex":53,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"81f0","startIndex":63,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null}],"paragraphs":[{"__ref":"Paragraph:578c2017870b_0"},{"__ref":"Paragraph:578c2017870b_1"},{"__ref":"Paragraph:578c2017870b_2"},{"__ref":"Paragraph:578c2017870b_3"},{"__ref":"Paragraph:578c2017870b_4"},{"__ref":"Paragraph:578c2017870b_5"},{"__ref":"Paragraph:578c2017870b_6"},{"__ref":"Paragraph:578c2017870b_7"},{"__ref":"Paragraph:578c2017870b_8"},{"__ref":"Paragraph:578c2017870b_9"},{"__ref":"Paragraph:578c2017870b_10"},{"__ref":"Paragraph:578c2017870b_11"},{"__ref":"Paragraph:578c2017870b_12"},{"__ref":"Paragraph:578c2017870b_13"},{"__ref":"Paragraph:578c2017870b_14"},{"__ref":"Paragraph:578c2017870b_15"},{"__ref":"Paragraph:578c2017870b_16"},{"__ref":"Paragraph:578c2017870b_17"},{"__ref":"Paragraph:578c2017870b_18"},{"__ref":"Paragraph:578c2017870b_19"},{"__ref":"Paragraph:578c2017870b_20"},{"__ref":"Paragraph:578c2017870b_21"},{"__ref":"Paragraph:578c2017870b_22"},{"__ref":"Paragraph:578c2017870b_23"},{"__ref":"Paragraph:578c2017870b_24"},{"__ref":"Paragraph:578c2017870b_25"},{"__ref":"Paragraph:578c2017870b_26"},{"__ref":"Paragraph:578c2017870b_27"},{"__ref":"Paragraph:578c2017870b_28"},{"__ref":"Paragraph:578c2017870b_29"},{"__ref":"Paragraph:578c2017870b_30"},{"__ref":"Paragraph:578c2017870b_31"},{"__ref":"Paragraph:578c2017870b_32"},{"__ref":"Paragraph:578c2017870b_33"},{"__ref":"Paragraph:578c2017870b_34"},{"__ref":"Paragraph:578c2017870b_35"},{"__ref":"Paragraph:578c2017870b_36"},{"__ref":"Paragraph:578c2017870b_37"},{"__ref":"Paragraph:578c2017870b_38"},{"__ref":"Paragraph:578c2017870b_39"},{"__ref":"Paragraph:578c2017870b_40"},{"__ref":"Paragraph:578c2017870b_41"},{"__ref":"Paragraph:578c2017870b_42"},{"__ref":"Paragraph:578c2017870b_43"},{"__ref":"Paragraph:578c2017870b_44"},{"__ref":"Paragraph:578c2017870b_45"},{"__ref":"Paragraph:578c2017870b_46"},{"__ref":"Paragraph:578c2017870b_47"},{"__ref":"Paragraph:578c2017870b_48"},{"__ref":"Paragraph:578c2017870b_49"},{"__ref":"Paragraph:578c2017870b_50"},{"__ref":"Paragraph:578c2017870b_51"},{"__ref":"Paragraph:578c2017870b_52"},{"__ref":"Paragraph:578c2017870b_53"},{"__ref":"Paragraph:578c2017870b_54"},{"__ref":"Paragraph:578c2017870b_55"},{"__ref":"Paragraph:578c2017870b_56"},{"__ref":"Paragraph:578c2017870b_57"},{"__ref":"Paragraph:578c2017870b_58"},{"__ref":"Paragraph:578c2017870b_59"},{"__ref":"Paragraph:578c2017870b_60"},{"__ref":"Paragraph:578c2017870b_61"},{"__ref":"Paragraph:578c2017870b_62"},{"__ref":"Paragraph:578c2017870b_63"},{"__ref":"Paragraph:578c2017870b_64"},{"__ref":"Paragraph:578c2017870b_65"}]},"validatedShareKey":"","shareKeyCreator":null},"creator":{"__ref":"User:e663600ab469"},"inResponseToEntityType":null,"isLocked":false,"isMarkedPaywallOnly":false,"lockedSource":"LOCKED_POST_SOURCE_NONE","mediumUrl":"https:\u002F\u002Fmedium.com\u002F@dzmitrybahdanau\u002Fthe-flops-calculus-of-language-model-training-3b19c1f025e4","primaryTopic":{"__ref":"Topic:1eca0103fff3"},"topics":[{"__typename":"Topic","slug":"machine-learning"}],"isPublished":true,"latestPublishedVersion":"578c2017870b","visibility":"PUBLIC","postResponses":{"__typename":"PostResponses","count":3},"clapCount":258,"allowResponses":true,"isLimitedState":false,"title":"The FLOPs Calculus of Language Model Training","isSeries":false,"sequence":null,"uniqueSlug":"the-flops-calculus-of-language-model-training-3b19c1f025e4","socialTitle":"","socialDek":"","canonicalUrl":"","metaDescription":"","latestPublishedAt":1671487076039,"readingTime":8.058805031446541,"previewContent":{"__typename":"PreviewContent","subtitle":"Extremely large language models like the famous GPT-3 by OpenAI are all the rage. Many of us are now trying to get a sense of scale of the…"},"previewImage":{"__ref":"ImageMetadata:0*sG59WF5K7qLZr307.jpg"},"isShortform":false,"seoTitle":"","firstPublishedAt":1641756009419,"updatedAt":1671487085469,"shortformType":"SHORTFORM_TYPE_LINK","seoDescription":"","viewerEdge":{"__ref":"PostViewerEdge:postId:3b19c1f025e4-viewerId:lo_7fabea0c44e5"},"isSuspended":false,"license":"ALL_RIGHTS_RESERVED","tags":[{"__ref":"Tag:language-model"},{"__ref":"Tag:flip-flops"},{"__ref":"Tag:deep-learning"},{"__ref":"Tag:artificial-intelligence"},{"__ref":"Tag:neural-networks"}],"isNewsletter":false,"statusForCollection":null,"pendingCollection":null,"detectedLanguage":"en","wordCount":2034,"layerCake":3,"responsesLocked":false}}</script><script>window.__MIDDLEWARE_STATE__={"session":{"xsrf":""},"cache":{"cacheStatus":"HIT"}}</script><script src="https://cdn-client.medium.com/lite/static/js/manifest.90218340.js"></script><script src="https://cdn-client.medium.com/lite/static/js/9865.1496d74a.js"></script><script src="https://cdn-client.medium.com/lite/static/js/main.3cf6f820.js"></script><script src="https://cdn-client.medium.com/lite/static/js/instrumentation.d9108df7.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/reporting.ff22a7a5.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5049.d1ead72d.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4810.6318add7.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6618.db187378.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2707.b0942613.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9977.5b3eb23a.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8599.1ab63137.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5250.9f9e01d2.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5787.e66a3a4d.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2648.26563adf.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8393.826a25fb.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/7549.2176f21f.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6589.7c500280.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/3735.afb7e926.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5642.0a97706a.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6546.cd03f950.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6834.08de95de.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/7346.72622eb9.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2420.2a5e2d95.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/839.ca7937c2.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/7975.d195c6f1.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/7394.bf599bc5.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2961.00a48598.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8204.c4082863.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4391.59acaed3.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/PostPage.MainContent.902ad94b.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8414.6565ad5f.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/3974.8d3e0217.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2527.a0afad8a.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/PostResponsesContent.36c2ecf4.chunk.js"></script><script>window.main();</script></body></html>