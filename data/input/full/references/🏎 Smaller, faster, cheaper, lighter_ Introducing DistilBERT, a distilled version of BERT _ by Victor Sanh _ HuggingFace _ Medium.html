<!doctype html><html lang="en"><head><title data-rh="true">üèé Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT | by Victor Sanh | HuggingFace | Medium</title><meta data-rh="true" charset="utf-8"/><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1,maximum-scale=1"/><meta data-rh="true" name="theme-color" content="#000000"/><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"/><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"/><meta data-rh="true" property="al:ios:app_name" content="Medium"/><meta data-rh="true" property="al:ios:app_store_id" content="828256236"/><meta data-rh="true" property="al:android:package" content="com.medium.reader"/><meta data-rh="true" property="fb:app_id" content="542599432471018"/><meta data-rh="true" property="og:site_name" content="Medium"/><meta data-rh="true" property="og:type" content="article"/><meta data-rh="true" property="article:published_time" content="2020-08-31T15:02:49.872Z"/><meta data-rh="true" name="title" content="üèé Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT | by Victor Sanh | HuggingFace | Medium"/><meta data-rh="true" property="og:title" content="üèé Smaller, faster, cheaper, lighter: Introducing DilBERT, a distilled version of BERT"/><meta data-rh="true" property="al:android:url" content="medium://p/8cf3380435b5"/><meta data-rh="true" property="al:ios:url" content="medium://p/8cf3380435b5"/><meta data-rh="true" property="al:android:app_name" content="Medium"/><meta data-rh="true" name="description" content="HuggingFace introduces DilBERT, a distilled and smaller version of Google AI‚Äôs Bert model with strong performances on language understanding. DilBert s included in the pytorch-transformers library."/><meta data-rh="true" property="og:description" content="You can find the code to reproduce the training of DilBERT along with pre-trained weights for DilBERT here."/><meta data-rh="true" property="og:url" content="https://medium.com/huggingface/distilbert-8cf3380435b5"/><meta data-rh="true" property="al:web:url" content="https://medium.com/huggingface/distilbert-8cf3380435b5"/><meta data-rh="true" property="og:image" content="https://miro.medium.com/v2/resize:fit:1200/1*IFVX74cEe8U5D1GveL1uZA.png"/><meta data-rh="true" property="article:author" content="https://medium.com/@victorsanh"/><meta data-rh="true" name="author" content="Victor Sanh"/><meta data-rh="true" name="robots" content="index,noarchive,follow,max-image-preview:large"/><meta data-rh="true" name="referrer" content="unsafe-url"/><meta data-rh="true" property="twitter:title" content="üèé Smaller, faster, cheaper, lighter: Introducing DilBERT, a distilled version of BERT"/><meta data-rh="true" name="twitter:site" content="@huggingface"/><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/8cf3380435b5"/><meta data-rh="true" property="twitter:description" content="You can find the code to reproduce the training of DilBERT along with pre-trained weights for DilBERT here."/><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/v2/resize:fit:1200/1*IFVX74cEe8U5D1GveL1uZA.png"/><meta data-rh="true" name="twitter:card" content="summary_large_image"/><meta data-rh="true" name="twitter:creator" content="@SanhEstPasMoi"/><meta data-rh="true" name="twitter:label1" content="Reading time"/><meta data-rh="true" name="twitter:data1" content="10 min read"/><link data-rh="true" rel="icon" href="https://miro.medium.com/v2/5d8de952517e8160e40ef9841c781cdc14a5db313057fa3c3de41c6f5b494b19"/><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="/osd.xml"/><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://miro.medium.com/v2/resize:fill:304:304/10fd5c419ac61637245384e7099e131627900034828f4f386bdaa47a74eae156"/><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://miro.medium.com/v2/resize:fill:240:240/10fd5c419ac61637245384e7099e131627900034828f4f386bdaa47a74eae156"/><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://miro.medium.com/v2/resize:fill:152:152/10fd5c419ac61637245384e7099e131627900034828f4f386bdaa47a74eae156"/><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://miro.medium.com/v2/resize:fill:120:120/10fd5c419ac61637245384e7099e131627900034828f4f386bdaa47a74eae156"/><link data-rh="true" rel="mask-icon" href="https://miro.medium.com/v2/resize:fill:1000:1000/7*GAOKVe--MXbEJmV9230oOQ.png" color="#171717"/><link data-rh="true" rel="preconnect" href="https://glyph.medium.com" crossOrigin=""/><link data-rh="true" id="glyph_preload_link" rel="preload" as="style" type="text/css" href="https://glyph.medium.com/css/unbound.css"/><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="https://glyph.medium.com/css/unbound.css"/><link data-rh="true" rel="author" href="https://medium.com/@victorsanh"/><link data-rh="true" rel="canonical" href="https://medium.com/huggingface/distilbert-8cf3380435b5"/><link data-rh="true" rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/8cf3380435b5"/><script data-rh="true" type="application/ld+json">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":["https:\u002F\u002Fmiro.medium.com\u002Fv2\u002Fresize:fit:1200\u002F1*IFVX74cEe8U5D1GveL1uZA.png"],"url":"https:\u002F\u002Fmedium.com\u002Fhuggingface\u002Fdistilbert-8cf3380435b5","dateCreated":"2019-08-28T14:43:24.584Z","datePublished":"2019-08-28T14:43:24.584Z","dateModified":"2021-12-11T12:27:09.113Z","headline":"üèé Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT","name":"üèé Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT","description":"HuggingFace introduces DilBERT, a distilled and smaller version of Google AI‚Äôs Bert model with strong performances on language understanding. DilBert s included in the pytorch-transformers library.","identifier":"8cf3380435b5","author":{"@type":"Person","name":"Victor Sanh","url":"https:\u002F\u002Fmedium.com\u002F@victorsanh"},"creator":["Victor Sanh"],"publisher":{"@type":"Organization","name":"HuggingFace","url":"https:\u002F\u002Fmedium.com\u002Fhuggingface","logo":{"@type":"ImageObject","width":165,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fv2\u002Fresize:fit:330\u002F1*NNALWVaZvBmLBZXPqRmXBQ.png"}},"mainEntityOfPage":"https:\u002F\u002Fmedium.com\u002Fhuggingface\u002Fdistilbert-8cf3380435b5"}</script><style type="text/css" data-fela-rehydration="609" data-fela-type="STATIC">html{box-sizing:border-box;-webkit-text-size-adjust:100%}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}:root{--reach-tabs:1;--reach-menu-button:1}#speechify-root{font-family:Sohne, sans-serif}div[data-popper-reference-hidden="true"]{visibility:hidden;pointer-events:none}.grecaptcha-badge{visibility:hidden}
/*XCode style (c) Angel Garcia <angelgarcia.mail@gmail.com>*/.hljs {background: #fff;color: black;
}/* Gray DOCTYPE selectors like WebKit */
.xml .hljs-meta {color: #c0c0c0;
}.hljs-comment,
.hljs-quote {color: #007400;
}.hljs-tag,
.hljs-attribute,
.hljs-keyword,
.hljs-selector-tag,
.hljs-literal,
.hljs-name {color: #aa0d91;
}.hljs-variable,
.hljs-template-variable {color: #3F6E74;
}.hljs-code,
.hljs-string,
.hljs-meta .hljs-string {color: #c41a16;
}.hljs-regexp,
.hljs-link {color: #0E0EFF;
}.hljs-title,
.hljs-symbol,
.hljs-bullet,
.hljs-number {color: #1c00cf;
}.hljs-section,
.hljs-meta {color: #643820;
}.hljs-title.class_,
.hljs-class .hljs-title,
.hljs-type,
.hljs-built_in,
.hljs-params {color: #5c2699;
}.hljs-attr {color: #836C28;
}.hljs-subst {color: #000;
}.hljs-formula {background-color: #eee;font-style: italic;
}.hljs-addition {background-color: #baeeba;
}.hljs-deletion {background-color: #ffc8bd;
}.hljs-selector-id,
.hljs-selector-class {color: #9b703f;
}.hljs-doctag,
.hljs-strong {font-weight: bold;
}.hljs-emphasis {font-style: italic;
}
</style><style type="text/css" data-fela-rehydration="609" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@-moz-keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}</style><style type="text/css" data-fela-rehydration="609" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{display:block}.m{position:sticky}.n{top:0}.o{z-index:500}.p{padding:0 24px}.q{align-items:center}.r{border-bottom:solid 1px #F2F2F2}.y{height:41px}.z{line-height:20px}.ab{display:flex}.ac{height:57px}.ae{flex:1 0 auto}.af{color:inherit}.ag{fill:inherit}.ah{font-size:inherit}.ai{border:inherit}.aj{font-family:inherit}.ak{letter-spacing:inherit}.al{font-weight:inherit}.am{padding:0}.an{margin:0}.ao{cursor:pointer}.ap:disabled{cursor:not-allowed}.aq:disabled{color:#6B6B6B}.ar:disabled{fill:#6B6B6B}.au{width:auto}.av path{fill:#242424}.aw{height:25px}.ax{margin-left:16px}.ay{border:none}.az{border-radius:20px}.ba{width:240px}.bb{background:#F9F9F9}.bc path{fill:#6B6B6B}.be{outline:none}.bf{font-family:sohne, "Helvetica Neue", Helvetica, Arial, sans-serif}.bg{font-size:14px}.bh{width:100%}.bi{padding:10px 20px 10px 0}.bj{background-color:transparent}.bk{color:#242424}.bl::placeholder{color:#6B6B6B}.bm{display:inline-block}.bn{margin-left:12px}.bo{margin-right:12px}.bp{border-radius:4px}.bq{margin-left:24px}.br{height:24px}.bx{background-color:#F9F9F9}.by{border-radius:50%}.bz{height:32px}.ca{width:32px}.cb{justify-content:center}.ch{max-width:680px}.ci{min-width:0}.cj{animation:k1 1.2s ease-in-out infinite}.ck{height:100vh}.cl{margin-bottom:16px}.cm{margin-top:48px}.cn{align-items:flex-start}.co{flex-direction:column}.cp{justify-content:space-between}.cq{margin-bottom:24px}.cw{width:80%}.cx{background-color:#F2F2F2}.dd{height:44px}.de{width:44px}.df{margin:auto 0}.dg{margin-bottom:4px}.dh{height:16px}.di{width:120px}.dj{width:80px}.dp{margin-bottom:8px}.dq{width:96%}.dr{width:98%}.ds{width:81%}.dt{margin-left:8px}.du{color:#6B6B6B}.dv{font-size:13px}.dw{height:100%}.ep{color:#FFFFFF}.eq{fill:#FFFFFF}.er{background:rgba(108, 120, 255, 1)}.es{border-color:rgba(108, 120, 255, 1)}.ew:disabled{cursor:inherit !important}.ex:disabled{opacity:0.3}.ey:disabled:hover{background:rgba(108, 120, 255, 1)}.ez:disabled:hover{border-color:rgba(108, 120, 255, 1)}.fa{border-radius:99em}.fb{border-width:1px}.fc{border-style:solid}.fd{box-sizing:border-box}.fe{text-decoration:none}.ff{text-align:center}.fi{margin-right:32px}.fj{position:relative}.fk{fill:#6B6B6B}.fn{background:transparent}.fo svg{margin-left:4px}.fp svg{fill:#6B6B6B}.fr{box-shadow:inset 0 0 0 1px rgba(0, 0, 0, 0.05)}.fs{position:absolute}.fz{margin:0 24px}.gd{background:rgba(255, 255, 255, 1)}.ge{border:1px solid #F2F2F2}.gf{box-shadow:0 1px 4px #F2F2F2}.gg{max-height:100vh}.gh{overflow-y:auto}.gi{left:0}.gj{top:calc(100vh + 100px)}.gk{bottom:calc(100vh + 100px)}.gl{width:10px}.gm{pointer-events:none}.gn{word-break:break-word}.go{word-wrap:break-word}.gp:after{display:block}.gq:after{content:""}.gr:after{clear:both}.gs{clear:both}.hb{margin-left:auto}.hc{margin-right:auto}.hd{max-width:4096px}.hj{padding-top:5px}.hk{padding-bottom:5px}.hm{cursor:zoom-in}.hn{z-index:auto}.hp{max-width:100%}.hq{height:auto}.hr{margin-top:10px}.hs{max-width:728px}.hv{text-decoration:underline}.hw{line-height:1.23}.hx{letter-spacing:0}.hy{font-style:normal}.hz{font-weight:700}.je{align-items:baseline}.jf{width:48px}.jg{height:48px}.jh{border:2px solid rgba(255, 255, 255, 1)}.ji{z-index:0}.jj{box-shadow:none}.jk{border:1px solid rgba(0, 0, 0, 0.05)}.jl{margin-left:-12px}.jm{width:28px}.jn{height:28px}.jo{z-index:1}.jp{width:24px}.jq{margin-bottom:2px}.jr{flex-wrap:nowrap}.js{font-size:16px}.jt{line-height:24px}.jv{margin:0 8px}.jw{display:inline}.jx{color:rgba(108, 120, 255, 1)}.jy{fill:rgba(108, 120, 255, 1)}.kb{flex:0 0 auto}.ke{flex-wrap:wrap}.kh{white-space:pre-wrap}.ki{margin-right:4px}.kj{overflow:hidden}.kk{max-height:20px}.kl{text-overflow:ellipsis}.km{display:-webkit-box}.kn{-webkit-line-clamp:1}.ko{-webkit-box-orient:vertical}.kp{word-break:break-all}.kr{padding-left:8px}.ks{padding-right:8px}.lt> *{flex-shrink:0}.lu{overflow-x:scroll}.lv::-webkit-scrollbar{display:none}.lw{scrollbar-width:none}.lx{-ms-overflow-style:none}.ly{width:74px}.lz{flex-direction:row}.ma{z-index:2}.md{-webkit-user-select:none}.me{border:0}.mf{fill:rgba(117, 117, 117, 1)}.mi{outline:0}.mj{user-select:none}.mk> svg{pointer-events:none}.mt{cursor:progress}.mu{margin-left:4px}.mv{margin-top:0px}.mw{opacity:1}.mx{padding:4px 0}.na{width:16px}.nc{display:inline-flex}.nf{padding:8px 2px}.ng svg{color:#6B6B6B}.nx{box-shadow:inset 3px 0 0 0 #242424}.ny{padding-left:23px}.nz{margin-left:-20px}.oa{line-height:1.58}.ob{letter-spacing:-0.004em}.oc{font-style:italic}.od{font-family:source-serif-pro, Georgia, Cambria, "Times New Roman", Times, serif}.oy{margin-bottom:-0.46em}.oz{max-width:2070px}.pf{font-weight:600}.pg{padding-left:30px}.ph{line-height:40px}.pi{letter-spacing:-0.009em}.pj{font-weight:300}.pk{font-size:28px}.pv{font-style:inherit}.pw{line-height:1.12}.px{letter-spacing:-0.022em}.qs{margin-bottom:-0.28em}.qy{max-width:1934px}.qz{max-width:249px}.ra{max-width:233px}.rb{max-width:692px}.rc{max-width:1332px}.rd{padding:2px 4px}.re{font-size:75%}.rf> strong{font-family:inherit}.rg{font-family:source-code-pro, Menlo, Monaco, "Courier New", Courier, monospace}.rh{margin-top:32px}.ri{margin-bottom:14px}.rj{padding-top:24px}.rk{padding-bottom:10px}.rl{background-color:#000000}.rm{height:3px}.rn{width:3px}.ro{margin-right:20px}.rp{max-width:2514px}.rq{max-width:1010px}.rw{max-width:656px}.rx{clear:left}.ry{float:left}.rz{font-size:66px}.sa{line-height:.83}.sg{margin:auto}.sh{padding-bottom:66.66666666666667%}.si{height:0}.sj{margin-bottom:26px}.sk{margin-top:6px}.sl{margin-top:8px}.sm{margin-right:8px}.sn{padding:8px 16px}.so{border-radius:100px}.sp{transition:background 300ms ease}.sr{white-space:nowrap}.ss{border-top:none}.st{height:52px}.su{max-height:52px}.sv{box-sizing:content-box}.sw{position:static}.sy{max-width:155px}.tj{height:0px}.tk{margin-bottom:40px}.tl{margin-bottom:48px}.tz{border-radius:2px}.ub{height:64px}.uc{width:64px}.ud{align-self:flex-end}.ue{flex:1 1 auto}.ui{padding-right:4px}.uj{font-weight:500}.uw{margin-top:16px}.ux{color:rgba(255, 255, 255, 1)}.uy{fill:rgba(255, 255, 255, 1)}.uz{background:rgba(25, 25, 25, 1)}.va{border-color:rgba(25, 25, 25, 1)}.vd:disabled{opacity:0.1}.ve:disabled:hover{background:rgba(25, 25, 25, 1)}.vf:disabled:hover{border-color:rgba(25, 25, 25, 1)}.vo{gap:18px}.vp{fill:rgba(61, 61, 61, 1)}.vr{fill:#242424}.vs{background:0}.vt{border-color:#242424}.vu:disabled:hover{color:#242424}.vv:disabled:hover{fill:#242424}.vw:disabled:hover{border-color:#242424}.wh{border-bottom:solid 1px #E5E5E5}.wi{margin-top:72px}.wj{padding:24px 0}.wk{margin-bottom:0px}.wl{margin-right:16px}.as:hover:not(:disabled){color:rgba(25, 25, 25, 1)}.at:hover:not(:disabled){fill:rgba(25, 25, 25, 1)}.et:hover{background:rgba(93, 105, 215, 1)}.eu:hover{border-color:rgba(93, 105, 215, 1)}.ev:hover{cursor:pointer}.fl:hover{color:#242424}.fm:hover{fill:#242424}.fq:hover svg{fill:#242424}.ft:hover{background-color:rgba(0, 0, 0, 0.1)}.ju:hover{text-decoration:underline}.jz:hover:not(:disabled){color:rgba(93, 105, 215, 1)}.ka:hover:not(:disabled){fill:rgba(93, 105, 215, 1)}.mh:hover{fill:rgba(8, 8, 8, 1)}.my:hover{fill:#000000}.mz:hover p{color:#000000}.nb:hover{color:#000000}.nh:hover svg{color:#000000}.sq:hover{background-color:#F2F2F2}.ua:hover{background-color:none}.vb:hover{background:#000000}.vc:hover{border-color:#242424}.vq:hover{fill:rgba(25, 25, 25, 1)}.bd:focus-within path{fill:#242424}.ho:focus{transform:scale(1.01)}.mg:focus{fill:rgba(8, 8, 8, 1)}.ni:focus svg{color:#000000}.ml:active{border-style:none}</style><style type="text/css" data-fela-rehydration="609" data-fela-type="RULE" media="all and (min-width: 1080px)">.d{display:none}.bw{width:64px}.cg{margin:0 64px}.cv{height:48px}.dc{margin-bottom:52px}.do{margin-bottom:48px}.ef{font-size:14px}.eg{line-height:20px}.em{font-size:13px}.eo{padding:5px 12px}.fh{display:flex}.fy{margin-bottom:68px}.gc{max-width:680px}.ha{max-width:1192px}.hi{margin-top:40px}.iu{font-size:42px}.iv{margin-top:1em}.iw{margin-bottom:32px}.ix{line-height:52px}.iy{letter-spacing:-0.011em}.jd{align-items:center}.lf{border-top:solid 1px #F2F2F2}.lg{border-bottom:solid 1px #F2F2F2}.lh{margin:32px 0 0}.li{padding:3px 8px}.lr> *{margin-right:24px}.ls> :last-child{margin-right:0}.ms{margin-top:0px}.ne{margin:0}.ou{font-size:20px}.ov{margin-top:2.14em}.ow{line-height:32px}.ox{letter-spacing:-0.003em}.pe{margin-top:56px}.pp{margin-top:1.75em}.pu{margin-top:2.64em}.qo{font-size:24px}.qp{margin-top:1.95em}.qq{line-height:30px}.qr{letter-spacing:-0.016em}.qx{margin-top:0.94em}.rv{margin-top:1.25em}.sf{padding-top:7px}.td{display:inline-block}.ti{margin-bottom:104px}.tm{flex-direction:row}.tp{margin-bottom:0}.tq{margin-right:20px}.uf{max-width:500px}.uu{line-height:24px}.uv{letter-spacing:0}.vk{margin-bottom:88px}.vn{margin-bottom:72px}.wb{width:min-width}.wg{padding-top:72px}</style><style type="text/css" data-fela-rehydration="609" data-fela-type="RULE" media="all and (max-width: 1079.98px)">.e{display:none}.ht{margin-left:auto}.hu{text-align:center}.mr{margin-top:0px}.tc{display:inline-block}</style><style type="text/css" data-fela-rehydration="609" data-fela-type="RULE" media="all and (max-width: 903.98px)">.f{display:none}.mq{margin-top:0px}.tb{display:inline-block}</style><style type="text/css" data-fela-rehydration="609" data-fela-type="RULE" media="all and (max-width: 727.98px)">.g{display:none}.mo{margin-top:0px}.mp{margin-right:0px}.ta{display:inline-block}</style><style type="text/css" data-fela-rehydration="609" data-fela-type="RULE" media="all and (max-width: 551.98px)">.h{display:none}.s{display:flex}.t{justify-content:space-between}.bs{width:24px}.cc{margin:0 24px}.cr{height:40px}.cy{margin-bottom:44px}.dk{margin-bottom:32px}.dx{font-size:13px}.dy{line-height:20px}.eh{padding:0px 8px 1px}.fu{margin-bottom:4px}.gt{margin:0}.gu{max-width:100%}.he{margin-top:32px}.ia{font-size:32px}.ib{margin-top:1.01em}.ic{margin-bottom:24px}.id{line-height:38px}.ie{letter-spacing:-0.014em}.iz{align-items:flex-start}.kc{flex-direction:column}.kf{margin-bottom:2px}.kt{margin:24px -24px 0}.ku{padding:0}.lj> *{margin-right:8px}.lk> :last-child{margin-right:24px}.mb{margin-left:0px}.mm{margin-top:0px}.mn{margin-right:0px}.nj{border:1px solid #F2F2F2}.nk{border-radius:99em}.nl{padding:0px 16px 0px 12px}.nm{height:38px}.nn{align-items:center}.np svg{margin-right:8px}.oe{font-size:18px}.of{margin-top:1.56em}.og{line-height:28px}.oh{letter-spacing:-0.003em}.pa{margin-top:40px}.pl{margin-top:1.08em}.pq{margin-top:2em}.py{font-size:20px}.pz{margin-top:1.2em}.qa{line-height:24px}.qb{letter-spacing:0}.qt{margin-top:0.67em}.rr{margin-top:0.93em}.sb{padding-top:0}.sz{display:inline-block}.te{margin-bottom:96px}.tx{margin-bottom:20px}.ty{margin-right:0}.uk{font-size:24px}.ul{line-height:30px}.um{letter-spacing:-0.016em}.vg{margin-bottom:64px}.vx{width:100%}.wc{padding-top:48px}.no:hover{border-color:#E5E5E5}</style><style type="text/css" data-fela-rehydration="609" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.i{display:none}.bv{width:64px}.cf{margin:0 64px}.cu{height:48px}.db{margin-bottom:52px}.dn{margin-bottom:48px}.ed{font-size:14px}.ee{line-height:20px}.ek{font-size:13px}.el{padding:5px 12px}.fg{display:flex}.fx{margin-bottom:68px}.gb{max-width:680px}.gz{max-width:1192px}.hh{margin-top:40px}.ip{font-size:42px}.iq{margin-top:1em}.ir{margin-bottom:32px}.is{line-height:52px}.it{letter-spacing:-0.011em}.jc{align-items:center}.lb{border-top:solid 1px #F2F2F2}.lc{border-bottom:solid 1px #F2F2F2}.ld{margin:32px 0 0}.le{padding:3px 8px}.lp> *{margin-right:24px}.lq> :last-child{margin-right:0}.nd{margin:0}.oq{font-size:20px}.or{margin-top:2.14em}.os{line-height:32px}.ot{letter-spacing:-0.003em}.pd{margin-top:56px}.po{margin-top:1.75em}.pt{margin-top:2.64em}.qk{font-size:24px}.ql{margin-top:1.95em}.qm{line-height:30px}.qn{letter-spacing:-0.016em}.qw{margin-top:0.94em}.ru{margin-top:1.25em}.se{padding-top:7px}.th{margin-bottom:104px}.tn{flex-direction:row}.tr{margin-bottom:0}.ts{margin-right:20px}.ug{max-width:500px}.us{line-height:24px}.ut{letter-spacing:0}.vj{margin-bottom:88px}.vm{margin-bottom:72px}.wa{width:min-width}.wf{padding-top:72px}</style><style type="text/css" data-fela-rehydration="609" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.j{display:none}.w{display:flex}.x{justify-content:space-between}.bu{width:64px}.ce{margin:0 48px}.ct{height:48px}.da{margin-bottom:52px}.dm{margin-bottom:48px}.eb{font-size:13px}.ec{line-height:20px}.ej{padding:0px 8px 1px}.fw{margin-bottom:68px}.ga{max-width:680px}.gx{margin:0}.gy{max-width:100%}.hg{margin-top:40px}.ik{font-size:42px}.il{margin-top:1em}.im{margin-bottom:32px}.in{line-height:52px}.io{letter-spacing:-0.011em}.jb{align-items:center}.kx{border-top:solid 1px #F2F2F2}.ky{border-bottom:solid 1px #F2F2F2}.kz{margin:32px 0 0}.la{padding:3px 8px}.ln> *{margin-right:24px}.lo> :last-child{margin-right:0}.om{font-size:20px}.on{margin-top:2.14em}.oo{line-height:32px}.op{letter-spacing:-0.003em}.pc{margin-top:56px}.pn{margin-top:1.75em}.ps{margin-top:2.64em}.qg{font-size:24px}.qh{margin-top:1.95em}.qi{line-height:30px}.qj{letter-spacing:-0.016em}.qv{margin-top:0.94em}.rt{margin-top:1.25em}.sd{padding-top:7px}.tg{margin-bottom:104px}.to{flex-direction:row}.tt{margin-bottom:0}.tu{margin-right:20px}.uh{max-width:500px}.uq{line-height:24px}.ur{letter-spacing:0}.vi{margin-bottom:88px}.vl{margin-bottom:72px}.vz{width:min-width}.we{padding-top:72px}</style><style type="text/css" data-fela-rehydration="609" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.k{display:none}.u{display:flex}.v{justify-content:space-between}.bt{width:24px}.cd{margin:0 24px}.cs{height:40px}.cz{margin-bottom:44px}.dl{margin-bottom:32px}.dz{font-size:13px}.ea{line-height:20px}.ei{padding:0px 8px 1px}.fv{margin-bottom:4px}.gv{margin:0}.gw{max-width:100%}.hf{margin-top:32px}.if{font-size:32px}.ig{margin-top:1.01em}.ih{margin-bottom:24px}.ii{line-height:38px}.ij{letter-spacing:-0.014em}.ja{align-items:flex-start}.kd{flex-direction:column}.kg{margin-bottom:2px}.kv{margin:24px 0 0}.kw{padding:0}.ll> *{margin-right:8px}.lm> :last-child{margin-right:8px}.mc{margin-left:0px}.nq{border:1px solid #F2F2F2}.nr{border-radius:99em}.ns{padding:0px 16px 0px 12px}.nt{height:38px}.nu{align-items:center}.nw svg{margin-right:8px}.oi{font-size:18px}.oj{margin-top:1.56em}.ok{line-height:28px}.ol{letter-spacing:-0.003em}.pb{margin-top:40px}.pm{margin-top:1.08em}.pr{margin-top:2em}.qc{font-size:20px}.qd{margin-top:1.2em}.qe{line-height:24px}.qf{letter-spacing:0}.qu{margin-top:0.67em}.rs{margin-top:0.93em}.sc{padding-top:0}.tf{margin-bottom:96px}.tv{margin-bottom:20px}.tw{margin-right:0}.un{font-size:24px}.uo{line-height:30px}.up{letter-spacing:-0.016em}.vh{margin-bottom:64px}.vy{width:100%}.wd{padding-top:48px}.nv:hover{border-color:#E5E5E5}</style><style type="text/css" data-fela-rehydration="609" data-fela-type="RULE" media="print">.sx{display:none}</style><style type="text/css" data-fela-rehydration="609" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)">.hl{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}</style><style type="text/css" data-fela-rehydration="609" data-fela-type="RULE" media="(orientation: landscape) and (max-width: 903.98px)">.kq{max-height:none}</style></head><body><div id="root"><div class="a b c"><div class="d e f g h i j k"></div><script>document.domain = document.domain;</script><div class="l c"><div class="l m n o c"><div class="p q r s t u v w x i d y z"><a class="du ag dv bf ak b am an ao ap aq ar as at s u w i d q dw z" href="https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8cf3380435b5&amp;%7Efeature=LoOpenInAppButton&amp;%7Echannel=ShowPostUnderCollection&amp;source=---top_nav_layout_nav----------------------------------" rel="noopener follow">Open in app<svg xmlns="http://www.w3.org/2000/svg" width="10" height="10" fill="none" viewBox="0 0 10 10" class="dt"><path fill="currentColor" d="M.985 8.485a.375.375 0 1 0 .53.53zM8.75 1.25h.375A.375.375 0 0 0 8.75.875zM8.375 6.5a.375.375 0 1 0 .75 0zM3.5.875a.375.375 0 1 0 0 .75zm-1.985 8.14 7.5-7.5-.53-.53-7.5 7.5zm6.86-7.765V6.5h.75V1.25zM3.5 1.625h5.25v-.75H3.5z"></path></svg></a><div class="ab q"><p class="bf b dx dy dz ea eb ec ed ee ef eg du"><span><button class="bf b dx dy eh dz ea ei eb ec ej ek ee el em eg eo ep eq er es et eu ev ew ex ey ez fa fb fc fd bm fe ff" data-testid="headerSignUpButton">Sign up</button></span></p><div class="ax l"><p class="bf b dx dy dz ea eb ec ed ee ef eg du"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerSignInButton" rel="noopener follow" href="/m/signin?operation=login&amp;redirect=https%3A%2F%2Fmedium.com%2Fhuggingface%2Fdistilbert-8cf3380435b5&amp;source=post_page---top_nav_layout_nav-----------------------global_nav-----------">Sign in</a></span></p></div></div></div><div class="p q r ab ac"><div class="ab q ae"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab" aria-label="Homepage" data-testid="headerMediumLogo" rel="noopener follow" href="/?source=---top_nav_layout_nav----------------------------------"><svg xmlns="http://www.w3.org/2000/svg" width="719" height="160" fill="none" viewBox="0 0 719 160" class="au av aw"><path fill="#242424" d="m174.104 9.734.215-.047V8.02H130.39L89.6 103.89 48.81 8.021H1.472v1.666l.212.047c8.018 1.81 12.09 4.509 12.09 14.242V137.93c0 9.734-4.087 12.433-12.106 14.243l-.212.047v1.671h32.118v-1.665l-.213-.048c-8.018-1.809-12.089-4.509-12.089-14.242V30.586l52.399 123.305h2.972l53.925-126.743V140.75c-.687 7.688-4.721 10.062-11.982 11.701l-.215.05v1.652h55.948v-1.652l-.215-.05c-7.269-1.639-11.4-4.013-12.087-11.701l-.037-116.774h.037c0-9.733 4.071-12.432 12.087-14.242m25.555 75.488c.915-20.474 8.268-35.252 20.606-35.507 3.806.063 6.998 1.312 9.479 3.714 5.272 5.118 7.751 15.812 7.368 31.793zm-.553 5.77h65.573v-.275c-.186-15.656-4.721-27.834-13.466-36.196-7.559-7.227-18.751-11.203-30.507-11.203h-.263c-6.101 0-13.584 1.48-18.909 4.16-6.061 2.807-11.407 7.003-15.855 12.511-7.161 8.874-11.499 20.866-12.554 34.343q-.05.606-.092 1.212a50 50 0 0 0-.065 1.151 85.807 85.807 0 0 0-.094 5.689c.71 30.524 17.198 54.917 46.483 54.917 25.705 0 40.675-18.791 44.407-44.013l-1.886-.664c-6.557 13.556-18.334 21.771-31.738 20.769-18.297-1.369-32.314-19.922-31.042-42.395m139.722 41.359c-2.151 5.101-6.639 7.908-12.653 7.908s-11.513-4.129-15.418-11.63c-4.197-8.053-6.405-19.436-6.405-32.92 0-28.067 8.729-46.22 22.24-46.22 5.657 0 10.111 2.807 12.236 7.704zm43.499 20.008c-8.019-1.897-12.089-4.722-12.089-14.951V1.309l-48.716 14.353v1.757l.299-.024c6.72-.543 11.278.386 13.925 2.83 2.072 1.915 3.082 4.853 3.082 8.987v18.66c-4.803-3.067-10.516-4.56-17.448-4.56-14.059 0-26.909 5.92-36.176 16.672-9.66 11.205-14.767 26.518-14.767 44.278-.003 31.72 15.612 53.039 38.851 53.039 13.595 0 24.533-7.449 29.54-20.013v16.865h43.711v-1.746zM424.1 19.819c0-9.904-7.468-17.374-17.375-17.374-9.859 0-17.573 7.632-17.573 17.374s7.721 17.374 17.573 17.374c9.907 0 17.375-7.47 17.375-17.374m11.499 132.546c-8.019-1.897-12.089-4.722-12.089-14.951h-.035V43.635l-43.714 12.551v1.705l.263.024c9.458.842 12.047 4.1 12.047 15.152v81.086h43.751v-1.746zm112.013 0c-8.018-1.897-12.089-4.722-12.089-14.951V43.635l-41.621 12.137v1.71l.246.026c7.733.813 9.967 4.257 9.967 15.36v59.279c-2.578 5.102-7.415 8.131-13.274 8.336-9.503 0-14.736-6.419-14.736-18.073V43.638l-43.714 12.55v1.703l.262.024c9.459.84 12.05 4.097 12.05 15.152v50.17a56.3 56.3 0 0 0 .91 10.444l.787 3.423c3.701 13.262 13.398 20.197 28.59 20.197 12.868 0 24.147-7.966 29.115-20.43v17.311h43.714v-1.747zm169.818 1.788v-1.749l-.213-.05c-8.7-2.006-12.089-5.789-12.089-13.49v-63.79c0-19.89-11.171-31.761-29.883-31.761-13.64 0-25.141 7.882-29.569 20.16-3.517-13.01-13.639-20.16-28.606-20.16-13.146 0-23.449 6.938-27.869 18.657V43.643L545.487 55.68v1.715l.263.024c9.345.829 12.047 4.181 12.047 14.95v81.784h40.787v-1.746l-.215-.053c-6.941-1.631-9.181-4.606-9.181-12.239V66.998c1.836-4.289 5.537-9.37 12.853-9.37 9.086 0 13.692 6.296 13.692 18.697v77.828h40.797v-1.746l-.215-.053c-6.94-1.631-9.18-4.606-9.18-12.239V75.066a42 42 0 0 0-.578-7.26c1.947-4.661 5.86-10.177 13.475-10.177 9.214 0 13.691 6.114 13.691 18.696v77.828z"></path></svg></a><div class="ax h"><div class="ab ay az ba bb q bc bd"><div class="bm" aria-hidden="false" aria-describedby="searchResults" aria-labelledby="searchResults"></div><div class="bn bo ab"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.092 11.06a6.95 6.95 0 1 1 13.9 0 6.95 6.95 0 0 1-13.9 0m6.95-8.05a8.05 8.05 0 1 0 5.13 14.26l3.75 3.75a.56.56 0 1 0 .79-.79l-3.73-3.73A8.05 8.05 0 0 0 11.042 3z" clip-rule="evenodd"></path></svg></div><input role="combobox" aria-controls="searchResults" aria-expanded="false" aria-label="search" data-testid="headerSearchInput" tabindex="0" class="ay be bf bg z bh bi bj bk bl" placeholder="Search" value=""/></div></div></div><div class="h k w fg fh"><div class="fi ab"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerWriteButton" rel="noopener follow" href="/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fnew-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav-----------"><div class="bf b bg z du fj fk ab q fl fm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" aria-label="Write"><path fill="currentColor" d="M14 4a.5.5 0 0 0 0-1zm7 6a.5.5 0 0 0-1 0zm-7-7H4v1h10zM3 4v16h1V4zm1 17h16v-1H4zm17-1V10h-1v10zm-1 1a1 1 0 0 0 1-1h-1zM3 20a1 1 0 0 0 1 1v-1zM4 3a1 1 0 0 0-1 1h1z"></path><path stroke="currentColor" d="m17.5 4.5-8.458 8.458a.25.25 0 0 0-.06.098l-.824 2.47a.25.25 0 0 0 .316.316l2.47-.823a.25.25 0 0 0 .098-.06L19.5 6.5m-2-2 2.323-2.323a.25.25 0 0 1 .354 0l1.646 1.646a.25.25 0 0 1 0 .354L19.5 6.5m-2-2 2 2"></path></svg><div class="dt l">Write</div></div></a></span></div></div><div class="k j i d"><div class="fi ab"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerSearchButton" rel="noopener follow" href="/search?source=---top_nav_layout_nav----------------------------------"><div class="bf b bg z du fj fk ab q fl fm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" aria-label="Search"><path fill="currentColor" fill-rule="evenodd" d="M4.092 11.06a6.95 6.95 0 1 1 13.9 0 6.95 6.95 0 0 1-13.9 0m6.95-8.05a8.05 8.05 0 1 0 5.13 14.26l3.75 3.75a.56.56 0 1 0 .79-.79l-3.73-3.73A8.05 8.05 0 0 0 11.042 3z" clip-rule="evenodd"></path></svg></div></a></div></div><div class="fi h k j"><div class="ab q"><p class="bf b dx dy dz ea eb ec ed ee ef eg du"><span><button class="bf b dx dy eh dz ea ei eb ec ej ek ee el em eg eo ep eq er es et eu ev ew ex ey ez fa fb fc fd bm fe ff" data-testid="headerSignUpButton">Sign up</button></span></p><div class="ax l"><p class="bf b dx dy dz ea eb ec ed ee ef eg du"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerSignInButton" rel="noopener follow" href="/m/signin?operation=login&amp;redirect=https%3A%2F%2Fmedium.com%2Fhuggingface%2Fdistilbert-8cf3380435b5&amp;source=post_page---top_nav_layout_nav-----------------------global_nav-----------">Sign in</a></span></p></div></div></div><div class="l" aria-hidden="false"><button class="ay fn am ab q ao fo fp fq" aria-label="user options menu" data-testid="headerUserIcon"><div class="l fj"><img alt="" class="l fd by bz ca cx" src="https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png" width="32" height="32" loading="lazy" role="presentation"/><div class="fr by l bz ca fs n ay ft"></div></div></button></div></div></div><div class="l"><div class="fu fv fw fx fy l"><div class="ab cb"><div class="ci bh fz ga gb gc"></div></div><article><div class="l"><div class="l"><span class="l"></span><section><div><div class="fs gi gj gk gl gm"></div><div class="gn go gp gq gr"><div class="gs"><div class="ab cb"><div class="gt gu gv gw gx gy cf gz cg ha ci bh"><figure class="he hf hg hh hi gs hj hk paragraph-image"><div role="button" tabindex="0" class="hl hm fj hn bh ho"><div class="hb hc hd"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*CVGvdYELFWlWmmOdTuRF4A.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*CVGvdYELFWlWmmOdTuRF4A.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*CVGvdYELFWlWmmOdTuRF4A.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*CVGvdYELFWlWmmOdTuRF4A.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*CVGvdYELFWlWmmOdTuRF4A.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*CVGvdYELFWlWmmOdTuRF4A.jpeg 1100w, https://miro.medium.com/v2/resize:fit:2000/format:webp/1*CVGvdYELFWlWmmOdTuRF4A.jpeg 2000w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 1000px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*CVGvdYELFWlWmmOdTuRF4A.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*CVGvdYELFWlWmmOdTuRF4A.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*CVGvdYELFWlWmmOdTuRF4A.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*CVGvdYELFWlWmmOdTuRF4A.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*CVGvdYELFWlWmmOdTuRF4A.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*CVGvdYELFWlWmmOdTuRF4A.jpeg 1100w, https://miro.medium.com/v2/resize:fit:2000/1*CVGvdYELFWlWmmOdTuRF4A.jpeg 2000w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 1000px"/><img alt="" class="bh hp hq c" width="1000" height="750" loading="eager" role="presentation"/></picture></div></div><figcaption class="hr ff hs hb hc ht hu bf b bg z du">Photo by <a class="af hv" href="https://unsplash.com/@shubhamsharan?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Shubham Sharan</a> on <a class="af hv" href="https://unsplash.com/search/photos/teacher?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh fz ga gb gc"><div><h1 id="4647" class="pw-post-title hw hx hy bf hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy bk" data-testid="storyTitle">üèé Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT</h1><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="iz ja jb jc jd ab"><div><div class="ab je"><div><div class="bm" aria-hidden="false"><a rel="noopener follow" href="/@victorsanh?source=post_page---byline--8cf3380435b5--------------------------------"><div class="l jf jg by jh ji"><div class="l fj"><img alt="Victor Sanh" class="l fd by dd de cx" src="https://miro.medium.com/v2/resize:fill:88:88/1*3AEtg8Ma0mqLlIwaF8Swow@2x.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"/><div class="jj by l dd de fs n jk ft"></div></div></div></a></div></div><div class="jl ab fj"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/huggingface?source=post_page---byline--8cf3380435b5--------------------------------" rel="noopener follow"><div class="l jm jn by jh jo"><div class="l fj"><img alt="HuggingFace" class="l fd by br jp cx" src="https://miro.medium.com/v2/resize:fill:48:48/1*ABKawA7BD68tMpncHfAo_Q@2x.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto"/><div class="jj by l br jp fs n jk ft"></div></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="jq ab q"><div class="ab q jr"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b js jt bk"><a class="af ag ah ai aj ak al am an ao ap aq ar ju" data-testid="authorName" rel="noopener follow" href="/@victorsanh?source=post_page---byline--8cf3380435b5--------------------------------">Victor Sanh</a></p></div></div></div><span class="jv jw" aria-hidden="true"><span class="bf b bg z du">¬∑</span></span><p class="bf b js jt du"><span><a class="jx jy ah ai aj ak al am an ao ap aq ar ex jz ka" rel="noopener follow" href="/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fac59742e5349&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fhuggingface%2Fdistilbert-8cf3380435b5&amp;user=Victor+Sanh&amp;userId=ac59742e5349&amp;source=post_page-ac59742e5349--byline--8cf3380435b5---------------------post_header-----------">Follow</a></span></p></div></div></span></div></div><div class="l kb"><span class="bf b bg z du"><div class="ab cn kc kd ke"><div class="kf kg ab"><div class="bf b bg z du ab kh"><span class="ki l kb">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar ju ab q" data-testid="publicationName" href="https://medium.com/huggingface?source=post_page---byline--8cf3380435b5--------------------------------" rel="noopener follow"><p class="bf b bg z kj kk kl km kn ko kp kq bk">HuggingFace</p></a></div></div></div><div class="h k"><span class="jv jw" aria-hidden="true"><span class="bf b bg z du">¬∑</span></span></div></div><span class="bf b bg z du"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="kr ks l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z du">¬∑</span></span></div><span data-testid="storyPublishDate">Aug 28, 2019</span></div></span></div></span></div></div></div><div class="ab cp kt ku kv kw kx ky kz la lb lc ld le lf lg lh li"><div class="h k w fg fh q"><div class="ly l"><div class="ab q lz ma"><div class="pw-multi-vote-icon fj ki mb mc md"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerClapButton" rel="noopener follow" href="/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fhuggingface%2F8cf3380435b5&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fhuggingface%2Fdistilbert-8cf3380435b5&amp;user=Victor+Sanh&amp;userId=ac59742e5349&amp;source=---header_actions--8cf3380435b5---------------------clap_footer-----------"><div><div class="bm" aria-hidden="false"><div class="me ao mf mg mh mi am mj mk ml md"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM13.916 3.953l1.523-2.112-1.184-.39zM8.589 1.84l1.522 2.112-.337-2.501zM18.523 18.92c-.86.86-1.75 1.246-2.62 1.33a6 6 0 0 0 .407-.372c2.388-2.389 2.86-4.951 1.399-7.623l-.912-1.603-.79-1.672c-.26-.56-.194-.98.203-1.288a.7.7 0 0 1 .546-.132c.283.046.546.231.728.5l2.363 4.157c.976 1.624 1.141 4.237-1.324 6.702m-10.999-.438L3.37 14.328a.828.828 0 0 1 .585-1.408.83.83 0 0 1 .585.242l2.158 2.157a.365.365 0 0 0 .516-.516l-2.157-2.158-1.449-1.449a.826.826 0 0 1 1.167-1.17l3.438 3.44a.363.363 0 0 0 .516 0 .364.364 0 0 0 0-.516L5.293 9.513l-.97-.97a.826.826 0 0 1 0-1.166.84.84 0 0 1 1.167 0l.97.968 3.437 3.436a.36.36 0 0 0 .517 0 .366.366 0 0 0 0-.516L6.977 7.83a.82.82 0 0 1-.241-.584.82.82 0 0 1 .824-.826c.219 0 .43.087.584.242l5.787 5.787a.366.366 0 0 0 .587-.415l-1.117-2.363c-.26-.56-.194-.98.204-1.289a.7.7 0 0 1 .546-.132c.283.046.545.232.727.501l2.193 3.86c1.302 2.38.883 4.59-1.277 6.75-1.156 1.156-2.602 1.627-4.19 1.367-1.418-.236-2.866-1.033-4.079-2.246M10.75 5.971l2.12 2.12c-.41.502-.465 1.17-.128 1.89l.22.465-3.523-3.523a.8.8 0 0 1-.097-.368c0-.22.086-.428.241-.584a.847.847 0 0 1 1.167 0m7.355 1.705c-.31-.461-.746-.758-1.23-.837a1.44 1.44 0 0 0-1.11.275c-.312.24-.505.543-.59.881a1.74 1.74 0 0 0-.906-.465 1.47 1.47 0 0 0-.82.106l-2.182-2.182a1.56 1.56 0 0 0-2.2 0 1.54 1.54 0 0 0-.396.701 1.56 1.56 0 0 0-2.21-.01 1.55 1.55 0 0 0-.416.753c-.624-.624-1.649-.624-2.237-.037a1.557 1.557 0 0 0 0 2.2c-.239.1-.501.238-.715.453a1.56 1.56 0 0 0 0 2.2l.516.515a1.556 1.556 0 0 0-.753 2.615L7.01 19c1.32 1.319 2.909 2.189 4.475 2.449q.482.08.971.08c.85 0 1.653-.198 2.393-.579.231.033.46.054.686.054 1.266 0 2.457-.52 3.505-1.567 2.763-2.763 2.552-5.734 1.439-7.586z" clip-rule="evenodd"></path></svg></div></div></div></a></span></div><div class="pw-multi-vote-count l mm mn mo mp mq mr ms"><p class="bf b dv z du"><span class="mt">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao me mw mx ab q fk my mz" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="mv"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"></path></svg><p class="bf b dv z du"><span class="pw-responses-count mu mv">20</span></p></button></div></div></div><div class="ab q lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx"><div class="na k j i d"></div><div class="h k"><div><div class="bm" aria-hidden="false"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerBookmarkButton" rel="noopener follow" href="/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8cf3380435b5&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fhuggingface%2Fdistilbert-8cf3380435b5&amp;source=---header_actions--8cf3380435b5---------------------bookmark_footer-----------"><svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" fill="none" viewBox="0 0 25 25" class="du nb" aria-label="Add to list bookmark button"><path fill="currentColor" d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .805.396L12.5 17l5.695 4.396A.5.5 0 0 0 19 21v-8.5a.5.5 0 0 0-1 0v7.485l-5.195-4.012a.5.5 0 0 0-.61 0L7 19.985z"></path></svg></a></span></div></div></div><div class="fd nc cn"><div class="l ae"><div class="ab cb"><div class="gt gv gx nd ne hp ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af fk ah ai aj ak al nf an ao ap ex ng nh mz ni nj nk nl nm s nn no np nq nr ns nt u nu nv nw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"></path></svg><div class="j i d"><p class="bf b bg z du">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af fk ah ai aj ak al nf an ao ap ex ng nh mz ni nj nk nl nm s nn no np nq nr ns nt u nu nv nw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"></path></svg><div class="j i d"><p class="bf b bg z du">Share</p></div></button></div></div></div></div></div></div></div></div></div><blockquote class="nx ny nz"><p id="2d50" class="oa ob oc od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk"><strong class="od hz">2019, October 3rd ‚Äî Update: </strong>We are releasing <a class="af hv" href="https://arxiv.org/abs/1910.01108" rel="noopener ugc nofollow" target="_blank">our NeurIPS 2019 workshop paper</a> describing our approach on DistilBERT with improved results: 97% of BERT‚Äôs performance on GLUE (<strong class="od hz">the results in the paper superseed the results presented here</strong>). The approach is slightly different from the one explained in this present blog post so this blog post should be a good entry point to the paper! We applied the same method to GPT2 and are releasing DistilGPT2! Training code and pre-trained weights for DistilBERT and DistilGPT2 are available <a class="af hv" href="https://github.com/huggingface/transformers/tree/master/examples/distillation" rel="noopener ugc nofollow" target="_blank">here</a>. ü§ó</p></blockquote><p id="2315" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">In the last 18 months, transfer learning from<strong class="od hz"> large-scale language models </strong>has significantly improved upon the state-of-the-art on pretty much every Natural Language Processing task.</p><p id="ec0d" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">Usually based on the Transformer architecture of <a class="af hv" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">Vaswani et al.</a>, these pre-trained language models keep getting<strong class="od hz"> larger and larger </strong>and<strong class="od hz"> </strong>being trained on <strong class="od hz">bigger datasets</strong>. The latest model from Nvidia has <a class="af hv" href="https://venturebeat.com/2019/08/13/nvidia-trains-worlds-largest-transformer-based-language-model/" rel="noopener ugc nofollow" target="_blank">8.3 billion parameters</a>: 24 times larger than BERT-large, 5 times larger than GPT-2, while <a class="af hv" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank">RoBERTa</a>, the latest work from Facebook AI, was trained on 160GB of text üòµ</p></div></div><div class="gs"><div class="ab cb"><div class="gt gu gv gw gx gy cf gz cg ha ci bh"><figure class="pa pb pc pd pe gs hj hk paragraph-image"><div role="button" tabindex="0" class="hl hm fj hn bh ho"><div class="hb hc oz"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*IFVX74cEe8U5D1GveL1uZA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*IFVX74cEe8U5D1GveL1uZA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*IFVX74cEe8U5D1GveL1uZA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*IFVX74cEe8U5D1GveL1uZA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*IFVX74cEe8U5D1GveL1uZA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*IFVX74cEe8U5D1GveL1uZA.png 1100w, https://miro.medium.com/v2/resize:fit:2000/format:webp/1*IFVX74cEe8U5D1GveL1uZA.png 2000w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 1000px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*IFVX74cEe8U5D1GveL1uZA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*IFVX74cEe8U5D1GveL1uZA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*IFVX74cEe8U5D1GveL1uZA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*IFVX74cEe8U5D1GveL1uZA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*IFVX74cEe8U5D1GveL1uZA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*IFVX74cEe8U5D1GveL1uZA.png 1100w, https://miro.medium.com/v2/resize:fit:2000/1*IFVX74cEe8U5D1GveL1uZA.png 2000w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 1000px"/><img alt="" class="bh hp hq c" width="1000" height="596" loading="eager" role="presentation"/></picture></div></div><figcaption class="hr ff hs hb hc ht hu bf b bg z du">Some people in the community question the relevance of keeping on training larger and larger Transformer especially when you take into account the financial and environmental cost of training. Here‚Äôs are some of the latest large models and their <strong class="bf pf">size in millions of parameters</strong>.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh fz ga gb gc"><p id="1a76" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">At Hugging Face, we experienced first-hand the growing popularity of these models as our <a class="af hv" href="https://github.com/huggingface/pytorch-transformers" rel="noopener ugc nofollow" target="_blank">NLP library</a> ‚Äî which encapsulates most of them ‚Äî got installed more than 400,000 times in just a few months.</p><p id="38cd" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">However, as these models were reaching a larger NLP community, an important and challenging question started to emerge. <strong class="od hz">How should we put these monsters in production?</strong> <strong class="od hz">How can we use such large models under low latency constraints?</strong> Do we need (costly) GPU servers to serve at scale?</p><blockquote class="pg"><p id="367c" class="ph pi hy bf pj pk pl pm pn po pp oy du">For many researchers and developers, these can be deal-breaking issues üí∏</p></blockquote><p id="9f73" class="pw-post-body-paragraph oa ob hy od b oe pq og oh oi pr ok ol om ps oo op oq pt os ot ou pu ow ox oy gn bk">To build more privacy-respecting systems, we noticed an increasing need to have <a class="af hv" href="https://github.com/huggingface/swift-coreml-transformers" rel="noopener ugc nofollow" target="_blank">machine learning systems operate <strong class="od hz">on the edge</strong></a> rather than calling a cloud API and sending possibly private data to servers. Running models on devices like your smartphone üì≤ also requires<strong class="od hz"> light-weight, responsive and energy-efficient models!</strong></p><p id="79fb" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">Last but not least, we are more and more concerned about the environmental cost of scaling exponentially computing requirements of these models.</p><blockquote class="pg"><p id="9819" class="ph pi hy bf pj pk pl pm pn po pp oy du">So, how can we reduce the size of these monster models<em class="pv">‚ÅâÔ∏è</em></p></blockquote><p id="25d2" class="pw-post-body-paragraph oa ob hy od b oe pq og oh oi pr ok ol om ps oo op oq pt os ot ou pu ow ox oy gn bk">There are many techniques available to tackle the previous questions. The most common tools include <strong class="od hz">quantization</strong> (approximating the weights of a network with a smaller precision) and <strong class="od hz">weights pruning </strong>(removing some connections in the network). For these technics, you can have a look at the excellent <a class="af hv" href="https://blog.rasa.com/compressing-bert-for-faster-prediction-2/" rel="noopener ugc nofollow" target="_blank">blog post of Rasa</a> on quantizing BERT.</p><p id="5d83" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">We decided to focus on <strong class="od hz">distillation</strong>: a technique you can use to compress a large model, called the teacher, into a smaller model, called the student.</p><h1 id="de6c" class="pw px hy bf pf py pz qa qb qc qd qe qf qg qh qi qj qk ql qm qn qo qp qq qr qs bk">‚öóÔ∏è Knowledge Distillation ‚Äî Transferring generalization capabilities</h1><p id="7a99" class="pw-post-body-paragraph oa ob hy od b oe qt og oh oi qu ok ol om qv oo op oq qw os ot ou qx ow ox oy gn bk"><em class="oc">Knowledge distillation</em> (sometimes also referred to as <em class="oc">teacher-student learning</em>) is a <strong class="od hz">compression technique in which a small model is trained to reproduce the behavior of a larger model</strong> (or an ensemble of models). It was introduced by <a class="af hv" href="https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf" rel="noopener ugc nofollow" target="_blank">Bucila et al.</a> and generalized by <a class="af hv" href="https://arxiv.org/abs/1503.02531" rel="noopener ugc nofollow" target="_blank">Hinton et al.</a> a few years later. We will follow the latter method.</p><p id="a284" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">In supervised learning, a classification model is generally trained to predict a gold class by maximizing its probability (softmax of logits) using the log-likelihood signal. In many cases, a good performance model will predict an output distribution with the correct class having a high probability, leaving other classes with <strong class="od hz">probabilities near zero</strong>.</p><blockquote class="pg"><p id="0910" class="ph pi hy bf pj pk pl pm pn po pp oy du"><strong class="al">But, some of these ‚Äúalmost-zero‚Äù probabilities are larger than the others, and this reflects, in part, the generalization capabilities of the model.</strong></p></blockquote><p id="c52b" class="pw-post-body-paragraph oa ob hy od b oe pq og oh oi pr ok ol om ps oo op oq pt os ot ou pu ow ox oy gn bk">For instance, a <em class="oc">desk chair</em> might be mistaken with an <em class="oc">armchair</em> but should usually not be mistaken with a <em class="oc">mushroom</em>. This uncertainty is sometimes referred to as the <strong class="od hz">‚Äúdark knowledge‚Äù üåö</strong></p><p id="a6cd" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">Another way to understand distillation is that it prevents the model to be too sure about its prediction (similarly to label smoothing).</p><p id="f167" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">Here is an example to see this idea in practice. In language modeling, we can easily observe this uncertainty by looking at the distribution over the vocabulary. Here are the top 20 guesses by BERT for completing this famous quote from the <em class="oc">Casablanca</em> movie:</p><figure class="pa pb pc pd pe gs hb hc paragraph-image"><div role="button" tabindex="0" class="hl hm fj hn bh ho"><div class="hb hc qy"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*X3SZ5AL75_z29XyuR4rpeQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*X3SZ5AL75_z29XyuR4rpeQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*X3SZ5AL75_z29XyuR4rpeQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*X3SZ5AL75_z29XyuR4rpeQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*X3SZ5AL75_z29XyuR4rpeQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*X3SZ5AL75_z29XyuR4rpeQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X3SZ5AL75_z29XyuR4rpeQ.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*X3SZ5AL75_z29XyuR4rpeQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*X3SZ5AL75_z29XyuR4rpeQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*X3SZ5AL75_z29XyuR4rpeQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*X3SZ5AL75_z29XyuR4rpeQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*X3SZ5AL75_z29XyuR4rpeQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*X3SZ5AL75_z29XyuR4rpeQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*X3SZ5AL75_z29XyuR4rpeQ.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" class="bh hp hq c" width="700" height="264" loading="lazy" role="presentation"/></picture></div></div><figcaption class="hr ff hs hb hc ht hu bf b bg z du">The top 20 guesses from BERT (base) for the masked token. The Language model identified two highly probable tokens (day &amp; life) followed by a long tail of valid tokens.</figcaption></figure><h1 id="4d7f" class="pw px hy bf pf py pz qa qb qc qd qe qf qg qh qi qj qk ql qm qn qo qp qq qr qs bk">üëØ‚Äç‚ôÇÔ∏è How can we copy this dark knowledge?</h1><p id="e4c3" class="pw-post-body-paragraph oa ob hy od b oe qt og oh oi qu ok ol om qv oo op oq qw os ot ou qx ow ox oy gn bk">In the <strong class="od hz">teacher-student training</strong>, we train a student network to mimic the <strong class="od hz">full output distribution</strong> of the teacher network (its knowledge).</p><blockquote class="nx ny nz"><p id="32e6" class="oa ob oc od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">We are training the student to generalize the same way as the teacher by matching the output distribution.</p></blockquote><p id="f67a" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">Rather than training with a cross-entropy over the hard targets (one-hot encoding of the gold class), we transfer the knowledge from the teacher to the student with a cross-entropy over the soft targets (probabilities of the teacher). Our training loss thus becomes:</p><figure class="pa pb pc pd pe gs hb hc paragraph-image"><div class="hb hc qz"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*GZkQPjKC_Wqx1F4Uu3FdiQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*GZkQPjKC_Wqx1F4Uu3FdiQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*GZkQPjKC_Wqx1F4Uu3FdiQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*GZkQPjKC_Wqx1F4Uu3FdiQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*GZkQPjKC_Wqx1F4Uu3FdiQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*GZkQPjKC_Wqx1F4Uu3FdiQ.png 1100w, https://miro.medium.com/v2/resize:fit:498/format:webp/1*GZkQPjKC_Wqx1F4Uu3FdiQ.png 498w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 249px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*GZkQPjKC_Wqx1F4Uu3FdiQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*GZkQPjKC_Wqx1F4Uu3FdiQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*GZkQPjKC_Wqx1F4Uu3FdiQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*GZkQPjKC_Wqx1F4Uu3FdiQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*GZkQPjKC_Wqx1F4Uu3FdiQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*GZkQPjKC_Wqx1F4Uu3FdiQ.png 1100w, https://miro.medium.com/v2/resize:fit:498/1*GZkQPjKC_Wqx1F4Uu3FdiQ.png 498w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 249px"/><img alt="" class="bh hp hq c" width="249" height="60" loading="lazy" role="presentation"/></picture></div><figcaption class="hr ff hs hb hc ht hu bf b bg z du">With <strong class="bf pf"><em class="pv">t</em></strong><em class="pv"> the logits from the teacher and </em><strong class="bf pf"><em class="pv">s</em></strong><em class="pv"> the logits of the student</em></figcaption></figure><p id="5b71" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk"><strong class="od hz">This loss is a richer training signal since a single example enforces much more constraint than a single hard target.</strong></p><p id="92c3" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">To further expose the mass of the distribution over the classes, Hinton et al. introduce a <strong class="od hz">softmax-temperature</strong>:</p><figure class="pa pb pc pd pe gs hb hc paragraph-image"><div class="hb hc ra"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*BaVyKMXRWaudFvcI9So8MQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*BaVyKMXRWaudFvcI9So8MQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*BaVyKMXRWaudFvcI9So8MQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*BaVyKMXRWaudFvcI9So8MQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*BaVyKMXRWaudFvcI9So8MQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*BaVyKMXRWaudFvcI9So8MQ.png 1100w, https://miro.medium.com/v2/resize:fit:466/format:webp/1*BaVyKMXRWaudFvcI9So8MQ.png 466w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 233px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*BaVyKMXRWaudFvcI9So8MQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*BaVyKMXRWaudFvcI9So8MQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*BaVyKMXRWaudFvcI9So8MQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*BaVyKMXRWaudFvcI9So8MQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*BaVyKMXRWaudFvcI9So8MQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*BaVyKMXRWaudFvcI9So8MQ.png 1100w, https://miro.medium.com/v2/resize:fit:466/1*BaVyKMXRWaudFvcI9So8MQ.png 466w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 233px"/><img alt="" class="bh hp hq c" width="233" height="72" loading="lazy" role="presentation"/></picture></div><figcaption class="hr ff hs hb hc ht hu bf b bg z du">T is the temperature parameter.</figcaption></figure><p id="6fa8" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">When <em class="oc">T ‚Üí 0</em>, the distribution becomes a Kronecker (and is equivalent to the one-hot target vector), when <em class="oc">T ‚Üí+‚àû</em>, it becomes a uniform distribution. <strong class="od hz">The same temperature parameter is applied both to the student and the teacher at training time, further revealing more signals for each training example</strong>. At inference, <em class="oc">T</em> is set to 1 and recover the standard Softmax.</p><h1 id="f8e3" class="pw px hy bf pf py pz qa qb qc qd qe qf qg qh qi qj qk ql qm qn qo qp qq qr qs bk">üóúHands-on coding in PyTorch ‚Äî Compressing BERT</h1><p id="cb9d" class="pw-post-body-paragraph oa ob hy od b oe qt og oh oi qu ok ol om qv oo op oq qw os ot ou qx ow ox oy gn bk">We want to compress a large language model using distilling. For distilling, we‚Äôll use the <a class="af hv" href="https://en.wikipedia.org/wiki/Kullback‚ÄìLeibler_divergence" rel="noopener ugc nofollow" target="_blank">Kullback-Leibler loss</a> since the optimizations are equivalent:</p><figure class="pa pb pc pd pe gs hb hc paragraph-image"><div class="hb hc rb"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*ipPFl15_nrY_jQ8QsAMC0w.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*ipPFl15_nrY_jQ8QsAMC0w.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*ipPFl15_nrY_jQ8QsAMC0w.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*ipPFl15_nrY_jQ8QsAMC0w.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*ipPFl15_nrY_jQ8QsAMC0w.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ipPFl15_nrY_jQ8QsAMC0w.png 1100w, https://miro.medium.com/v2/resize:fit:1384/format:webp/1*ipPFl15_nrY_jQ8QsAMC0w.png 1384w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 692px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*ipPFl15_nrY_jQ8QsAMC0w.png 640w, https://miro.medium.com/v2/resize:fit:720/1*ipPFl15_nrY_jQ8QsAMC0w.png 720w, https://miro.medium.com/v2/resize:fit:750/1*ipPFl15_nrY_jQ8QsAMC0w.png 750w, https://miro.medium.com/v2/resize:fit:786/1*ipPFl15_nrY_jQ8QsAMC0w.png 786w, https://miro.medium.com/v2/resize:fit:828/1*ipPFl15_nrY_jQ8QsAMC0w.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*ipPFl15_nrY_jQ8QsAMC0w.png 1100w, https://miro.medium.com/v2/resize:fit:1384/1*ipPFl15_nrY_jQ8QsAMC0w.png 1384w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 692px"/><img alt="" class="bh hp hq c" width="692" height="63" loading="lazy" role="presentation"/></picture></div></figure><p id="679f" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">When computing the gradients with respect to <em class="oc">q</em> (the student distribution) we obtain the same gradients. It allows us to leverage PyTorch implementation for faster computation:</p><figure class="pa pb pc pd pe gs hb hc paragraph-image"><div role="button" tabindex="0" class="hl hm fj hn bh ho"><div class="hb hc rc"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*sZSjyGEbxkQfEgHRxCZTaw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*sZSjyGEbxkQfEgHRxCZTaw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*sZSjyGEbxkQfEgHRxCZTaw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*sZSjyGEbxkQfEgHRxCZTaw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*sZSjyGEbxkQfEgHRxCZTaw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*sZSjyGEbxkQfEgHRxCZTaw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sZSjyGEbxkQfEgHRxCZTaw.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*sZSjyGEbxkQfEgHRxCZTaw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*sZSjyGEbxkQfEgHRxCZTaw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*sZSjyGEbxkQfEgHRxCZTaw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*sZSjyGEbxkQfEgHRxCZTaw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*sZSjyGEbxkQfEgHRxCZTaw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*sZSjyGEbxkQfEgHRxCZTaw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*sZSjyGEbxkQfEgHRxCZTaw.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" class="bh hp hq c" width="700" height="532" loading="lazy" role="presentation"/></picture></div></div><figcaption class="hr ff hs hb hc ht hu bf b bg z du">A Knowledge distillation training step in PyTorch. Copy the gist from <a class="af hv" href="https://gist.github.com/VictorSanh/db90644aae5094654db87f9769c2e5ae" rel="noopener ugc nofollow" target="_blank">here</a>.</figcaption></figure><p id="e9cf" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">Using the teacher signal, we are able to train a <strong class="od hz">smaller language model, </strong>we call<strong class="od hz"> DistilBERT, </strong>from the <strong class="od hz">supervision of BERT </strong>üë®‚Äçüë¶ (we used the English <code class="cx rd re rf rg b">bert-base-uncased</code> version of BERT).</p><p id="8b30" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">Following Hinton et al., the training loss is a linear combination of the <em class="oc">distillation loss</em> and the masked <em class="oc">language modeling loss</em>. Our student is a small version of BERT in which we <em class="oc">removed the token-type embeddings and the pooler</em> (used for the next sentence classification task) and kept the rest of the architecture identical while reducing the numbers of layers by a factor of two.</p><blockquote class="pg"><p id="53fb" class="ph pi hy bf pj pk pl pm pn po pp oy du">Overall, our distilled model, <strong class="al">DistilBERT,</strong> has <strong class="al">about half</strong> the total number of parameters of BERT base and retains 95% of BERT‚Äôs performances on the language understanding benchmark GLUE.</p></blockquote><blockquote class="nx ny nz"><p id="9f55" class="oa ob oc od b oe pq og oh oi pr ok ol om ps oo op oq pt os ot ou pu ow ox oy gn bk"><strong class="od hz"><em class="hy">‚ùìNote 1</em> ‚Äî Why not reducing the hidden size as well?<br/></strong>Reducing it from 768 to 512 would reduce the total number of parameters by ~2. However, in modern frameworks, most of the operations are highly optimized and variations on the last dimension of the tensor (hidden dimension) have a small impact on most of the operations used in the Transformer architecture (linear layers and layer normalisation). In our experiments, the number of layers was the determining factor for the inference time, more than the hidden size.<br/>Smaller does not necessarily imply faster‚Ä¶</p><p id="08a4" class="oa ob oc od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk"><strong class="od hz"><em class="hy">‚ùìNote 2</em> ‚Äî Some works on distillation like </strong><a class="af hv" href="https://arxiv.org/pdf/1903.12136.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="od hz">Tang et al.</strong></a><strong class="od hz"> use the L2 distance as a distillation loss directly on downstream tasks</strong>.<br/>Our early experiments suggested that the cross-entropy loss leads to significantly better performance in our case. We hypothesis that in a language modeling setup, the output space (vocabulary) is significantly larger than the dimension of the downstream task output space. The logits may thus compensate for each other in the L2 loss.</p></blockquote></div></div></div><div class="ab cb rh ri rj rk" role="separator"><span class="rl by bm rm rn ro"></span><span class="rl by bm rm rn ro"></span><span class="rl by bm rm rn"></span></div><div class="gn go gp gq gr"><div class="ab cb"><div class="ci bh fz ga gb gc"><p id="e49d" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">Training a sub-network is <strong class="od hz">not only about the architecture. </strong>It is also about <strong class="od hz">finding the right initialization for the sub-network to converge</strong> (see <a class="af hv" href="https://arxiv.org/abs/1803.03635" rel="noopener ugc nofollow" target="_blank">The Lottery Ticket Hypothesis</a> for instance). We thus initialize our student, DistilBERT<em class="oc">,</em> from its teacher, BERT, by taking one layer out of two, leveraging the common hidden size between student and teacher.</p><p id="0a01" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">We also used a few training tricks from the recent <a class="af hv" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank">RoBERTa paper</a> which showed that <strong class="od hz">the way BERT is trained is crucial for its final performance</strong>. Following RoBERTa, we trained DistilBERT<strong class="od hz"> </strong>on very <strong class="od hz">large batches</strong> leveraging gradient accumulation (up to 4000 examples per batch), with <strong class="od hz">dynamic masking</strong> and <strong class="od hz">removed the next sentence prediction objective</strong>.</p><p id="80be" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">Our training setup is voluntarily limited in terms of resources. We train DistilBERT<strong class="od hz"> </strong>on <strong class="od hz">eight 16GB V100 GPUs for approximately three and a half days </strong>using the concatenation of Toronto Book Corpus and English Wikipedia (same data as original BERT).</p><p id="3193" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">The code for DistilBERT is adapted in part from Facebook <a class="af hv" href="https://github.com/facebookresearch/XLM" rel="noopener ugc nofollow" target="_blank">XLM</a>‚Äôs code and in part from our PyTorch version of Google AI <a class="af hv" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank">Bert</a> and is available in our <a class="af hv" href="https://github.com/huggingface/pytorch-transformers/" rel="noopener ugc nofollow" target="_blank">pytorch-transformers library</a> üëæ along with several trained and fine-tuned versions of DistilBert and the code to reproduce the training and fine-tuning.</p><h1 id="4e84" class="pw px hy bf pf py pz qa qb qc qd qe qf qg qh qi qj qk ql qm qn qo qp qq qr qs bk">üé¢ Model performances ‚Äî Testing <em class="pv">DistilBERT</em></h1><p id="e76e" class="pw-post-body-paragraph oa ob hy od b oe qt og oh oi qu ok ol om qv oo op oq qw os ot ou qx ow ox oy gn bk">We compare the performance of DistilBERT on the development sets of the <a class="af hv" href="https://gluebenchmark.com" rel="noopener ugc nofollow" target="_blank"><strong class="od hz">GLUE benchmark</strong></a><strong class="od hz"> </strong>against two baselines: BERT base (DistilBERT‚Äôs teacher) and a strong non-transformer baseline from NYU: two BiLSTMs on top of ELMo. We use the <a class="af hv" href="https://github.com/nyu-mll/jiant" rel="noopener ugc nofollow" target="_blank">jiant</a> library from NYU for ELMo baselines and <a class="af hv" href="https://github.com/huggingface/pytorch-transformers" rel="noopener ugc nofollow" target="_blank">pytorch-transformers</a> for the BERT baseline.</p><p id="0b3d" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">As shown in the following table, DistilBERT‚Äôs performances <strong class="od hz">compare favorably with the baselines</strong> while having respectively about half and one third the number of parameters (more on this below). Among the 9 tasks, DistilBERT is <strong class="od hz">always on par or improving over the ELMo baseline</strong> (up to 14 points of accuracy on QNLI). DistilBERT also <strong class="od hz">compares surprisingly well to BERT</strong>: we are able to <strong class="od hz">retain more than 95% of the performance</strong> while having 40% fewer parameters.</p></div></div><div class="gs"><div class="ab cb"><div class="gt gu gv gw gx gy cf gz cg ha ci bh"><figure class="pa pb pc pd pe gs hj hk paragraph-image"><div role="button" tabindex="0" class="hl hm fj hn bh ho"><div class="hb hc rp"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*hHwcSZEazpY_PwArgBzHtw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*hHwcSZEazpY_PwArgBzHtw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*hHwcSZEazpY_PwArgBzHtw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*hHwcSZEazpY_PwArgBzHtw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*hHwcSZEazpY_PwArgBzHtw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*hHwcSZEazpY_PwArgBzHtw.png 1100w, https://miro.medium.com/v2/resize:fit:2000/format:webp/1*hHwcSZEazpY_PwArgBzHtw.png 2000w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 1000px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*hHwcSZEazpY_PwArgBzHtw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*hHwcSZEazpY_PwArgBzHtw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*hHwcSZEazpY_PwArgBzHtw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*hHwcSZEazpY_PwArgBzHtw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*hHwcSZEazpY_PwArgBzHtw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*hHwcSZEazpY_PwArgBzHtw.png 1100w, https://miro.medium.com/v2/resize:fit:2000/1*hHwcSZEazpY_PwArgBzHtw.png 2000w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 1000px"/><img alt="" class="bh hp hq c" width="1000" height="85" loading="eager" role="presentation"/></picture></div></div><figcaption class="hr ff hs hb hc ht hu bf b bg z du">Comparison on the dev sets of the GLUE benchmark. ELMo results as reported by the authors. BERT and DistilBERT results are medians of 5 runs with different seeds.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh fz ga gb gc"><p id="c155" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">In terms of inference time, DistilBERT is more than <strong class="od hz">60% faster and smaller </strong>than BERT and<strong class="od hz"> 120% faster and smaller </strong>than ELMo+BiLSTM<strong class="od hz"> üêé</strong></p><figure class="pa pb pc pd pe gs hb hc paragraph-image"><div role="button" tabindex="0" class="hl hm fj hn bh ho"><div class="hb hc rq"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*RLBWful7k50nV9zTJCuZ4Q.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*RLBWful7k50nV9zTJCuZ4Q.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*RLBWful7k50nV9zTJCuZ4Q.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*RLBWful7k50nV9zTJCuZ4Q.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*RLBWful7k50nV9zTJCuZ4Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*RLBWful7k50nV9zTJCuZ4Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RLBWful7k50nV9zTJCuZ4Q.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*RLBWful7k50nV9zTJCuZ4Q.png 640w, https://miro.medium.com/v2/resize:fit:720/1*RLBWful7k50nV9zTJCuZ4Q.png 720w, https://miro.medium.com/v2/resize:fit:750/1*RLBWful7k50nV9zTJCuZ4Q.png 750w, https://miro.medium.com/v2/resize:fit:786/1*RLBWful7k50nV9zTJCuZ4Q.png 786w, https://miro.medium.com/v2/resize:fit:828/1*RLBWful7k50nV9zTJCuZ4Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*RLBWful7k50nV9zTJCuZ4Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*RLBWful7k50nV9zTJCuZ4Q.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" class="bh hp hq c" width="700" height="140" loading="eager" role="presentation"/></picture></div></div></figure><p id="c4b9" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">To further investigate the speed-up/size trade-off of DistilBERT, we compare, in the left table, the number of parameters of each model along with the inference time needed to do a full pass on the STS-B dev set on CPU (using a batch size of 1).</p></div></div></div><div class="ab cb rh ri rj rk" role="separator"><span class="rl by bm rm rn ro"></span><span class="rl by bm rm rn ro"></span><span class="rl by bm rm rn"></span></div><div class="gn go gp gq gr"><div class="ab cb"><div class="ci bh fz ga gb gc"><h1 id="3be2" class="pw px hy bf pf py rr qa qb qc rs qe qf qg rt qi qj qk ru qm qn qo rv qq qr qs bk"><strong class="al">üîÆ Downstream task: Distillation &amp; transfer-learning</strong></h1><p id="427d" class="pw-post-body-paragraph oa ob hy od b oe qt og oh oi qu ok ol om qv oo op oq qw os ot ou qx ow ox oy gn bk">We further study the use of DistilBERT on downstream tasks under efficient inference constraints. We use our compact pre-trained language model by fine-tuning it a classification task. A nice way to actually <strong class="od hz">mix distillation pre-training and transfer-learning</strong>!</p><figure class="pa pb pc pd pe gs hb hc paragraph-image"><div class="hb hc rw"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*BtwpBJvHfdYUHCpt-wLvGw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*BtwpBJvHfdYUHCpt-wLvGw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*BtwpBJvHfdYUHCpt-wLvGw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*BtwpBJvHfdYUHCpt-wLvGw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*BtwpBJvHfdYUHCpt-wLvGw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*BtwpBJvHfdYUHCpt-wLvGw.png 1100w, https://miro.medium.com/v2/resize:fit:1312/format:webp/1*BtwpBJvHfdYUHCpt-wLvGw.png 1312w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 656px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*BtwpBJvHfdYUHCpt-wLvGw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*BtwpBJvHfdYUHCpt-wLvGw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*BtwpBJvHfdYUHCpt-wLvGw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*BtwpBJvHfdYUHCpt-wLvGw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*BtwpBJvHfdYUHCpt-wLvGw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*BtwpBJvHfdYUHCpt-wLvGw.png 1100w, https://miro.medium.com/v2/resize:fit:1312/1*BtwpBJvHfdYUHCpt-wLvGw.png 1312w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 656px"/><img alt="" class="bh hp hq c" width="656" height="292" loading="eager" role="presentation"/></picture></div><figcaption class="hr ff hs hb hc ht hu bf b bg z du">Extract from the IMDB Review dataset ‚Äî Source: <a class="af hv" href="https://www.kaggle.com/desiredewaele/sentiment-analysis-on-imdb-reviews" rel="noopener ugc nofollow" target="_blank">Kaggle</a></figcaption></figure><p id="87aa" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">We selected the <strong class="od hz">IMDB Review Sentiment Classification </strong>which<strong class="od hz"> </strong>is composed of 50&#x27;000 reviews in English labeled as positive or negative: 25&#x27;000 for training and 25&#x27;000 for test (and with balanced classes). We trained on a single 12GB K80.</p><p id="e798" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">First, we train <code class="cx rd re rf rg b">bert-base-uncased</code> on our dataset. Our dear BERT üíã reaches <strong class="od hz">an accuracy of 93.46%</strong> (average of 6 runs) without any hyper-parameters search.</p><p id="f989" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">We then train DistilBERT, using the same hyper-parameters. The compressed model reaches <strong class="od hz">an accuracy of 93.07% </strong>(average of 6runs). An absolute difference of 0.4% in performances for a 60% reduction in latency and 40% in size üèé!</p><blockquote class="nx ny nz"><p id="03d5" class="oa ob oc od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk"><strong class="od hz"><em class="hy">‚ùìNote 3</em></strong> ‚Äî <a class="af hv" href="https://twitter.com/PiotrCzapla/status/1168120760201859072?s=20" rel="noopener ugc nofollow" target="_blank">As noted by the community</a>, you can reach comparable or better score on the IMDB benchmark with lighter methods (size-wise and inference-wise) like ULMFiT. We encourage you to compare on your own use-case! In particular, DistilBERT can give a sensible lower-bound on Bert‚Äôs performances with the advantage of faster training.</p></blockquote><p id="fab0" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk rx"><span class="l ry rz sa bo sb sc sd se sf fj">A</span>nother common application of NLP is <strong class="od hz">Question Answering. </strong>We compared the results of the <code class="cx rd re rf rg b">bert-base-uncased</code> version of BERT with DistilBERT on the SQuAD 1.1 dataset. On the development set, BERT reaches an F1 score of 88.5 and an EM (Exact-match) score of 81.2. We train DistilBERT on the same set of hyper-parameters and reach scores of <em class="oc">85.1 F1 and 76.5 EM, within 3 to 5 points of the full BERT</em>.</p><p id="7fc2" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">We also studied whether we could add another step of distillation during the adaptation phase by finetuning DistilBERT on SQuAD using the finetuned BERT model as a teacher with a knowledge distillation loss.</p><blockquote class="nx ny nz"><p id="f13c" class="oa ob oc od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">Here we are finetuning by distilling a question answering model into a language model previously pre-trained with knowledge distillation! That a lot of teachers and studentsüéì</p></blockquote><p id="6c5a" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">In this case, we were able to reach interesting performances given the size of the network: <strong class="od hz">86.2 F1 and 78.1 EM, ie. within 3 points of the full model!</strong></p><p id="9b1e" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">Other works have also attempted to accelerate question answering models. Notably, <a class="af hv" href="https://twitter.com/DebajyotiChat17" rel="noopener ugc nofollow" target="_blank">Debajyoti Chatterjee</a>, uploaded <a class="af hv" href="https://arxiv.org/abs/1904.00796" rel="noopener ugc nofollow" target="_blank">an interesting work on arXiv</a> which follows a similar method for the adaptation phase on SQuAD (initializing a student from its teacher, and training a question-answering model via distillation). His experiments present similar relative performances with regards to BERT (base uncased). The main difference with our present work is that we pre-train DistilBERT with a general objective (Masked Language Modeling) in order to obtain a model that can be used for transfer-learning on a large range of tasks via finetuning (GLUE, SQuAD, classification‚Ä¶).</p><h1 id="0928" class="pw px hy bf pf py pz qa qb qc qd qe qf qg qh qi qj qk ql qm qn qo qp qq qr qs bk">üôå Less is more: smaller models also spark joy üåü</h1><p id="dcc3" class="pw-post-body-paragraph oa ob hy od b oe qt og oh oi qu ok ol om qv oo op oq qw os ot ou qx ow ox oy gn bk">We are very excited about <strong class="od hz">DistilBERT‚Äôs potential</strong>. The work we‚Äôve presented is just the beginning of what can be done and raises many questions: How far can we compress these models with knowledge distillation? Can these technics be used to get further insights into the knowledge stored in the large version? What aspects of linguistic/semantics do we lose in this type of compression?‚Ä¶</p><p id="cf78" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">One essential aspect of our work at HuggingFace is <strong class="od hz">open-source </strong>and<strong class="od hz"> knowledge sharing</strong> as you can see from our <a class="af hv" href="https://github.com/huggingface" rel="noopener ugc nofollow" target="_blank">GitHub</a> and <a class="af hv" href="http://www.medium.com/huggingface" rel="noopener">medium</a> pages. We think it is both the easiest and fairest way for everyone to participate and reap the fruits of the remarkable progress of deep learning for NLP.</p><p id="3b2a" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk">Thus, together with this blog post, we release the code of our experiments üéÆ (in particular the code to reproduce the training and fine-tuning of DistilBERT) along with a trained version of DistilBERT in our <a class="af hv" href="https://github.com/huggingface/pytorch-transformers/" rel="noopener ugc nofollow" target="_blank">pytorch-transformers library</a>üî•.</p><p id="be4e" class="pw-post-body-paragraph oa ob hy od b oe of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy gn bk"><em class="oc">Many thanks to Sam Bowman, Alex Wang and Thibault F√©vry for feedback and discussions!</em></p></div></div></div><div class="ab cb rh ri rj rk" role="separator"><span class="rl by bm rm rn ro"></span><span class="rl by bm rm rn ro"></span><span class="rl by bm rm rn"></span></div><div class="gn go gp gq gr"><div class="ab cb"><div class="ci bh fz ga gb gc"><figure class="pa pb pc pd pe gs"><div class="sg kj l fj"><div class="sh si l"></div></div></figure></div></div></div></div></section></div></div></article></div><div class="ab cb"><div class="ci bh fz ga gb gc"><div class="sj sk ab ke"><div class="sl ab"><a class="sm ay am ao" rel="noopener follow" href="/tag/machine-learning?source=post_page-----8cf3380435b5--------------------------------"><div class="sn fj cx so ge sp sq bf b bg z bk sr">Machine Learning</div></a></div><div class="sl ab"><a class="sm ay am ao" rel="noopener follow" href="/tag/nlp?source=post_page-----8cf3380435b5--------------------------------"><div class="sn fj cx so ge sp sq bf b bg z bk sr">NLP</div></a></div><div class="sl ab"><a class="sm ay am ao" rel="noopener follow" href="/tag/bert?source=post_page-----8cf3380435b5--------------------------------"><div class="sn fj cx so ge sp sq bf b bg z bk sr">Bert</div></a></div><div class="sl ab"><a class="sm ay am ao" rel="noopener follow" href="/tag/distillation?source=post_page-----8cf3380435b5--------------------------------"><div class="sn fj cx so ge sp sq bf b bg z bk sr">Distillation</div></a></div><div class="sl ab"><a class="sm ay am ao" rel="noopener follow" href="/tag/transformers?source=post_page-----8cf3380435b5--------------------------------"><div class="sn fj cx so ge sp sq bf b bg z bk sr">Transformers</div></a></div></div></div></div><div class="l"></div><footer class="ss ri st su sv ab q sw jo c"><div class="l ae"><div class="ab cb"><div class="ci bh fz ga gb gc"><div class="ab cp sx"><div class="ab q lz"><div class="sy l"><span class="l sz ta tb e d"><div class="ab q lz ma"><div class="pw-multi-vote-icon fj ki mb mc md"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="footerClapButton" rel="noopener follow" href="/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fhuggingface%2F8cf3380435b5&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fhuggingface%2Fdistilbert-8cf3380435b5&amp;user=Victor+Sanh&amp;userId=ac59742e5349&amp;source=---footer_actions--8cf3380435b5---------------------clap_footer-----------"><div><div class="bm" aria-hidden="false"><div class="me ao mf mg mh mi am mj mk ml md"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM13.916 3.953l1.523-2.112-1.184-.39zM8.589 1.84l1.522 2.112-.337-2.501zM18.523 18.92c-.86.86-1.75 1.246-2.62 1.33a6 6 0 0 0 .407-.372c2.388-2.389 2.86-4.951 1.399-7.623l-.912-1.603-.79-1.672c-.26-.56-.194-.98.203-1.288a.7.7 0 0 1 .546-.132c.283.046.546.231.728.5l2.363 4.157c.976 1.624 1.141 4.237-1.324 6.702m-10.999-.438L3.37 14.328a.828.828 0 0 1 .585-1.408.83.83 0 0 1 .585.242l2.158 2.157a.365.365 0 0 0 .516-.516l-2.157-2.158-1.449-1.449a.826.826 0 0 1 1.167-1.17l3.438 3.44a.363.363 0 0 0 .516 0 .364.364 0 0 0 0-.516L5.293 9.513l-.97-.97a.826.826 0 0 1 0-1.166.84.84 0 0 1 1.167 0l.97.968 3.437 3.436a.36.36 0 0 0 .517 0 .366.366 0 0 0 0-.516L6.977 7.83a.82.82 0 0 1-.241-.584.82.82 0 0 1 .824-.826c.219 0 .43.087.584.242l5.787 5.787a.366.366 0 0 0 .587-.415l-1.117-2.363c-.26-.56-.194-.98.204-1.289a.7.7 0 0 1 .546-.132c.283.046.545.232.727.501l2.193 3.86c1.302 2.38.883 4.59-1.277 6.75-1.156 1.156-2.602 1.627-4.19 1.367-1.418-.236-2.866-1.033-4.079-2.246M10.75 5.971l2.12 2.12c-.41.502-.465 1.17-.128 1.89l.22.465-3.523-3.523a.8.8 0 0 1-.097-.368c0-.22.086-.428.241-.584a.847.847 0 0 1 1.167 0m7.355 1.705c-.31-.461-.746-.758-1.23-.837a1.44 1.44 0 0 0-1.11.275c-.312.24-.505.543-.59.881a1.74 1.74 0 0 0-.906-.465 1.47 1.47 0 0 0-.82.106l-2.182-2.182a1.56 1.56 0 0 0-2.2 0 1.54 1.54 0 0 0-.396.701 1.56 1.56 0 0 0-2.21-.01 1.55 1.55 0 0 0-.416.753c-.624-.624-1.649-.624-2.237-.037a1.557 1.557 0 0 0 0 2.2c-.239.1-.501.238-.715.453a1.56 1.56 0 0 0 0 2.2l.516.515a1.556 1.556 0 0 0-.753 2.615L7.01 19c1.32 1.319 2.909 2.189 4.475 2.449q.482.08.971.08c.85 0 1.653-.198 2.393-.579.231.033.46.054.686.054 1.266 0 2.457-.52 3.505-1.567 2.763-2.763 2.552-5.734 1.439-7.586z" clip-rule="evenodd"></path></svg></div></div></div></a></span></div><div class="pw-multi-vote-count l mm mn mo mp mq mr ms"><p class="bf b dv z du"><span class="mt">--</span></p></div></div></span><span class="l h g f tc td"><div class="ab q lz ma"><div class="pw-multi-vote-icon fj ki mb mc md"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="footerClapButton" rel="noopener follow" href="/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fhuggingface%2F8cf3380435b5&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fhuggingface%2Fdistilbert-8cf3380435b5&amp;user=Victor+Sanh&amp;userId=ac59742e5349&amp;source=---footer_actions--8cf3380435b5---------------------clap_footer-----------"><div><div class="bm" aria-hidden="false"><div class="me ao mf mg mh mi am mj mk ml md"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM13.916 3.953l1.523-2.112-1.184-.39zM8.589 1.84l1.522 2.112-.337-2.501zM18.523 18.92c-.86.86-1.75 1.246-2.62 1.33a6 6 0 0 0 .407-.372c2.388-2.389 2.86-4.951 1.399-7.623l-.912-1.603-.79-1.672c-.26-.56-.194-.98.203-1.288a.7.7 0 0 1 .546-.132c.283.046.546.231.728.5l2.363 4.157c.976 1.624 1.141 4.237-1.324 6.702m-10.999-.438L3.37 14.328a.828.828 0 0 1 .585-1.408.83.83 0 0 1 .585.242l2.158 2.157a.365.365 0 0 0 .516-.516l-2.157-2.158-1.449-1.449a.826.826 0 0 1 1.167-1.17l3.438 3.44a.363.363 0 0 0 .516 0 .364.364 0 0 0 0-.516L5.293 9.513l-.97-.97a.826.826 0 0 1 0-1.166.84.84 0 0 1 1.167 0l.97.968 3.437 3.436a.36.36 0 0 0 .517 0 .366.366 0 0 0 0-.516L6.977 7.83a.82.82 0 0 1-.241-.584.82.82 0 0 1 .824-.826c.219 0 .43.087.584.242l5.787 5.787a.366.366 0 0 0 .587-.415l-1.117-2.363c-.26-.56-.194-.98.204-1.289a.7.7 0 0 1 .546-.132c.283.046.545.232.727.501l2.193 3.86c1.302 2.38.883 4.59-1.277 6.75-1.156 1.156-2.602 1.627-4.19 1.367-1.418-.236-2.866-1.033-4.079-2.246M10.75 5.971l2.12 2.12c-.41.502-.465 1.17-.128 1.89l.22.465-3.523-3.523a.8.8 0 0 1-.097-.368c0-.22.086-.428.241-.584a.847.847 0 0 1 1.167 0m7.355 1.705c-.31-.461-.746-.758-1.23-.837a1.44 1.44 0 0 0-1.11.275c-.312.24-.505.543-.59.881a1.74 1.74 0 0 0-.906-.465 1.47 1.47 0 0 0-.82.106l-2.182-2.182a1.56 1.56 0 0 0-2.2 0 1.54 1.54 0 0 0-.396.701 1.56 1.56 0 0 0-2.21-.01 1.55 1.55 0 0 0-.416.753c-.624-.624-1.649-.624-2.237-.037a1.557 1.557 0 0 0 0 2.2c-.239.1-.501.238-.715.453a1.56 1.56 0 0 0 0 2.2l.516.515a1.556 1.556 0 0 0-.753 2.615L7.01 19c1.32 1.319 2.909 2.189 4.475 2.449q.482.08.971.08c.85 0 1.653-.198 2.393-.579.231.033.46.054.686.054 1.266 0 2.457-.52 3.505-1.567 2.763-2.763 2.552-5.734 1.439-7.586z" clip-rule="evenodd"></path></svg></div></div></div></a></span></div><div class="pw-multi-vote-count l mm mn mo mp mq mr ms"><p class="bf b dv z du"><span class="mt">--</span></p></div></div></span></div><div class="bq ab"><div><div class="bm" aria-hidden="false"><button class="ao me mw mx ab q fk my mz" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="mv"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"></path></svg><p class="bf b bg z du"><span class="pw-responses-count mu mv">20</span></p></button></div></div></div></div><div class="ab q"><div class="ro l kb"><div><div class="bm" aria-hidden="false"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="footerBookmarkButton" rel="noopener follow" href="/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8cf3380435b5&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fhuggingface%2Fdistilbert-8cf3380435b5&amp;source=---footer_actions--8cf3380435b5---------------------bookmark_footer-----------"><svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" fill="none" viewBox="0 0 25 25" class="du nb" aria-label="Add to list bookmark button"><path fill="currentColor" d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .805.396L12.5 17l5.695 4.396A.5.5 0 0 0 19 21v-8.5a.5.5 0 0 0-1 0v7.485l-5.195-4.012a.5.5 0 0 0-.61 0L7 19.985z"></path></svg></a></span></div></div></div><div class="ro l kb"><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="footerSocialShareButton" class="af fk ah ai aj ak al nf an ao ap ex ng nh mz ni"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div></footer><div class="te tf tg th ti l"><div class="ab cb"><div class="ci bh fz ga gb gc"><div class="tj bh r tk"></div><div class="tl l"><div class="ab tm tn to kd kc"><div class="tp tq tr ts tt tu tv tw tx ty ab cp"><div class="h k"><a href="https://medium.com/huggingface?source=post_page---post_publication_info--8cf3380435b5--------------------------------" rel="noopener follow"><div class="fj ab"><img alt="HuggingFace" class="tz jf jg cx" src="https://miro.medium.com/v2/resize:fill:96:96/1*ABKawA7BD68tMpncHfAo_Q@2x.png" width="48" height="48" loading="lazy"/><div class="tz l jg jf fs n fr ua"></div></div></a></div><div class="j i d"><a href="https://medium.com/huggingface?source=post_page---post_publication_info--8cf3380435b5--------------------------------" rel="noopener follow"><div class="fj ab"><img alt="HuggingFace" class="tz uc ub cx" src="https://miro.medium.com/v2/resize:fill:128:128/1*ABKawA7BD68tMpncHfAo_Q@2x.png" width="64" height="64" loading="lazy"/><div class="tz l ub uc fs n fr ua"></div></div></a></div><div class="j i d ud kb"><div class="ab"></div></div></div><div class="ab co ue"><div class="uf ug uh gw gu l"><a class="af ag ah aj ak al am an ao ap aq ar as at ab q" href="https://medium.com/huggingface?source=post_page---post_publication_info--8cf3380435b5--------------------------------" rel="noopener follow"><h2 class="pw-author-name bf uj uk ul um un uo up om uq ur oq us ut ou uu uv bk"><span class="gn ui">Published in <!-- -->HuggingFace</span></h2></a><div class="sl ab je"><div class="l kb"><span class="pw-follower-count bf b bg z du"><a class="af ag ah ai aj ak al am an ao ap aq ar ju" rel="noopener follow" href="/huggingface/followers?source=post_page---post_publication_info--8cf3380435b5--------------------------------">8.3K Followers</a></span></div><div class="bf b bg z du ab kh"><span class="jv l" aria-hidden="true"><span class="bf b bg z du">¬∑</span></span><a class="af ag ah ai aj ak al am an ao ap aq ar ju" rel="noopener follow" href="/huggingface/simple-considerations-for-simple-people-building-fancy-neural-networks-7abc3c0f0bd7?source=post_page---post_publication_info--8cf3380435b5--------------------------------">Last published¬†<span>Sep 22, 2020</span></a></div></div><div class="uw l"><p class="bf b bg z bk"><span class="gn">Stories @ Hugging Face</span></p></div></div></div><div class="h k"><div class="ab"></div></div></div></div><div class="ab tm tn to kd kc"><div class="tp tq tr ts tt tu tv tw tx ty ab cp"><div class="h k"><a tabindex="0" rel="noopener follow" href="/@victorsanh?source=post_page---post_author_info--8cf3380435b5--------------------------------"><div class="l fj"><img alt="Victor Sanh" class="l fd by jg jf cx" src="https://miro.medium.com/v2/resize:fill:96:96/1*3AEtg8Ma0mqLlIwaF8Swow@2x.jpeg" width="48" height="48" loading="lazy"/><div class="fr by l jg jf fs n ay ua"></div></div></a></div><div class="j i d"><a tabindex="0" rel="noopener follow" href="/@victorsanh?source=post_page---post_author_info--8cf3380435b5--------------------------------"><div class="l fj"><img alt="Victor Sanh" class="l fd by ub uc cx" src="https://miro.medium.com/v2/resize:fill:128:128/1*3AEtg8Ma0mqLlIwaF8Swow@2x.jpeg" width="64" height="64" loading="lazy"/><div class="fr by l ub uc fs n ay ua"></div></div></a></div><div class="j i d ud kb"><div class="ab"><span><button class="bf b bg z ux sn uy uz va vb vc ev ew vd ve vf fa fb fc fd bm fe ff">Follow</button></span></div></div></div><div class="ab co ue"><div class="uf ug uh gw gu l"><a class="af ag ah aj ak al am an ao ap aq ar as at ab q" rel="noopener follow" href="/@victorsanh?source=post_page---post_author_info--8cf3380435b5--------------------------------"><h2 class="pw-author-name bf uj uk ul um un uo up om uq ur oq us ut ou uu uv bk"><span class="gn ui">Written by <!-- -->Victor Sanh</span></h2></a><div class="sl ab je"><div class="l kb"><span class="pw-follower-count bf b bg z du"><a class="af ag ah ai aj ak al am an ao ap aq ar ju" rel="noopener follow" href="/@victorsanh/followers?source=post_page---post_author_info--8cf3380435b5--------------------------------">1.1K Followers</a></span></div><div class="bf b bg z du ab kh"><span class="jv l" aria-hidden="true"><span class="bf b bg z du">¬∑</span></span><a class="af ag ah ai aj ak al am an ao ap aq ar ju" rel="noopener follow" href="/@victorsanh/following?source=post_page---post_author_info--8cf3380435b5--------------------------------">9 Following</a></div></div><div class="uw l"><p class="bf b bg z bk"><span class="gn">Dog sitter by day, Scientist at @huggingface ü§ó by night | Into Natural Language Processing, started with Computer Vision</span></p></div></div></div><div class="h k"><div class="ab"><span><button class="bf b bg z ux sn uy uz va vb vc ev ew vd ve vf fa fb fc fd bm fe ff">Follow</button></span></div></div></div></div></div></div><div class="vg vh vi vj vk l"><div class="tj bh r vg vh vl vm vn"></div><div class="ab cb"><div class="ci bh fz ga gb gc"><div class="ab q cp"><h2 class="bf uj py qa qb qc qe qf qg qi qj qk qm qn qo qq qr bk">Responses (<!-- -->20<!-- -->)</h2><div class="ab vo"><div><div class="bm" aria-hidden="false"><a class="vp vq" href="https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--8cf3380435b5--------------------------------" rel="noopener follow" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" viewBox="0 0 25 25"><path fill-rule="evenodd" d="M11.987 5.036a.754.754 0 0 1 .914-.01c.972.721 1.767 1.218 2.6 1.543.828.322 1.719.485 2.887.505a.755.755 0 0 1 .741.757c-.018 3.623-.43 6.256-1.449 8.21-1.034 1.984-2.662 3.209-4.966 4.083a.75.75 0 0 1-.537-.003c-2.243-.874-3.858-2.095-4.897-4.074-1.024-1.951-1.457-4.583-1.476-8.216a.755.755 0 0 1 .741-.757c1.195-.02 2.1-.182 2.923-.503.827-.322 1.6-.815 2.519-1.535m.468.903c-.897.69-1.717 1.21-2.623 1.564-.898.35-1.856.527-3.026.565.037 3.45.469 5.817 1.36 7.515.884 1.684 2.25 2.762 4.284 3.571 2.092-.81 3.465-1.89 4.344-3.575.886-1.698 1.299-4.065 1.334-7.512-1.149-.039-2.091-.217-2.99-.567-.906-.353-1.745-.873-2.683-1.561m-.009 9.155a2.672 2.672 0 1 0 0-5.344 2.672 2.672 0 0 0 0 5.344m0 1a3.672 3.672 0 1 0 0-7.344 3.672 3.672 0 0 0 0 7.344m-1.813-3.777.525-.526.916.917 1.623-1.625.526.526-2.149 2.152z" clip-rule="evenodd"></path></svg></a></div></div></div></div><div class="rh l"><button class="bf b bg z bk sn vr vs vt nb my vc ev ew ex vu vv vw fa vx vy vz wa wb fb fc fd bm fe ff">See all responses</button></div></div></div></div><div class="wc wd we wf wg l bx"><div class="h k j"><div class="tj bh wh wi"></div><div class="ab cb"><div class="ci bh fz ga gb gc"><div class="wj ab lz ke"><div class="wk wl l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://help.medium.com/hc/en-us?source=post_page-----8cf3380435b5--------------------------------" rel="noopener follow"><p class="bf b dv z du">Help</p></a></div><div class="wk wl l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.statuspage.io/?source=post_page-----8cf3380435b5--------------------------------" rel="noopener follow"><p class="bf b dv z du">Status</p></a></div><div class="wk wl l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="/about?autoplay=1&amp;source=post_page-----8cf3380435b5--------------------------------"><p class="bf b dv z du">About</p></a></div><div class="wk wl l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----8cf3380435b5--------------------------------"><p class="bf b dv z du">Careers</p></a></div><div class="wk wl l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="pressinquiries@medium.com?source=post_page-----8cf3380435b5--------------------------------" rel="noopener follow"><p class="bf b dv z du">Press</p></a></div><div class="wk wl l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://blog.medium.com/?source=post_page-----8cf3380435b5--------------------------------" rel="noopener follow"><p class="bf b dv z du">Blog</p></a></div><div class="wk wl l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8cf3380435b5--------------------------------" rel="noopener follow"><p class="bf b dv z du">Privacy</p></a></div><div class="wk wl l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8cf3380435b5--------------------------------" rel="noopener follow"><p class="bf b dv z du">Terms</p></a></div><div class="wk wl l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://speechify.com/medium?source=post_page-----8cf3380435b5--------------------------------" rel="noopener follow"><p class="bf b dv z du">Text to speech</p></a></div><div class="wk l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="/business?source=post_page-----8cf3380435b5--------------------------------"><p class="bf b dv z du">Teams</p></a></div></div></div></div></div></div></div></div></div></div><script>window.__BUILD_ID__="main-20241203-152048-8e3bb7adcf"</script><script>window.__GRAPHQL_URI__ = "https://medium.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"algolia":{"queries":{}},"cache":{"experimentGroupSet":true,"reason":"","group":"enabled","tags":["group-edgeCachePosts","post-8cf3380435b5","user-ac59742e5349","collection-ba0dbdd23ac6"],"serverVariantState":"44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a","middlewareEnabled":true,"cacheStatus":"DYNAMIC","shouldUseCache":true,"vary":[],"lohpSummerUpsellEnabled":false,"publicationHierarchyEnabledWeb":false,"postBottomResponsesEnabled":false},"client":{"hydrated":false,"isUs":false,"isNativeMedium":false,"isSafariMobile":false,"isSafari":false,"isFirefox":false,"routingEntity":{"type":"DEFAULT","explicit":false},"viewerIsBot":false},"debug":{"requestId":"7dc7048d-b44b-43d8-ae62-7f9efe047869","hybridDevServices":[],"originalSpanCarrier":{"traceparent":"00-dbd4233971b33ee55c25609a22478f8e-26e7fdd6455605a4-01"}},"multiVote":{"clapsPerPost":{}},"navigation":{"branch":{"show":null,"hasRendered":null,"blockedByCTA":false},"hideGoogleOneTap":false,"hasRenderedAlternateUserBanner":null,"currentLocation":"https:\u002F\u002Fmedium.com\u002Fhuggingface\u002Fdistilbert-8cf3380435b5","host":"medium.com","hostname":"medium.com","referrer":"","hasSetReferrer":false,"susiModal":{"step":null,"operation":"register"},"postRead":false,"partnerProgram":{"selectedCountryCode":null},"queryString":"","currentHash":""},"config":{"nodeEnv":"production","version":"main-20241203-152048-8e3bb7adcf","target":"production","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","recaptcha3Key":"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5","recaptchaEnterpriseKeyId":"6Le-uGgpAAAAAPprRaokM8AKthQ9KNGdoxaGUvVp","datadog":{"applicationId":"6702d87d-a7e0-42fe-bbcb-95b469547ea0","clientToken":"pub853ea8d17ad6821d9f8f11861d23dfed","rumToken":"pubf9cc52896502b9413b68ba36fc0c7162","context":{"deployment":{"target":"production","tag":"main-20241203-152048-8e3bb7adcf","commit":"8e3bb7adcfacbe91364f4ad68b2f03bbb2b50a7c"}},"datacenter":"us"},"googleAnalyticsCode":"G-7JY7T788PK","googlePay":{"apiVersion":"2","apiVersionMinor":"0","merchantId":"BCR2DN6TV7EMTGBM","merchantName":"Medium","instanceMerchantId":"13685562959212738550"},"applePay":{"version":3},"signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumMastodonDomainName":"me.dm","mediumOwnedAndOperatedCollectionIds":["8a9336e5bb4","b7e45b22fec3","193b68bd4fba","8d6b8a439e32","54c98c43354d","3f6ecf56618","d944778ce714","92d2092dc598","ae2a65f35510","1285ba81cada","544c7006046e","fc8964313712","40187e704f1c","88d9857e584e","7b6769f2748b","bcc38c8f6edf","cef6983b292","cb8577c9149e","444d13b52878","713d7dbc99b0","ef8e90590e66","191186aaafa0","55760f21cdc5","9dc80918cc93","bdc4052bbdba","8ccfed20cbb2"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"topicsToFollow":["d61cf867d93f","8a146bc21b28","1eca0103fff3","4d562ee63426","aef1078a3ef5","e15e46793f8d","6158eb913466","55f1c20aba7a","3d18b94f6858","4861fee224fd","63c6f1f93ee","1d98b3a9a871","decb52b64abf","ae5d4995e225","830cded25262"],"topicToTagMappings":{"accessibility":"accessibility","addiction":"addiction","android-development":"android-development","art":"art","artificial-intelligence":"artificial-intelligence","astrology":"astrology","basic-income":"basic-income","beauty":"beauty","biotech":"biotech","blockchain":"blockchain","books":"books","business":"business","cannabis":"cannabis","cities":"cities","climate-change":"climate-change","comics":"comics","coronavirus":"coronavirus","creativity":"creativity","cryptocurrency":"cryptocurrency","culture":"culture","cybersecurity":"cybersecurity","data-science":"data-science","design":"design","digital-life":"digital-life","disability":"disability","economy":"economy","education":"education","equality":"equality","family":"family","feminism":"feminism","fiction":"fiction","film":"film","fitness":"fitness","food":"food","freelancing":"freelancing","future":"future","gadgets":"gadgets","gaming":"gaming","gun-control":"gun-control","health":"health","history":"history","humor":"humor","immigration":"immigration","ios-development":"ios-development","javascript":"javascript","justice":"justice","language":"language","leadership":"leadership","lgbtqia":"lgbtqia","lifestyle":"lifestyle","machine-learning":"machine-learning","makers":"makers","marketing":"marketing","math":"math","media":"media","mental-health":"mental-health","mindfulness":"mindfulness","money":"money","music":"music","neuroscience":"neuroscience","nonfiction":"nonfiction","outdoors":"outdoors","parenting":"parenting","pets":"pets","philosophy":"philosophy","photography":"photography","podcasts":"podcast","poetry":"poetry","politics":"politics","privacy":"privacy","product-management":"product-management","productivity":"productivity","programming":"programming","psychedelics":"psychedelics","psychology":"psychology","race":"race","relationships":"relationships","religion":"religion","remote-work":"remote-work","san-francisco":"san-francisco","science":"science","self":"self","self-driving-cars":"self-driving-cars","sexuality":"sexuality","social-media":"social-media","society":"society","software-engineering":"software-engineering","space":"space","spirituality":"spirituality","sports":"sports","startups":"startup","style":"style","technology":"technology","transportation":"transportation","travel":"travel","true-crime":"true-crime","tv":"tv","ux":"ux","venture-capital":"venture-capital","visual-design":"visual-design","work":"work","world":"world","writing":"writing"},"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"7*V1_7XP4snlmqrc_0Njontw.png","height":110,"width":500},"postLogo":{"imageId":"bd978bb536350a710e8efb012513429cabdc4c28700604261aeda246d0f980b7","height":810,"width":1440},"postPreviewImage":{"imageId":"1*hn4v1tCaJy7cWMyb0bpNpQ.png","height":386,"width":579}},"collectionStructuredData":{"8d6b8a439e32":{"name":"Elemental","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F980\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png","width":980,"height":159}}},"3f6ecf56618":{"name":"Forge","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F596\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png","width":596,"height":183}}},"ae2a65f35510":{"name":"GEN","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F264\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png","width":264,"height":140}}},"88d9857e584e":{"name":"LEVEL","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png","width":540,"height":108}}},"7b6769f2748b":{"name":"Marker","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F383\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png","width":383,"height":92}}},"444d13b52878":{"name":"OneZero","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*cw32fIqCbRWzwJaoQw6BUg.png","width":540,"height":123}}},"8ccfed20cbb2":{"name":"Zora","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png","width":540,"height":106}}}},"embeddedPostIds":{"coronavirus":"cd3010f9d81f"},"sharedCdcMessaging":{"COVID_APPLICABLE_TAG_SLUGS":[],"COVID_APPLICABLE_TOPIC_NAMES":[],"COVID_APPLICABLE_TOPIC_NAMES_FOR_TOPIC_PAGE":[],"COVID_MESSAGES":{"tierA":{"text":"For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":66,"end":73,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"tierB":{"text":"Anyone can publish on Medium per our Policies, but we don‚Äôt fact-check every story. For more info about the coronavirus, see cdc.gov.","markups":[{"start":37,"end":45,"href":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Fcategories\u002F201931128-Policies-Safety"},{"start":125,"end":132,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"paywall":{"text":"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":56,"end":70,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":138,"end":145,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"unbound":{"text":"This article is free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":45,"end":59,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":127,"end":134,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]}},"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST":["3b31a67bff4a"]},"sharedVoteMessaging":{"TAGS":["politics","election-2020","government","us-politics","election","2020-presidential-race","trump","donald-trump","democrats","republicans","congress","republican-party","democratic-party","biden","joe-biden","maga"],"TOPICS":["politics","election"],"MESSAGE":{"text":"Find out more about the U.S. election results here.","markups":[{"start":46,"end":50,"href":"https:\u002F\u002Fcookpolitical.com\u002F2020-national-popular-vote-tracker"}]},"EXCLUDE_POSTS":["397ef29e3ca5"]},"embedPostRules":[],"recircOptions":{"v1":{"limit":3},"v2":{"limit":8}},"braintreeClientKey":"production_zjkj96jm_m56f8fqpf7ngnrd4","braintree":{"enabled":true,"merchantId":"m56f8fqpf7ngnrd4","merchantAccountId":{"usd":"AMediumCorporation_instant","eur":"amediumcorporation_EUR","cad":"amediumcorporation_CAD"},"publicKey":"ds2nn34bg2z7j5gd","braintreeEnvironment":"production","dashboardUrl":"https:\u002F\u002Fwww.braintreegateway.com\u002Fmerchants","gracePeriodDurationInDays":14,"mediumMembershipPlanId":{"monthly":"ce105f8c57a3","monthlyV2":"e8a5e126-792b-4ee6-8fba-d574c1b02fc5","monthlyWithTrial":"d5ee3dbe3db8","monthlyPremium":"fa741a9b47a2","yearly":"a40ad4a43185","yearlyV2":"3815d7d6-b8ca-4224-9b8c-182f9047866e","yearlyStaff":"d74fb811198a","yearlyWithTrial":"b3bc7350e5c7","yearlyPremium":"e21bd2c12166","monthlyOneYearFree":"e6c0637a-2bad-4171-ab4f-3c268633d83c","monthly25PercentOffFirstYear":"235ecc62-0cdb-49ae-9378-726cd21c504b","monthly20PercentOffFirstYear":"ba518864-9c13-4a99-91ca-411bf0cac756","monthly15PercentOffFirstYear":"594c029b-9f89-43d5-88f8-8173af4e070e","monthly10PercentOffFirstYear":"c6c7bc9a-40f2-4b51-8126-e28511d5bdb0","monthlyForStudents":"629ebe51-da7d-41fd-8293-34cd2f2030a8","yearlyOneYearFree":"78ba7be9-0d9f-4ece-aa3e-b54b826f2bf1","yearly25PercentOffFirstYear":"2dbb010d-bb8f-4eeb-ad5c-a08509f42d34","yearly20PercentOffFirstYear":"47565488-435b-47f8-bf93-40d5fbe0ebc8","yearly15PercentOffFirstYear":"8259809b-0881-47d9-acf7-6c001c7f720f","yearly10PercentOffFirstYear":"9dd694fb-96e1-472c-8d9e-3c868d5c1506","yearlyForStudents":"e29345ef-ab1c-4234-95c5-70e50fe6bc23","monthlyCad":"p52orjkaceei","yearlyCad":"h4q9g2up9ktt"},"braintreeDiscountId":{"oneMonthFree":"MONTHS_FREE_01","threeMonthsFree":"MONTHS_FREE_03","sixMonthsFree":"MONTHS_FREE_06","fiftyPercentOffOneYear":"FIFTY_PERCENT_OFF_ONE_YEAR"},"3DSecureVersion":"2","defaultCurrency":"usd","providerPlanIdCurrency":{"4ycw":"usd","rz3b":"usd","3kqm":"usd","jzw6":"usd","c2q2":"usd","nnsw":"usd","q8qw":"usd","d9y6":"usd","fx7w":"cad","nwf2":"cad"}},"paypalClientId":"AXj1G4fotC2GE8KzWX9mSxCH1wmPE3nJglf4Z2ig_amnhvlMVX87otaq58niAg9iuLktVNF_1WCMnN7v","paypal":{"host":"https:\u002F\u002Fapi.paypal.com:443","clientMode":"production","serverMode":"live","webhookId":"4G466076A0294510S","monthlyPlan":{"planId":"P-9WR0658853113943TMU5FDQA","name":"Medium Membership (Monthly) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"yearlyPlan":{"planId":"P-7N8963881P8875835MU5JOPQ","name":"Medium Membership (Annual) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"oneYearGift":{"name":"Medium Membership (1 Year, Digital Gift Code)","description":"Unlimited access to the best and brightest stories on Medium. Gift codes can be redeemed at medium.com\u002Fredeem.","price":"50.00","currency":"USD","sku":"membership-gift-1-yr"},"oldMonthlyPlan":{"planId":"P-96U02458LM656772MJZUVH2Y","name":"Medium Membership (Monthly)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"oldYearlyPlan":{"planId":"P-59P80963JF186412JJZU3SMI","name":"Medium Membership (Annual)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"monthlyPlanWithTrial":{"planId":"P-66C21969LR178604GJPVKUKY","name":"Medium Membership (Monthly) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"yearlyPlanWithTrial":{"planId":"P-6XW32684EX226940VKCT2MFA","name":"Medium Membership (Annual) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"oldMonthlyPlanNoSetupFee":{"planId":"P-4N046520HR188054PCJC7LJI","name":"Medium Membership (Monthly)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"oldYearlyPlanNoSetupFee":{"planId":"P-7A4913502Y5181304CJEJMXQ","name":"Medium Membership (Annual)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"sdkUrl":"https:\u002F\u002Fwww.paypal.com\u002Fsdk\u002Fjs"},"stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl","log":{"json":true,"level":"info"},"imageUploadMaxSizeMb":25,"staffPicks":{"title":"Staff Picks","catalogId":"c7bc6e1ee00f"}},"session":{"xsrf":""}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY":{"__typename":"Query","viewer":null,"collectionByDomainOrSlug({\"domainOrSlug\":\"huggingface\"})":{"__ref":"Collection:ba0dbdd23ac6"},"postResult({\"id\":\"8cf3380435b5\"})":{"__ref":"Post:8cf3380435b5"}},"ImageMetadata:":{"__typename":"ImageMetadata","id":""},"Collection:ba0dbdd23ac6":{"__typename":"Collection","id":"ba0dbdd23ac6","favicon":{"__ref":"ImageMetadata:"},"customStyleSheet":null,"colorPalette":{"__typename":"ColorPalette","highlightSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FFECF1FF","point":0},{"__typename":"ColorPoint","color":"#FFE9EFFF","point":0.1},{"__typename":"ColorPoint","color":"#FFE5EDFF","point":0.2},{"__typename":"ColorPoint","color":"#FFE2EBFF","point":0.3},{"__typename":"ColorPoint","color":"#FFDEE9FF","point":0.4},{"__typename":"ColorPoint","color":"#FFDAE7FF","point":0.5},{"__typename":"ColorPoint","color":"#FFD7E4FF","point":0.6},{"__typename":"ColorPoint","color":"#FFD3E2FF","point":0.7},{"__typename":"ColorPoint","color":"#FFD0E0FF","point":0.8},{"__typename":"ColorPoint","color":"#FFCCDDFF","point":0.9},{"__typename":"ColorPoint","color":"#FFC8DAFF","point":1}]},"defaultBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FF6C78FF","point":0},{"__typename":"ColorPoint","color":"#FF6470EE","point":0.1},{"__typename":"ColorPoint","color":"#FF5D69D7","point":0.2},{"__typename":"ColorPoint","color":"#FF5661C1","point":0.3},{"__typename":"ColorPoint","color":"#FF4E58AB","point":0.4},{"__typename":"ColorPoint","color":"#FF464F95","point":0.5},{"__typename":"ColorPoint","color":"#FF3E4680","point":0.6},{"__typename":"ColorPoint","color":"#FF353C6B","point":0.7},{"__typename":"ColorPoint","color":"#FF2B3156","point":0.8},{"__typename":"ColorPoint","color":"#FF212641","point":0.9},{"__typename":"ColorPoint","color":"#FF16192C","point":1}]},"tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF6671FF","colorPoints":[{"__typename":"ColorPoint","color":"#FF6671FF","point":0},{"__typename":"ColorPoint","color":"#FF7583FF","point":0.1},{"__typename":"ColorPoint","color":"#FF8493FF","point":0.2},{"__typename":"ColorPoint","color":"#FF93A2FF","point":0.3},{"__typename":"ColorPoint","color":"#FFA2B1FF","point":0.4},{"__typename":"ColorPoint","color":"#FFB1BEFF","point":0.5},{"__typename":"ColorPoint","color":"#FFC0CCFF","point":0.6},{"__typename":"ColorPoint","color":"#FFCFD9FF","point":0.7},{"__typename":"ColorPoint","color":"#FFDDE5FF","point":0.8},{"__typename":"ColorPoint","color":"#FFECF2FF","point":0.9},{"__typename":"ColorPoint","color":"#FFFAFEFF","point":1}]}},"domain":null,"slug":"huggingface","googleAnalyticsId":null,"editors":[{"__typename":"CollectionMastheadUserItem","user":{"__ref":"User:1291f145e92b"}},{"__typename":"CollectionMastheadUserItem","user":{"__ref":"User:532eb11694d3"}},{"__typename":"CollectionMastheadUserItem","user":{"__ref":"User:167597463903"}},{"__typename":"CollectionMastheadUserItem","user":{"__ref":"User:44db9513d0fd"}},{"__typename":"CollectionMastheadUserItem","user":{"__ref":"User:faa2fb35ef9"}},{"__typename":"CollectionMastheadUserItem","user":{"__ref":"User:ac59742e5349"}},{"__typename":"CollectionMastheadUserItem","user":{"__ref":"User:bae05483be6d"}},{"__typename":"CollectionMastheadUserItem","user":{"__ref":"User:6b82043c70c6"}},{"__typename":"CollectionMastheadUserItem","user":{"__ref":"User:b1574f0c6c5e"}},{"__typename":"CollectionMastheadUserItem","user":{"__ref":"User:bb5da8a1d5ab"}},{"__typename":"CollectionMastheadUserItem","user":{"__ref":"User:a78169e66dc7"}},{"__typename":"CollectionMastheadUserItem","user":{"__ref":"User:a476c06173e0"}},{"__typename":"CollectionMastheadUserItem","user":{"__ref":"User:bac18b20e254"}}],"name":"HuggingFace","avatar":{"__ref":"ImageMetadata:1*ABKawA7BD68tMpncHfAo_Q@2x.png"},"description":"Stories @ Hugging Face","subscriberCount":8305,"latestPostsConnection({\"paging\":{\"limit\":1}})":{"__typename":"PostConnection","posts":[{"__ref":"Post:7abc3c0f0bd7"}]},"viewerEdge":{"__ref":"CollectionViewerEdge:collectionId:ba0dbdd23ac6-viewerId:lo_e0d1c7203e51"},"twitterUsername":"huggingface","facebookPageId":null,"logo":{"__ref":"ImageMetadata:1*NNALWVaZvBmLBZXPqRmXBQ.png"}},"User:1291f145e92b":{"__typename":"User","id":"1291f145e92b"},"User:532eb11694d3":{"__typename":"User","id":"532eb11694d3"},"User:167597463903":{"__typename":"User","id":"167597463903"},"User:44db9513d0fd":{"__typename":"User","id":"44db9513d0fd"},"User:faa2fb35ef9":{"__typename":"User","id":"faa2fb35ef9"},"User:ac59742e5349":{"__typename":"User","id":"ac59742e5349","customDomainState":null,"hasSubdomain":false,"username":"victorsanh","name":"Victor Sanh","newsletterV3":{"__ref":"NewsletterV3:3c60d7bf0bde"},"linkedAccounts":{"__ref":"LinkedAccounts:ac59742e5349"},"isSuspended":false,"imageId":"1*3AEtg8Ma0mqLlIwaF8Swow@2x.jpeg","mediumMemberAt":0,"verifications":{"__typename":"VerifiedInfo","isBookAuthor":false},"socialStats":{"__typename":"SocialStats","followerCount":1154,"followingCount":0,"collectionFollowingCount":9},"bio":"Dog sitter by day, Scientist at @huggingface ü§ó by night | Into Natural Language Processing, started with Computer Vision","isPartnerProgramEnrolled":false,"viewerEdge":{"__ref":"UserViewerEdge:userId:ac59742e5349-viewerId:lo_e0d1c7203e51"},"viewerIsUser":false,"postSubscribeMembershipUpsellShownAt":0,"membership":null,"allowNotes":true,"twitterScreenName":"SanhEstPasMoi"},"User:bae05483be6d":{"__typename":"User","id":"bae05483be6d"},"User:6b82043c70c6":{"__typename":"User","id":"6b82043c70c6"},"User:b1574f0c6c5e":{"__typename":"User","id":"b1574f0c6c5e"},"User:bb5da8a1d5ab":{"__typename":"User","id":"bb5da8a1d5ab"},"User:a78169e66dc7":{"__typename":"User","id":"a78169e66dc7"},"User:a476c06173e0":{"__typename":"User","id":"a476c06173e0"},"User:bac18b20e254":{"__typename":"User","id":"bac18b20e254"},"ImageMetadata:1*ABKawA7BD68tMpncHfAo_Q@2x.png":{"__typename":"ImageMetadata","id":"1*ABKawA7BD68tMpncHfAo_Q@2x.png"},"Post:7abc3c0f0bd7":{"__typename":"Post","id":"7abc3c0f0bd7","firstPublishedAt":1600781472290,"creator":{"__ref":"User:ac59742e5349"},"collection":{"__ref":"Collection:ba0dbdd23ac6"},"isSeries":false,"mediumUrl":"https:\u002F\u002Fmedium.com\u002Fhuggingface\u002Fsimple-considerations-for-simple-people-building-fancy-neural-networks-7abc3c0f0bd7","sequence":null,"uniqueSlug":"simple-considerations-for-simple-people-building-fancy-neural-networks-7abc3c0f0bd7"},"LinkedAccounts:ac59742e5349":{"__typename":"LinkedAccounts","mastodon":null,"id":"ac59742e5349"},"UserViewerEdge:userId:ac59742e5349-viewerId:lo_e0d1c7203e51":{"__typename":"UserViewerEdge","id":"userId:ac59742e5349-viewerId:lo_e0d1c7203e51","isFollowing":false,"isUser":false,"isMuting":false},"NewsletterV3:3c60d7bf0bde":{"__typename":"NewsletterV3","id":"3c60d7bf0bde","type":"NEWSLETTER_TYPE_AUTHOR","slug":"ac59742e5349","name":"ac59742e5349","collection":null,"user":{"__ref":"User:ac59742e5349"}},"Topic:1eca0103fff3":{"__typename":"Topic","slug":"machine-learning","id":"1eca0103fff3","name":"Machine Learning"},"ImageMetadata:1*CVGvdYELFWlWmmOdTuRF4A.jpeg":{"__typename":"ImageMetadata","id":"1*CVGvdYELFWlWmmOdTuRF4A.jpeg","originalHeight":3072,"originalWidth":4096,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:b704e9d465b6_0":{"__typename":"Paragraph","id":"b704e9d465b6_0","name":"4a0c","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*CVGvdYELFWlWmmOdTuRF4A.jpeg"},"text":"Photo by Shubham Sharan on Unsplash","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":9,"end":23,"href":"https:\u002F\u002Funsplash.com\u002F@shubhamsharan?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":27,"end":35,"href":"https:\u002F\u002Funsplash.com\u002Fsearch\u002Fphotos\u002Fteacher?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_1":{"__typename":"Paragraph","id":"b704e9d465b6_1","name":"4647","type":"H3","href":null,"layout":null,"metadata":null,"text":"üèé Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_2":{"__typename":"Paragraph","id":"b704e9d465b6_2","name":"2d50","type":"BQ","href":null,"layout":null,"metadata":null,"text":"2019, October 3rd ‚Äî Update: We are releasing our NeurIPS 2019 workshop paper describing our approach on DistilBERT with improved results: 97% of BERT‚Äôs performance on GLUE (the results in the paper superseed the results presented here). The approach is slightly different from the one explained in this present blog post so this blog post should be a good entry point to the paper! We applied the same method to GPT2 and are releasing DistilGPT2! Training code and pre-trained weights for DistilBERT and DistilGPT2 are available here. ü§ó","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":45,"end":76,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1910.01108","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":529,"end":533,"href":"https:\u002F\u002Fgithub.com\u002Fhuggingface\u002Ftransformers\u002Ftree\u002Fmaster\u002Fexamples\u002Fdistillation","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":0,"end":28,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":173,"end":234,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_3":{"__typename":"Paragraph","id":"b704e9d465b6_3","name":"2315","type":"P","href":null,"layout":null,"metadata":null,"text":"In the last 18 months, transfer learning from large-scale language models has significantly improved upon the state-of-the-art on pretty much every Natural Language Processing task.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":45,"end":74,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_4":{"__typename":"Paragraph","id":"b704e9d465b6_4","name":"ec0d","type":"P","href":null,"layout":null,"metadata":null,"text":"Usually based on the Transformer architecture of Vaswani et al., these pre-trained language models keep getting larger and larger and being trained on bigger datasets. The latest model from Nvidia has 8.3 billion parameters: 24 times larger than BERT-large, 5 times larger than GPT-2, while RoBERTa, the latest work from Facebook AI, was trained on 160GB of text üòµ","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":49,"end":63,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1706.03762","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":201,"end":223,"href":"https:\u002F\u002Fventurebeat.com\u002F2019\u002F08\u002F13\u002Fnvidia-trains-worlds-largest-transformer-based-language-model\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":291,"end":298,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1907.11692","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":111,"end":130,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":133,"end":134,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":151,"end":166,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*IFVX74cEe8U5D1GveL1uZA.png":{"__typename":"ImageMetadata","id":"1*IFVX74cEe8U5D1GveL1uZA.png","originalHeight":1232,"originalWidth":2070,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:b704e9d465b6_5":{"__typename":"Paragraph","id":"b704e9d465b6_5","name":"1018","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*IFVX74cEe8U5D1GveL1uZA.png"},"text":"Some people in the community question the relevance of keeping on training larger and larger Transformer especially when you take into account the financial and environmental cost of training. Here‚Äôs are some of the latest large models and their size in millions of parameters.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":246,"end":276,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_6":{"__typename":"Paragraph","id":"b704e9d465b6_6","name":"1a76","type":"P","href":null,"layout":null,"metadata":null,"text":"At Hugging Face, we experienced first-hand the growing popularity of these models as our NLP library ‚Äî which encapsulates most of them ‚Äî got installed more than 400,000 times in just a few months.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":89,"end":100,"href":"https:\u002F\u002Fgithub.com\u002Fhuggingface\u002Fpytorch-transformers","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_7":{"__typename":"Paragraph","id":"b704e9d465b6_7","name":"38cd","type":"P","href":null,"layout":null,"metadata":null,"text":"However, as these models were reaching a larger NLP community, an important and challenging question started to emerge. How should we put these monsters in production? How can we use such large models under low latency constraints? Do we need (costly) GPU servers to serve at scale?","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":120,"end":167,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":168,"end":231,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_8":{"__typename":"Paragraph","id":"b704e9d465b6_8","name":"367c","type":"PQ","href":null,"layout":null,"metadata":null,"text":"For many researchers and developers, these can be deal-breaking issues üí∏","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_9":{"__typename":"Paragraph","id":"b704e9d465b6_9","name":"9f73","type":"P","href":null,"layout":null,"metadata":null,"text":"To build more privacy-respecting systems, we noticed an increasing need to have machine learning systems operate on the edge rather than calling a cloud API and sending possibly private data to servers. Running models on devices like your smartphone üì≤ also requires light-weight, responsive and energy-efficient models!","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":80,"end":124,"href":"https:\u002F\u002Fgithub.com\u002Fhuggingface\u002Fswift-coreml-transformers","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":113,"end":124,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":266,"end":320,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_10":{"__typename":"Paragraph","id":"b704e9d465b6_10","name":"79fb","type":"P","href":null,"layout":null,"metadata":null,"text":"Last but not least, we are more and more concerned about the environmental cost of scaling exponentially computing requirements of these models.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_11":{"__typename":"Paragraph","id":"b704e9d465b6_11","name":"9819","type":"PQ","href":null,"layout":null,"metadata":null,"text":"So, how can we reduce the size of these monster models‚ÅâÔ∏è","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":54,"end":56,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_12":{"__typename":"Paragraph","id":"b704e9d465b6_12","name":"25d2","type":"P","href":null,"layout":null,"metadata":null,"text":"There are many techniques available to tackle the previous questions. The most common tools include quantization (approximating the weights of a network with a smaller precision) and weights pruning (removing some connections in the network). For these technics, you can have a look at the excellent blog post of Rasa on quantizing BERT.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":300,"end":317,"href":"https:\u002F\u002Fblog.rasa.com\u002Fcompressing-bert-for-faster-prediction-2\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":100,"end":112,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":183,"end":199,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_13":{"__typename":"Paragraph","id":"b704e9d465b6_13","name":"5d83","type":"P","href":null,"layout":null,"metadata":null,"text":"We decided to focus on distillation: a technique you can use to compress a large model, called the teacher, into a smaller model, called the student.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":23,"end":35,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_14":{"__typename":"Paragraph","id":"b704e9d465b6_14","name":"de6c","type":"H3","href":null,"layout":null,"metadata":null,"text":"‚öóÔ∏è Knowledge Distillation ‚Äî Transferring generalization capabilities","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_15":{"__typename":"Paragraph","id":"b704e9d465b6_15","name":"7a99","type":"P","href":null,"layout":null,"metadata":null,"text":"Knowledge distillation (sometimes also referred to as teacher-student learning) is a compression technique in which a small model is trained to reproduce the behavior of a larger model (or an ensemble of models). It was introduced by Bucila et al. and generalized by Hinton et al. a few years later. We will follow the latter method.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":234,"end":247,"href":"https:\u002F\u002Fwww.cs.cornell.edu\u002F~caruana\u002Fcompression.kdd06.pdf","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":267,"end":280,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1503.02531","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":85,"end":184,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":0,"end":22,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":54,"end":78,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_16":{"__typename":"Paragraph","id":"b704e9d465b6_16","name":"a284","type":"P","href":null,"layout":null,"metadata":null,"text":"In supervised learning, a classification model is generally trained to predict a gold class by maximizing its probability (softmax of logits) using the log-likelihood signal. In many cases, a good performance model will predict an output distribution with the correct class having a high probability, leaving other classes with probabilities near zero.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":328,"end":351,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_17":{"__typename":"Paragraph","id":"b704e9d465b6_17","name":"0910","type":"PQ","href":null,"layout":null,"metadata":null,"text":"But, some of these ‚Äúalmost-zero‚Äù probabilities are larger than the others, and this reflects, in part, the generalization capabilities of the model.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":148,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_18":{"__typename":"Paragraph","id":"b704e9d465b6_18","name":"c52b","type":"P","href":null,"layout":null,"metadata":null,"text":"For instance, a desk chair might be mistaken with an armchair but should usually not be mistaken with a mushroom. This uncertainty is sometimes referred to as the ‚Äúdark knowledge‚Äù üåö","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":163,"end":182,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":16,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":53,"end":61,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":104,"end":112,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_19":{"__typename":"Paragraph","id":"b704e9d465b6_19","name":"a6cd","type":"P","href":null,"layout":null,"metadata":null,"text":"Another way to understand distillation is that it prevents the model to be too sure about its prediction (similarly to label smoothing).","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_20":{"__typename":"Paragraph","id":"b704e9d465b6_20","name":"f167","type":"P","href":null,"layout":null,"metadata":null,"text":"Here is an example to see this idea in practice. In language modeling, we can easily observe this uncertainty by looking at the distribution over the vocabulary. Here are the top 20 guesses by BERT for completing this famous quote from the Casablanca movie:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":240,"end":250,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*X3SZ5AL75_z29XyuR4rpeQ.png":{"__typename":"ImageMetadata","id":"1*X3SZ5AL75_z29XyuR4rpeQ.png","originalHeight":728,"originalWidth":1934,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:b704e9d465b6_21":{"__typename":"Paragraph","id":"b704e9d465b6_21","name":"46be","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*X3SZ5AL75_z29XyuR4rpeQ.png"},"text":"The top 20 guesses from BERT (base) for the masked token. The Language model identified two highly probable tokens (day & life) followed by a long tail of valid tokens.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_22":{"__typename":"Paragraph","id":"b704e9d465b6_22","name":"4d7f","type":"H3","href":null,"layout":null,"metadata":null,"text":"üëØ‚Äç‚ôÇÔ∏è How can we copy this dark knowledge?","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_23":{"__typename":"Paragraph","id":"b704e9d465b6_23","name":"e4c3","type":"P","href":null,"layout":null,"metadata":null,"text":"In the teacher-student training, we train a student network to mimic the full output distribution of the teacher network (its knowledge).","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":7,"end":31,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":73,"end":97,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_24":{"__typename":"Paragraph","id":"b704e9d465b6_24","name":"32e6","type":"BQ","href":null,"layout":null,"metadata":null,"text":"We are training the student to generalize the same way as the teacher by matching the output distribution.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_25":{"__typename":"Paragraph","id":"b704e9d465b6_25","name":"f67a","type":"P","href":null,"layout":null,"metadata":null,"text":"Rather than training with a cross-entropy over the hard targets (one-hot encoding of the gold class), we transfer the knowledge from the teacher to the student with a cross-entropy over the soft targets (probabilities of the teacher). Our training loss thus becomes:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*GZkQPjKC_Wqx1F4Uu3FdiQ.png":{"__typename":"ImageMetadata","id":"1*GZkQPjKC_Wqx1F4Uu3FdiQ.png","originalHeight":60,"originalWidth":249,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:b704e9d465b6_26":{"__typename":"Paragraph","id":"b704e9d465b6_26","name":"8045","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*GZkQPjKC_Wqx1F4Uu3FdiQ.png"},"text":"With t the logits from the teacher and s the logits of the student","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":5,"end":6,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":39,"end":40,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":5,"end":66,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_27":{"__typename":"Paragraph","id":"b704e9d465b6_27","name":"5b71","type":"P","href":null,"layout":null,"metadata":null,"text":"This loss is a richer training signal since a single example enforces much more constraint than a single hard target.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":117,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_28":{"__typename":"Paragraph","id":"b704e9d465b6_28","name":"92c3","type":"P","href":null,"layout":null,"metadata":null,"text":"To further expose the mass of the distribution over the classes, Hinton et al. introduce a softmax-temperature:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":91,"end":110,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*BaVyKMXRWaudFvcI9So8MQ.png":{"__typename":"ImageMetadata","id":"1*BaVyKMXRWaudFvcI9So8MQ.png","originalHeight":72,"originalWidth":233,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:b704e9d465b6_29":{"__typename":"Paragraph","id":"b704e9d465b6_29","name":"4c4a","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*BaVyKMXRWaudFvcI9So8MQ.png"},"text":"T is the temperature parameter.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_30":{"__typename":"Paragraph","id":"b704e9d465b6_30","name":"6fa8","type":"P","href":null,"layout":null,"metadata":null,"text":"When T ‚Üí 0, the distribution becomes a Kronecker (and is equivalent to the one-hot target vector), when T ‚Üí+‚àû, it becomes a uniform distribution. The same temperature parameter is applied both to the student and the teacher at training time, further revealing more signals for each training example. At inference, T is set to 1 and recover the standard Softmax.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":146,"end":298,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":5,"end":10,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":104,"end":109,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":314,"end":315,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_31":{"__typename":"Paragraph","id":"b704e9d465b6_31","name":"f8e3","type":"H3","href":null,"layout":null,"metadata":null,"text":"üóúHands-on coding in PyTorch ‚Äî Compressing BERT","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_32":{"__typename":"Paragraph","id":"b704e9d465b6_32","name":"cb9d","type":"P","href":null,"layout":null,"metadata":null,"text":"We want to compress a large language model using distilling. For distilling, we‚Äôll use the Kullback-Leibler loss since the optimizations are equivalent:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":91,"end":112,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FKullback‚ÄìLeibler_divergence","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*ipPFl15_nrY_jQ8QsAMC0w.png":{"__typename":"ImageMetadata","id":"1*ipPFl15_nrY_jQ8QsAMC0w.png","originalHeight":63,"originalWidth":692,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:b704e9d465b6_33":{"__typename":"Paragraph","id":"b704e9d465b6_33","name":"edfc","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*ipPFl15_nrY_jQ8QsAMC0w.png"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_34":{"__typename":"Paragraph","id":"b704e9d465b6_34","name":"679f","type":"P","href":null,"layout":null,"metadata":null,"text":"When computing the gradients with respect to q (the student distribution) we obtain the same gradients. It allows us to leverage PyTorch implementation for faster computation:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":45,"end":46,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*sZSjyGEbxkQfEgHRxCZTaw.png":{"__typename":"ImageMetadata","id":"1*sZSjyGEbxkQfEgHRxCZTaw.png","originalHeight":1012,"originalWidth":1332,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:b704e9d465b6_35":{"__typename":"Paragraph","id":"b704e9d465b6_35","name":"d8b5","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*sZSjyGEbxkQfEgHRxCZTaw.png"},"text":"A Knowledge distillation training step in PyTorch. Copy the gist from here.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":70,"end":74,"href":"https:\u002F\u002Fgist.github.com\u002FVictorSanh\u002Fdb90644aae5094654db87f9769c2e5ae","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_36":{"__typename":"Paragraph","id":"b704e9d465b6_36","name":"e9cf","type":"P","href":null,"layout":null,"metadata":null,"text":"Using the teacher signal, we are able to train a smaller language model, we call DistilBERT, from the supervision of BERT üë®‚Äçüë¶ (we used the English bert-base-uncased version of BERT).","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":149,"end":166,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":49,"end":73,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":80,"end":93,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":102,"end":122,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_37":{"__typename":"Paragraph","id":"b704e9d465b6_37","name":"8b30","type":"P","href":null,"layout":null,"metadata":null,"text":"Following Hinton et al., the training loss is a linear combination of the distillation loss and the masked language modeling loss. Our student is a small version of BERT in which we removed the token-type embeddings and the pooler (used for the next sentence classification task) and kept the rest of the architecture identical while reducing the numbers of layers by a factor of two.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":74,"end":91,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":107,"end":129,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":182,"end":230,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_38":{"__typename":"Paragraph","id":"b704e9d465b6_38","name":"53fb","type":"PQ","href":null,"layout":null,"metadata":null,"text":"Overall, our distilled model, DistilBERT, has about half the total number of parameters of BERT base and retains 95% of BERT‚Äôs performances on the language understanding benchmark GLUE.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":30,"end":41,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":46,"end":56,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_39":{"__typename":"Paragraph","id":"b704e9d465b6_39","name":"9f55","type":"BQ","href":null,"layout":null,"metadata":null,"text":"‚ùìNote 1 ‚Äî Why not reducing the hidden size as well?\nReducing it from 768 to 512 would reduce the total number of parameters by ~2. However, in modern frameworks, most of the operations are highly optimized and variations on the last dimension of the tensor (hidden dimension) have a small impact on most of the operations used in the Transformer architecture (linear layers and layer normalisation). In our experiments, the number of layers was the determining factor for the inference time, more than the hidden size.\nSmaller does not necessarily imply faster‚Ä¶","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":52,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":0,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_40":{"__typename":"Paragraph","id":"b704e9d465b6_40","name":"08a4","type":"BQ","href":null,"layout":null,"metadata":null,"text":"‚ùìNote 2 ‚Äî Some works on distillation like Tang et al. use the L2 distance as a distillation loss directly on downstream tasks.\nOur early experiments suggested that the cross-entropy loss leads to significantly better performance in our case. We hypothesis that in a language modeling setup, the output space (vocabulary) is significantly larger than the dimension of the downstream task output space. The logits may thus compensate for each other in the L2 loss.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":42,"end":53,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1903.12136.pdf","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":0,"end":125,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":0,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_41":{"__typename":"Paragraph","id":"b704e9d465b6_41","name":"e49d","type":"P","href":null,"layout":null,"metadata":null,"text":"Training a sub-network is not only about the architecture. It is also about finding the right initialization for the sub-network to converge (see The Lottery Ticket Hypothesis for instance). We thus initialize our student, DistilBERT, from its teacher, BERT, by taking one layer out of two, leveraging the common hidden size between student and teacher.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":146,"end":175,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1803.03635","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":26,"end":59,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":76,"end":140,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":233,"end":234,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_42":{"__typename":"Paragraph","id":"b704e9d465b6_42","name":"0a01","type":"P","href":null,"layout":null,"metadata":null,"text":"We also used a few training tricks from the recent RoBERTa paper which showed that the way BERT is trained is crucial for its final performance. Following RoBERTa, we trained DistilBERT on very large batches leveraging gradient accumulation (up to 4000 examples per batch), with dynamic masking and removed the next sentence prediction objective.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":51,"end":64,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1907.11692","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":83,"end":143,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":185,"end":186,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":194,"end":207,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":279,"end":294,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":299,"end":345,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_43":{"__typename":"Paragraph","id":"b704e9d465b6_43","name":"80be","type":"P","href":null,"layout":null,"metadata":null,"text":"Our training setup is voluntarily limited in terms of resources. We train DistilBERT on eight 16GB V100 GPUs for approximately three and a half days using the concatenation of Toronto Book Corpus and English Wikipedia (same data as original BERT).","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":84,"end":85,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":88,"end":149,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_44":{"__typename":"Paragraph","id":"b704e9d465b6_44","name":"3193","type":"P","href":null,"layout":null,"metadata":null,"text":"The code for DistilBERT is adapted in part from Facebook XLM‚Äôs code and in part from our PyTorch version of Google AI Bert and is available in our pytorch-transformers library üëæ along with several trained and fine-tuned versions of DistilBert and the code to reproduce the training and fine-tuning.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":57,"end":60,"href":"https:\u002F\u002Fgithub.com\u002Ffacebookresearch\u002FXLM","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":118,"end":122,"href":"https:\u002F\u002Fgithub.com\u002Fgoogle-research\u002Fbert","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":147,"end":175,"href":"https:\u002F\u002Fgithub.com\u002Fhuggingface\u002Fpytorch-transformers\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_45":{"__typename":"Paragraph","id":"b704e9d465b6_45","name":"4e84","type":"H3","href":null,"layout":null,"metadata":null,"text":"üé¢ Model performances ‚Äî Testing DistilBERT","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":32,"end":42,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_46":{"__typename":"Paragraph","id":"b704e9d465b6_46","name":"e76e","type":"P","href":null,"layout":null,"metadata":null,"text":"We compare the performance of DistilBERT on the development sets of the GLUE benchmark against two baselines: BERT base (DistilBERT‚Äôs teacher) and a strong non-transformer baseline from NYU: two BiLSTMs on top of ELMo. We use the jiant library from NYU for ELMo baselines and pytorch-transformers for the BERT baseline.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":72,"end":86,"href":"https:\u002F\u002Fgluebenchmark.com","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":230,"end":235,"href":"https:\u002F\u002Fgithub.com\u002Fnyu-mll\u002Fjiant","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":276,"end":296,"href":"https:\u002F\u002Fgithub.com\u002Fhuggingface\u002Fpytorch-transformers","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":72,"end":87,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_47":{"__typename":"Paragraph","id":"b704e9d465b6_47","name":"0b3d","type":"P","href":null,"layout":null,"metadata":null,"text":"As shown in the following table, DistilBERT‚Äôs performances compare favorably with the baselines while having respectively about half and one third the number of parameters (more on this below). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo baseline (up to 14 points of accuracy on QNLI). DistilBERT also compares surprisingly well to BERT: we are able to retain more than 95% of the performance while having 40% fewer parameters.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":59,"end":95,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":227,"end":276,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":332,"end":366,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":383,"end":422,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*hHwcSZEazpY_PwArgBzHtw.png":{"__typename":"ImageMetadata","id":"1*hHwcSZEazpY_PwArgBzHtw.png","originalHeight":212,"originalWidth":2514,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:b704e9d465b6_48":{"__typename":"Paragraph","id":"b704e9d465b6_48","name":"2b89","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*hHwcSZEazpY_PwArgBzHtw.png"},"text":"Comparison on the dev sets of the GLUE benchmark. ELMo results as reported by the authors. BERT and DistilBERT results are medians of 5 runs with different seeds.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_49":{"__typename":"Paragraph","id":"b704e9d465b6_49","name":"c155","type":"P","href":null,"layout":null,"metadata":null,"text":"In terms of inference time, DistilBERT is more than 60% faster and smaller than BERT and 120% faster and smaller than ELMo+BiLSTM üêé","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":52,"end":75,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":88,"end":113,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":129,"end":132,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*RLBWful7k50nV9zTJCuZ4Q.png":{"__typename":"ImageMetadata","id":"1*RLBWful7k50nV9zTJCuZ4Q.png","originalHeight":202,"originalWidth":1010,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:b704e9d465b6_50":{"__typename":"Paragraph","id":"b704e9d465b6_50","name":"f1d1","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*RLBWful7k50nV9zTJCuZ4Q.png"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_51":{"__typename":"Paragraph","id":"b704e9d465b6_51","name":"c4b9","type":"P","href":null,"layout":null,"metadata":null,"text":"To further investigate the speed-up\u002Fsize trade-off of DistilBERT, we compare, in the left table, the number of parameters of each model along with the inference time needed to do a full pass on the STS-B dev set on CPU (using a batch size of 1).","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_52":{"__typename":"Paragraph","id":"b704e9d465b6_52","name":"3be2","type":"H3","href":null,"layout":null,"metadata":null,"text":"üîÆ Downstream task: Distillation & transfer-learning","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":52,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_53":{"__typename":"Paragraph","id":"b704e9d465b6_53","name":"427d","type":"P","href":null,"layout":null,"metadata":null,"text":"We further study the use of DistilBERT on downstream tasks under efficient inference constraints. We use our compact pre-trained language model by fine-tuning it a classification task. A nice way to actually mix distillation pre-training and transfer-learning!","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":208,"end":259,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*BtwpBJvHfdYUHCpt-wLvGw.png":{"__typename":"ImageMetadata","id":"1*BtwpBJvHfdYUHCpt-wLvGw.png","originalHeight":292,"originalWidth":656,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:b704e9d465b6_54":{"__typename":"Paragraph","id":"b704e9d465b6_54","name":"dea7","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*BtwpBJvHfdYUHCpt-wLvGw.png"},"text":"Extract from the IMDB Review dataset ‚Äî Source: Kaggle","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":47,"end":53,"href":"https:\u002F\u002Fwww.kaggle.com\u002Fdesiredewaele\u002Fsentiment-analysis-on-imdb-reviews","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_55":{"__typename":"Paragraph","id":"b704e9d465b6_55","name":"87aa","type":"P","href":null,"layout":null,"metadata":null,"text":"We selected the IMDB Review Sentiment Classification which is composed of 50'000 reviews in English labeled as positive or negative: 25'000 for training and 25'000 for test (and with balanced classes). We trained on a single 12GB K80.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":16,"end":53,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":58,"end":59,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_56":{"__typename":"Paragraph","id":"b704e9d465b6_56","name":"e798","type":"P","href":null,"layout":null,"metadata":null,"text":"First, we train bert-base-uncased on our dataset. Our dear BERT üíã reaches an accuracy of 93.46% (average of 6 runs) without any hyper-parameters search.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":16,"end":33,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":75,"end":96,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_57":{"__typename":"Paragraph","id":"b704e9d465b6_57","name":"f989","type":"P","href":null,"layout":null,"metadata":null,"text":"We then train DistilBERT, using the same hyper-parameters. The compressed model reaches an accuracy of 93.07% (average of 6runs). An absolute difference of 0.4% in performances for a 60% reduction in latency and 40% in size üèé!","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":88,"end":110,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_58":{"__typename":"Paragraph","id":"b704e9d465b6_58","name":"03d5","type":"BQ","href":null,"layout":null,"metadata":null,"text":"‚ùìNote 3 ‚Äî As noted by the community, you can reach comparable or better score on the IMDB benchmark with lighter methods (size-wise and inference-wise) like ULMFiT. We encourage you to compare on your own use-case! In particular, DistilBERT can give a sensible lower-bound on Bert‚Äôs performances with the advantage of faster training.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":10,"end":35,"href":"https:\u002F\u002Ftwitter.com\u002FPiotrCzapla\u002Fstatus\u002F1168120760201859072?s=20","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":0,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":0,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_59":{"__typename":"Paragraph","id":"b704e9d465b6_59","name":"fab0","type":"P","href":null,"layout":null,"metadata":null,"text":"Another common application of NLP is Question Answering. We compared the results of the bert-base-uncased version of BERT with DistilBERT on the SQuAD 1.1 dataset. On the development set, BERT reaches an F1 score of 88.5 and an EM (Exact-match) score of 81.2. We train DistilBERT on the same set of hyper-parameters and reach scores of 85.1 F1 and 76.5 EM, within 3 to 5 points of the full BERT.","hasDropCap":true,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":88,"end":105,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":37,"end":57,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":336,"end":394,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_60":{"__typename":"Paragraph","id":"b704e9d465b6_60","name":"7fc2","type":"P","href":null,"layout":null,"metadata":null,"text":"We also studied whether we could add another step of distillation during the adaptation phase by finetuning DistilBERT on SQuAD using the finetuned BERT model as a teacher with a knowledge distillation loss.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_61":{"__typename":"Paragraph","id":"b704e9d465b6_61","name":"f13c","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Here we are finetuning by distilling a question answering model into a language model previously pre-trained with knowledge distillation! That a lot of teachers and studentsüéì","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_62":{"__typename":"Paragraph","id":"b704e9d465b6_62","name":"6c5a","type":"P","href":null,"layout":null,"metadata":null,"text":"In this case, we were able to reach interesting performances given the size of the network: 86.2 F1 and 78.1 EM, ie. within 3 points of the full model!","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":92,"end":151,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_63":{"__typename":"Paragraph","id":"b704e9d465b6_63","name":"9b1e","type":"P","href":null,"layout":null,"metadata":null,"text":"Other works have also attempted to accelerate question answering models. Notably, Debajyoti Chatterjee, uploaded an interesting work on arXiv which follows a similar method for the adaptation phase on SQuAD (initializing a student from its teacher, and training a question-answering model via distillation). His experiments present similar relative performances with regards to BERT (base uncased). The main difference with our present work is that we pre-train DistilBERT with a general objective (Masked Language Modeling) in order to obtain a model that can be used for transfer-learning on a large range of tasks via finetuning (GLUE, SQuAD, classification‚Ä¶).","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":82,"end":102,"href":"https:\u002F\u002Ftwitter.com\u002FDebajyotiChat17","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":113,"end":141,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1904.00796","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_64":{"__typename":"Paragraph","id":"b704e9d465b6_64","name":"0928","type":"H3","href":null,"layout":null,"metadata":null,"text":"üôå Less is more: smaller models also spark joy üåü","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_65":{"__typename":"Paragraph","id":"b704e9d465b6_65","name":"dcc3","type":"P","href":null,"layout":null,"metadata":null,"text":"We are very excited about DistilBERT‚Äôs potential. The work we‚Äôve presented is just the beginning of what can be done and raises many questions: How far can we compress these models with knowledge distillation? Can these technics be used to get further insights into the knowledge stored in the large version? What aspects of linguistic\u002Fsemantics do we lose in this type of compression?‚Ä¶","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":26,"end":48,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_66":{"__typename":"Paragraph","id":"b704e9d465b6_66","name":"cf78","type":"P","href":null,"layout":null,"metadata":null,"text":"One essential aspect of our work at HuggingFace is open-source and knowledge sharing as you can see from our GitHub and medium pages. We think it is both the easiest and fairest way for everyone to participate and reap the fruits of the remarkable progress of deep learning for NLP.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":109,"end":115,"href":"https:\u002F\u002Fgithub.com\u002Fhuggingface","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":120,"end":126,"href":"http:\u002F\u002Fwww.medium.com\u002Fhuggingface","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":51,"end":63,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":66,"end":84,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_67":{"__typename":"Paragraph","id":"b704e9d465b6_67","name":"3b2a","type":"P","href":null,"layout":null,"metadata":null,"text":"Thus, together with this blog post, we release the code of our experiments üéÆ (in particular the code to reproduce the training and fine-tuning of DistilBERT) along with a trained version of DistilBERT in our pytorch-transformers libraryüî•.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":209,"end":237,"href":"https:\u002F\u002Fgithub.com\u002Fhuggingface\u002Fpytorch-transformers\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:b704e9d465b6_68":{"__typename":"Paragraph","id":"b704e9d465b6_68","name":"be4e","type":"P","href":null,"layout":null,"metadata":null,"text":"Many thanks to Sam Bowman, Alex Wang and Thibault F√©vry for feedback and discussions!","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":0,"end":85,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:9028cd193efdc5a465b8ac91e4702628":{"__typename":"MediaResource","id":"9028cd193efdc5a465b8ac91e4702628","iframeSrc":"https:\u002F\u002Fcdn.embedly.com\u002Fwidgets\u002Fmedia.html?src=https%3A%2F%2Fhuggingface.typeform.com%2Fto%2FP4EATC%3Ftypeform-embed%3Doembed%26format%3Djson&display_name=Typeform&url=https%3A%2F%2Fhuggingface.typeform.com%2Fto%2FP4EATC&image=https%3A%2F%2Fimages.typeform.com%2Fimages%2FMpqBWPwLRZ7Z%2Fimage%2Fdefault&key=a19fcc184b9711e1b4764040d3dc5c07&type=text%2Fhtml&schema=typeform","iframeHeight":600,"iframeWidth":900,"title":"Hugging Face"},"Paragraph:b704e9d465b6_69":{"__typename":"Paragraph","id":"b704e9d465b6_69","name":"b710","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:9028cd193efdc5a465b8ac91e4702628"}},"mixtapeMetadata":null},"CollectionViewerEdge:collectionId:ba0dbdd23ac6-viewerId:lo_e0d1c7203e51":{"__typename":"CollectionViewerEdge","id":"collectionId:ba0dbdd23ac6-viewerId:lo_e0d1c7203e51","isEditor":false,"isMuting":false},"ImageMetadata:1*NNALWVaZvBmLBZXPqRmXBQ.png":{"__typename":"ImageMetadata","id":"1*NNALWVaZvBmLBZXPqRmXBQ.png","originalWidth":220,"originalHeight":80},"PostViewerEdge:postId:8cf3380435b5-viewerId:lo_e0d1c7203e51":{"__typename":"PostViewerEdge","shouldIndexPostForExternalSearch":true,"id":"postId:8cf3380435b5-viewerId:lo_e0d1c7203e51"},"Tag:machine-learning":{"__typename":"Tag","id":"machine-learning","displayTitle":"Machine Learning","normalizedTagSlug":"machine-learning"},"Tag:nlp":{"__typename":"Tag","id":"nlp","displayTitle":"NLP","normalizedTagSlug":"nlp"},"Tag:bert":{"__typename":"Tag","id":"bert","displayTitle":"Bert","normalizedTagSlug":"bert"},"Tag:distillation":{"__typename":"Tag","id":"distillation","displayTitle":"Distillation","normalizedTagSlug":"distillation"},"Tag:transformers":{"__typename":"Tag","id":"transformers","displayTitle":"Transformers","normalizedTagSlug":"transformers"},"Post:8cf3380435b5":{"__typename":"Post","id":"8cf3380435b5","collection":{"__ref":"Collection:ba0dbdd23ac6"},"content({\"postMeteringOptions\":{}})":{"__typename":"PostContent","isLockedPreviewOnly":false,"bodyModel":{"__typename":"RichText","sections":[{"__typename":"Section","name":"627a","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"2a1b","startIndex":41,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"06f4","startIndex":52,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"6f3b","startIndex":69,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null}],"paragraphs":[{"__ref":"Paragraph:b704e9d465b6_0"},{"__ref":"Paragraph:b704e9d465b6_1"},{"__ref":"Paragraph:b704e9d465b6_2"},{"__ref":"Paragraph:b704e9d465b6_3"},{"__ref":"Paragraph:b704e9d465b6_4"},{"__ref":"Paragraph:b704e9d465b6_5"},{"__ref":"Paragraph:b704e9d465b6_6"},{"__ref":"Paragraph:b704e9d465b6_7"},{"__ref":"Paragraph:b704e9d465b6_8"},{"__ref":"Paragraph:b704e9d465b6_9"},{"__ref":"Paragraph:b704e9d465b6_10"},{"__ref":"Paragraph:b704e9d465b6_11"},{"__ref":"Paragraph:b704e9d465b6_12"},{"__ref":"Paragraph:b704e9d465b6_13"},{"__ref":"Paragraph:b704e9d465b6_14"},{"__ref":"Paragraph:b704e9d465b6_15"},{"__ref":"Paragraph:b704e9d465b6_16"},{"__ref":"Paragraph:b704e9d465b6_17"},{"__ref":"Paragraph:b704e9d465b6_18"},{"__ref":"Paragraph:b704e9d465b6_19"},{"__ref":"Paragraph:b704e9d465b6_20"},{"__ref":"Paragraph:b704e9d465b6_21"},{"__ref":"Paragraph:b704e9d465b6_22"},{"__ref":"Paragraph:b704e9d465b6_23"},{"__ref":"Paragraph:b704e9d465b6_24"},{"__ref":"Paragraph:b704e9d465b6_25"},{"__ref":"Paragraph:b704e9d465b6_26"},{"__ref":"Paragraph:b704e9d465b6_27"},{"__ref":"Paragraph:b704e9d465b6_28"},{"__ref":"Paragraph:b704e9d465b6_29"},{"__ref":"Paragraph:b704e9d465b6_30"},{"__ref":"Paragraph:b704e9d465b6_31"},{"__ref":"Paragraph:b704e9d465b6_32"},{"__ref":"Paragraph:b704e9d465b6_33"},{"__ref":"Paragraph:b704e9d465b6_34"},{"__ref":"Paragraph:b704e9d465b6_35"},{"__ref":"Paragraph:b704e9d465b6_36"},{"__ref":"Paragraph:b704e9d465b6_37"},{"__ref":"Paragraph:b704e9d465b6_38"},{"__ref":"Paragraph:b704e9d465b6_39"},{"__ref":"Paragraph:b704e9d465b6_40"},{"__ref":"Paragraph:b704e9d465b6_41"},{"__ref":"Paragraph:b704e9d465b6_42"},{"__ref":"Paragraph:b704e9d465b6_43"},{"__ref":"Paragraph:b704e9d465b6_44"},{"__ref":"Paragraph:b704e9d465b6_45"},{"__ref":"Paragraph:b704e9d465b6_46"},{"__ref":"Paragraph:b704e9d465b6_47"},{"__ref":"Paragraph:b704e9d465b6_48"},{"__ref":"Paragraph:b704e9d465b6_49"},{"__ref":"Paragraph:b704e9d465b6_50"},{"__ref":"Paragraph:b704e9d465b6_51"},{"__ref":"Paragraph:b704e9d465b6_52"},{"__ref":"Paragraph:b704e9d465b6_53"},{"__ref":"Paragraph:b704e9d465b6_54"},{"__ref":"Paragraph:b704e9d465b6_55"},{"__ref":"Paragraph:b704e9d465b6_56"},{"__ref":"Paragraph:b704e9d465b6_57"},{"__ref":"Paragraph:b704e9d465b6_58"},{"__ref":"Paragraph:b704e9d465b6_59"},{"__ref":"Paragraph:b704e9d465b6_60"},{"__ref":"Paragraph:b704e9d465b6_61"},{"__ref":"Paragraph:b704e9d465b6_62"},{"__ref":"Paragraph:b704e9d465b6_63"},{"__ref":"Paragraph:b704e9d465b6_64"},{"__ref":"Paragraph:b704e9d465b6_65"},{"__ref":"Paragraph:b704e9d465b6_66"},{"__ref":"Paragraph:b704e9d465b6_67"},{"__ref":"Paragraph:b704e9d465b6_68"},{"__ref":"Paragraph:b704e9d465b6_69"}]},"validatedShareKey":"","shareKeyCreator":null},"creator":{"__ref":"User:ac59742e5349"},"inResponseToEntityType":null,"isLocked":false,"isMarkedPaywallOnly":false,"lockedSource":"LOCKED_POST_SOURCE_NONE","mediumUrl":"https:\u002F\u002Fmedium.com\u002Fhuggingface\u002Fdistilbert-8cf3380435b5","primaryTopic":{"__ref":"Topic:1eca0103fff3"},"topics":[{"__typename":"Topic","slug":"machine-learning"}],"isPublished":true,"latestPublishedVersion":"b704e9d465b6","visibility":"PUBLIC","postResponses":{"__typename":"PostResponses","count":20},"clapCount":4187,"allowResponses":true,"isLimitedState":false,"title":"üèé Smaller, faster, cheaper, lighter: Introducing DilBERT, a distilled version of BERT","isSeries":false,"sequence":null,"uniqueSlug":"distilbert-8cf3380435b5","socialTitle":"","socialDek":"","canonicalUrl":"","metaDescription":"HuggingFace introduces DilBERT, a distilled and smaller version of Google AI‚Äôs Bert model with strong performances on language understanding. DilBert s included in the pytorch-transformers library.","latestPublishedAt":1598886169872,"readingTime":9.872641509433961,"previewContent":{"__typename":"PreviewContent","subtitle":"You can find the code to reproduce the training of DilBERT along with pre-trained weights for DilBERT here."},"previewImage":{"__ref":"ImageMetadata:1*IFVX74cEe8U5D1GveL1uZA.png"},"isShortform":false,"seoTitle":"","firstPublishedAt":1567003404584,"updatedAt":1639225629113,"shortformType":"SHORTFORM_TYPE_LINK","seoDescription":"","viewerEdge":{"__ref":"PostViewerEdge:postId:8cf3380435b5-viewerId:lo_e0d1c7203e51"},"isSuspended":false,"license":"ALL_RIGHTS_RESERVED","tags":[{"__ref":"Tag:machine-learning"},{"__ref":"Tag:nlp"},{"__ref":"Tag:bert"},{"__ref":"Tag:distillation"},{"__ref":"Tag:transformers"}],"isNewsletter":false,"statusForCollection":"APPROVED","pendingCollection":null,"detectedLanguage":"en","wordCount":2285,"layerCake":3,"responsesLocked":false}}</script><script>window.__MIDDLEWARE_STATE__={"session":{"xsrf":""},"cache":{"cacheStatus":"HIT"}}</script><script src="https://cdn-client.medium.com/lite/static/js/manifest.490136b0.js"></script><script src="https://cdn-client.medium.com/lite/static/js/9865.1496d74a.js"></script><script src="https://cdn-client.medium.com/lite/static/js/main.e23dbeb6.js"></script><script src="https://cdn-client.medium.com/lite/static/js/instrumentation.d9108df7.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/reporting.ff22a7a5.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9120.5df29668.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5049.d1ead72d.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4810.6318add7.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6618.db187378.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2707.b0942613.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9977.5b3eb23a.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8599.1ab63137.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5250.9f9e01d2.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5787.e66a3a4d.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2648.26563adf.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8393.826a25fb.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/7082.124c8a15.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6589.5efd110b.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/3735.afb7e926.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5642.0a97706a.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6546.cd03f950.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6834.08de95de.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/7346.72622eb9.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2420.2a5e2d95.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/839.ca7937c2.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/7975.d195c6f1.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/7394.bf599bc5.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2961.00a48598.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8204.c4082863.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4391.59acaed3.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/PostPage.MainContent.289eec71.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8414.6565ad5f.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/3974.8d3e0217.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2527.a0afad8a.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/PostResponsesContent.36c2ecf4.chunk.js"></script><script>window.main();</script></body></html>