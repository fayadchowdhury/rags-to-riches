{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adcd2ed2-2870-43eb-b1cd-e48a657d6527",
   "metadata": {},
   "source": [
    "# Document parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47330db8-0cc6-4ecf-aed7-e70bce4e3404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/fayad/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymupdf\n",
    "from pypdf import PdfReader\n",
    "from IPython.display import display, Markdown\n",
    "import json\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24addb2a-8064-4309-a5b6-ff6f3f507714",
   "metadata": {},
   "source": [
    "### PyMuPDF4LLM experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2dbb0ea-4a68-473c-8849-265ad06a0200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf4llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be5ad630-1765-44a2-89f9-d831c76f94ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "lec_doc_path = \"../data/input/full/references/superglue.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faab1897-1988-4fbd-9058-b89151732ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = pymupdf4llm.to_markdown(lec_doc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d50641a-848e-4fec-a49a-0b1b72278d73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# **SuperGLUE: A Stickier Benchmark for** **General-Purpose Language Understanding Systems**\n",
       "\n",
       "**Alex Wang** _[∗]_ **Yada Pruksachatkun** _[∗]_ **Nikita Nangia** _[∗]_\n",
       "New York University New York University New York University\n",
       "\n",
       "\n",
       "**Amanpreet Singh** _[∗]_ **Julian Michael** **Felix Hill** **Omer Levy**\n",
       "Facebook AI Research University of Washington DeepMind Facebook AI Research\n",
       "\n",
       "\n",
       "**Samuel R. Bowman**\n",
       "New York University\n",
       "\n",
       "\n",
       "**Abstract**\n",
       "\n",
       "\n",
       "In the last year, new models and methods for pretraining and transfer learning have\n",
       "driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers\n",
       "a single-number metric that summarizes progress on a diverse set of such tasks,\n",
       "but performance on the benchmark has recently surpassed the level of non-expert\n",
       "humans, suggesting limited headroom for further research. In this paper we present\n",
       "SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard.\n",
       "SuperGLUE is available at `[super.gluebenchmark.com](https://super.gluebenchmark.com/)` .\n",
       "\n",
       "\n",
       "**1** **Introduction**\n",
       "\n",
       "\n",
       "In the past year, there has been notable progress across many natural language processing (NLP)\n",
       "tasks, led by methods such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018),\n",
       "and BERT (Devlin et al., 2019). The common thread connecting these methods is that they couple\n",
       "self-supervised learning from massive unlabelled text corpora with a recipe for effectively adapting\n",
       "the resulting model to target tasks. The tasks that have proven amenable to this general approach\n",
       "include question answering, sentiment analysis, textual entailment, and parsing, among many others\n",
       "(Devlin et al., 2019; Kitaev and Klein, 2018, i.a.).\n",
       "\n",
       "\n",
       "In this context, the GLUE benchmark (Wang et al., 2019a) has become a prominent evaluation\n",
       "framework for research towards general-purpose language understanding technologies. GLUE is\n",
       "a collection of nine language understanding tasks built on existing public datasets, together with\n",
       "private test data, an evaluation server, a single-number target metric, and an accompanying expertconstructed diagnostic set. GLUE was designed to provide a general-purpose evaluation of language\n",
       "understanding that covers a range of training data volumes, task genres, and task formulations. We\n",
       "believe it was these aspects that made GLUE particularly appropriate for exhibiting the transferlearning potential of approaches like OpenAI GPT and BERT.\n",
       "\n",
       "\n",
       "The progress of the last twelve months has eroded headroom on the GLUE benchmark dramatically.\n",
       "While some tasks (Figure 1) and some linguistic phenomena (Figure 2 in Appendix B) measured\n",
       "in GLUE remain difficult, the current state of the art GLUE Score as of early July 2019 (88.4 from\n",
       "Yang et al., 2019) surpasses human performance (87.1 from Nangia and Bowman, 2019) by 1.3\n",
       "\n",
       "\n",
       "_∗_ Equal contribution. Correspondence: `glue-benchmark-admin@googlegroups.com`\n",
       "\n",
       "\n",
       "33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\n",
       "\n",
       "\n",
       "|1.2|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "|.<br>|||||||||||||\n",
       "|0.7<br>0.8<br>0.9<br>1.0<br>|||||||||||||\n",
       "|0.7<br>0.8<br>0.9<br>1.0<br>|||||||||||||\n",
       "|0.7<br>0.8<br>0.9<br>1.0<br>||+ELMo+Attn|OpenAI GPT|sk Adapters|ERT (Large)|T on STILTs|BERT + BAM|SemBERT|orkel MeTaL|LICE (Large)|(ensemble)|(ensemble)|\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "Figure 1: GLUE benchmark performance for submitted systems, rescaled to set human performance\n",
       "to 1.0, shown as a single number score, and broken down into the nine constituent task performances.\n",
       "For tasks with multiple metrics, we use an average of the metrics. More information on the tasks\n",
       "included in GLUE can be found in Wang et al. (2019a) and in Warstadt et al. (2018, CoLA), Socher\n",
       "et al. (2013, SST-2), Dolan and Brockett (2005, MRPC), Cer et al. (2017, STS-B), and Williams et al.\n",
       "(2018, MNLI), and Rajpurkar et al. (2016, the original data source for QNLI).\n",
       "\n",
       "\n",
       "points, and in fact exceeds this human performance estimate on four tasks. Consequently, while there\n",
       "remains substantial scope for improvement towards GLUE’s high-level goals, the original version of\n",
       "the benchmark is no longer a suitable metric for quantifying such progress.\n",
       "\n",
       "\n",
       "In response, we introduce SuperGLUE, a new benchmark designed to pose a more rigorous test of\n",
       "language understanding. SuperGLUE has the same high-level motivation as GLUE: to provide a\n",
       "simple, hard-to-game measure of progress toward general-purpose language understanding technologies for English. We anticipate that significant progress on SuperGLUE should require substantive\n",
       "innovations in a number of core areas of machine learning, including sample-efficient, transfer,\n",
       "multitask, and unsupervised or self-supervised learning.\n",
       "\n",
       "\n",
       "SuperGLUE follows the basic design of GLUE: It consists of a public leaderboard built around\n",
       "eight language understanding tasks, drawing on existing data, accompanied by a single-number\n",
       "performance metric, and an analysis toolkit. However, it improves upon GLUE in several ways:\n",
       "\n",
       "\n",
       "_•_ **More challenging tasks:** SuperGLUE retains the two hardest tasks in GLUE. The remaining tasks were identified from those submitted to an open call for task proposals and were\n",
       "selected based on difficulty for current NLP approaches.\n",
       "\n",
       "\n",
       "_•_ **More diverse task formats:** The task formats in GLUE are limited to sentence- and\n",
       "sentence-pair classification. We expand the set of task formats in SuperGLUE to include\n",
       "coreference resolution and question answering (QA).\n",
       "\n",
       "\n",
       "_•_ **Comprehensive human baselines:** We include human performance estimates for all benchmark tasks, which verify that substantial headroom exists between a strong BERT-based\n",
       "baseline and human performance.\n",
       "\n",
       "\n",
       "_•_ **Improved code support:** SuperGLUE is distributed with a new, modular toolkit for work\n",
       "on pretraining, multi-task learning, and transfer learning in NLP, built around standard tools\n",
       "including PyTorch (Paszke et al., 2017) and AllenNLP (Gardner et al., 2017).\n",
       "\n",
       "\n",
       "_•_ **Refined usage rules:** The conditions for inclusion on the SuperGLUE leaderboard have\n",
       "been revamped to ensure fair competition, an informative leaderboard, and full credit\n",
       "assignment to data and task creators.\n",
       "\n",
       "\n",
       "The SuperGLUE leaderboard, data, and software tools are available at `[super.gluebenchmark.com](https://super.gluebenchmark.com/)` .\n",
       "\n",
       "\n",
       "2\n",
       "\n",
       "\n",
       "**2** **Related Work**\n",
       "\n",
       "\n",
       "Much work prior to GLUE demonstrated that training neural models with large amounts of available\n",
       "supervision can produce representations that effectively transfer to a broad range of NLP tasks\n",
       "(Collobert and Weston, 2008; Dai and Le, 2015; Kiros et al., 2015; Hill et al., 2016; Conneau and\n",
       "Kiela, 2018; McCann et al., 2017; Peters et al., 2018). GLUE was presented as a formal challenge\n",
       "affording straightforward comparison between such task-agnostic transfer learning techniques. Other\n",
       "similarly-motivated benchmarks include SentEval (Conneau and Kiela, 2018), which specifically\n",
       "evaluates fixed-size sentence embeddings, and DecaNLP (McCann et al., 2018), which recasts a set\n",
       "of target tasks into a general question-answering format and prohibits task-specific parameters. In\n",
       "contrast, GLUE provides a lightweight classification API and no restrictions on model architecture or\n",
       "parameter sharing, which seems to have been well-suited to recent work in this area.\n",
       "\n",
       "\n",
       "Since its release, GLUE has been used as a testbed and showcase by the developers of several\n",
       "influential models, including GPT (Radford et al., 2018) and BERT (Devlin et al., 2019). As shown\n",
       "in Figure 1, progress on GLUE since its release has been striking. On GLUE, GPT and BERT\n",
       "achieved scores of 72.8 and 80.2 respectively, relative to 66.5 for an ELMo-based model (Peters\n",
       "et al., 2018) and 63.7 for the strongest baseline with no multitask learning or pretraining above the\n",
       "word level. Recent models (Liu et al., 2019d; Yang et al., 2019) have clearly surpassed estimates of\n",
       "non-expert human performance on GLUE (Nangia and Bowman, 2019). The success of these models\n",
       "on GLUE has been driven by ever-increasing model capacity, compute power, and data quantity, as\n",
       "well as innovations in model expressivity (from recurrent to bidirectional recurrent to multi-headed\n",
       "transformer encoders) and degree of contextualization (from learning representation of words in\n",
       "isolation to using uni-directional contexts and ultimately to leveraging bidirectional contexts).\n",
       "\n",
       "\n",
       "In parallel to work scaling up pretrained models, several studies have focused on complementary\n",
       "methods for augmenting performance of pretrained models. Phang et al. (2018) show that BERT can\n",
       "be improved using two-stage pretraining, i.e., fine-tuning the pretrained model on an intermediate\n",
       "data-rich supervised task before fine-tuning it again on a data-poor target task. Liu et al. (2019d,c) and\n",
       "Bach et al. (2018) get further improvements respectively via multi-task finetuning and using massive\n",
       "amounts of weak supervision. Anonymous (2018) demonstrate that knowledge distillation (Hinton\n",
       "et al., 2015; Furlanello et al., 2018) can lead to student networks that outperform their teachers.\n",
       "Overall, the quantity and quality of research contributions aimed at the challenges posed by GLUE\n",
       "underline the utility of this style of benchmark for machine learning researchers looking to evaluate\n",
       "new application-agnostic methods on language understanding.\n",
       "\n",
       "\n",
       "Limits to current approaches are also apparent via the GLUE suite. Performance on the GLUE\n",
       "diagnostic entailment dataset, at 0.42 _R_ 3, falls far below the average human performance of 0.80\n",
       "_R_ 3 reported in the original GLUE publication, with models performing near, or even below, chance\n",
       "on some linguistic phenomena (Figure 2, Appendix B). While some initially difficult categories\n",
       "saw gains from advances on GLUE (e.g., double negation), others remain hard (restrictivity) or\n",
       "even adversarial (disjunction, downward monotonicity). This suggests that even as unsupervised\n",
       "pretraining produces ever-better statistical summaries of text, it remains difficult to extract many\n",
       "details crucial to semantics without the right kind of supervision. Much recent work has made similar\n",
       "observations about the limitations of existing pretrained models (Jia and Liang, 2017; Naik et al.,\n",
       "2018; McCoy and Linzen, 2019; McCoy et al., 2019; Liu et al., 2019a,b).\n",
       "\n",
       "\n",
       "**3** **SuperGLUE Overview**\n",
       "\n",
       "\n",
       "**3.1** **Design Process**\n",
       "\n",
       "\n",
       "The goal of SuperGLUE is to provide a simple, robust evaluation metric of any method capable of\n",
       "being applied to a broad range of language understanding tasks. To that end, in designing SuperGLUE,\n",
       "we identify the following desiderata of tasks in the benchmark:\n",
       "\n",
       "\n",
       "_•_ **Task substance:** Tasks should test a system’s ability to understand and reason about texts\n",
       "written in English.\n",
       "\n",
       "_•_ **Task difficulty:** Tasks should be beyond the scope of current state-of-the-art systems,\n",
       "but solvable by most college-educated English speakers. We exclude tasks that require\n",
       "domain-specific knowledge, e.g. medical notes or scientific papers.\n",
       "\n",
       "\n",
       "3\n",
       "\n",
       "\n",
       "Table 1: The tasks included in SuperGLUE. _WSD_ stands for word sense disambiguation, _NLI_ is\n",
       "natural language inference, _coref._ is coreference resolution, and _QA_ is question answering. For\n",
       "MultiRC, we list the number of total answers for 456/83/166 train/dev/test questions. The metrics for\n",
       "MultiRC are binary F1 on all answer-options and exact match.\n",
       "\n",
       "\n",
       "**Corpus** _|_ **Train** _|_ _|_ **Dev** _|_ _|_ **Test** _|_ **Task** **Metrics** **Text Sources**\n",
       "\n",
       "\n",
       "BoolQ 9427 3270 3245 QA acc. Google queries, Wikipedia\n",
       "CB 250 57 250 NLI acc./F1 various\n",
       "COPA 400 100 500 QA acc. blogs, photography encyclopedia\n",
       "MultiRC 5100 953 1800 QA F1 _a_ /EM various\n",
       "ReCoRD 101k 10k 10k QA F1/EM news (CNN, Daily Mail)\n",
       "RTE 2500 278 300 NLI acc. news, Wikipedia\n",
       "WiC 6000 638 1400 WSD acc. WordNet, VerbNet, Wiktionary\n",
       "WSC 554 104 146 coref. acc. fiction books\n",
       "\n",
       "\n",
       "_•_ **Evaluability:** Tasks must have an automatic performance metric that corresponds well to\n",
       "human judgments of output quality. Certain text generation tasks fail to meet this criteria\n",
       "due to issues surrounding automatic metrics like ROUGE and BLEU (Callison-Burch et al.,\n",
       "2006; Liu et al., 2016, i.a.).\n",
       "\n",
       "\n",
       "_•_ **Public data:** We require that tasks have _existing_ public training data in order to minimize\n",
       "the risks involved in newly-created datasets. We also prefer tasks for which we have access\n",
       "to (or could create) a test set with private labels.\n",
       "\n",
       "\n",
       "_•_ **Task format:** We prefer tasks that had relatively simple input and output formats, to avoid\n",
       "incentivizing the users of the benchmark to create complex task-specific model architectures.\n",
       "Nevertheless, while GLUE is restricted to tasks involving single sentence or sentence pair\n",
       "inputs, for SuperGLUE we expand the scope to consider tasks with longer inputs. This\n",
       "yields a set of tasks that requires understanding individual tokens in context, complete\n",
       "sentences, inter-sentence relations, and entire paragraphs.\n",
       "\n",
       "\n",
       "_•_ **License:** We require that task data be available under licences that allow use and redistribution for research purposes.\n",
       "\n",
       "\n",
       "To identify possible tasks for SuperGLUE, we disseminated a public call for task proposals to the\n",
       "NLP community, and received approximately 30 proposals. We filtered these proposals according\n",
       "to our criteria. Many proposals were not suitable due to licensing issues, complex formats, and\n",
       "insufficient headroom; we provide examples of such tasks in Appendix D. For each of the remaining\n",
       "tasks, we ran a BERT-based baseline and a human baseline, and filtered out tasks which were either\n",
       "too challenging for humans without extensive training or too easy for our machine baselines.\n",
       "\n",
       "\n",
       "**3.2** **Selected Tasks**\n",
       "\n",
       "\n",
       "Following this process, we arrived at eight tasks to use in SuperGLUE. See Tables 1 and 2 for details\n",
       "and specific examples of each task.\n",
       "\n",
       "\n",
       "**BoolQ** (Boolean Questions, Clark et al., 2019) is a QA task where each example consists of a short\n",
       "passage and a yes/no question about the passage. The questions are provided anonymously and\n",
       "unsolicited by users of the Google search engine, and afterwards paired with a paragraph from a\n",
       "Wikipedia article containing the answer. Following the original work, we evaluate with accuracy.\n",
       "\n",
       "\n",
       "**CB** (CommitmentBank, De Marneffe et al., 2019) is a corpus of short texts in which at least one\n",
       "sentence contains an embedded clause. Each of these embedded clauses is annotated with the degree\n",
       "to which it appears the person who wrote the text is _committed_ to the truth of the clause. The resulting\n",
       "task framed as three-class textual entailment on examples that are drawn from the Wall Street Journal,\n",
       "fiction from the British National Corpus, and Switchboard. Each example consists of a premise\n",
       "containing an embedded clause and the corresponding hypothesis is the extraction of that clause.\n",
       "We use a subset of the data that had inter-annotator agreement above 80% . The data is imbalanced\n",
       "(relatively fewer _neutral_ examples), so we evaluate using accuracy and F1, where for multi-class F1\n",
       "we compute the unweighted average of the F1 per class.\n",
       "\n",
       "\n",
       "4\n",
       "\n",
       "\n",
       "Table 2: Development set examples from the tasks in SuperGLUE. **Bold** text represents part of the\n",
       "example format for each task. Text in _italics_ is part of the model input. _Underlined_ text is specially\n",
       "marked in the input. Text in a `monospaced font` represents the expected model output.\n",
       "\n",
       "\n",
       "**Passage:** _Barq’s – Barq’s is an American soft drink. Its brand of root beer is notable for having caffeine._\n",
       "_Barq’s, created by Edward Barq and bottled since the turn of the 20th century, is owned by the Barq_\n",
       "_family but bottled by the Coca-Cola Company. It was known as Barq’s Famous Olde Tyme Root Beer_\n",
       "_until 2012._\n",
       "**Question:** _is barq’s root beer a pepsi product_ **Answer:** `No`\n",
       "\n",
       "\n",
       "**Text:** _B: And yet, uh, I we-, I hope to see employer based, you know, helping out. You know, child, uh,_\n",
       "_care centers at the place of employment and things like that, that will help out. A: Uh-huh. B: What do_\n",
       "_you think, do you think we are, setting a trend?_\n",
       "**Hypothesis:** _they are setting a trend_ **Entailment:** `Unknown`\n",
       "\n",
       "\n",
       "**Premise:** _My body cast a shadow over the grass._ **Question:** _What’s the CAUSE for this?_\n",
       "**Alternative 1:** _The sun was rising._ **Alternative 2:** _The grass was cut._\n",
       "**Correct Alternative:** `1`\n",
       "\n",
       "\n",
       "**Paragraph:** _Susan wanted to have a birthday party. She called all of her friends. She has five friends._\n",
       "_Her mom said that Susan can invite them all to the party. Her first friend could not go to the party_\n",
       "_because she was sick. Her second friend was going out of town. Her third friend was not so sure if her_\n",
       "_parents would let her. The fourth friend said maybe. The fifth friend could go to the party for sure. Susan_\n",
       "_was a little sad. On the day of the party, all five friends showed up. Each friend had a present for Susan._\n",
       "_Susan was happy and sent each friend a thank you card the next week_\n",
       "**Question:** _Did Susan’s sick friend recover?_ **Candidate answers:** _Yes, she recovered_ ( `T` ), _No_ ( `F` ), _Yes_\n",
       "( `T` ), _No, she didn’t recover_ ( `F` ), _Yes, she was at Susan’s party_ ( `T` )\n",
       "\n",
       "\n",
       "**Paragraph:** _(_ _CNN_ _)_ _Puerto Rico_ _on Sunday overwhelmingly voted for statehood. But Congress, the only_\n",
       "_body that can approve new states, will ultimately decide whether the status of the_ _US_ _commonwealth_\n",
       "_changes. Ninety-seven percent of the votes in the nonbinding referendum favored statehood, an increase_\n",
       "_over the results of a 2012 referendum, official results from the_ _State Electorcal Commission_ _show. It_\n",
       "_was the fifth such vote on statehood. \"Today, we the people of_ _Puerto Rico_ _are sending a strong and_\n",
       "_clear message to the_ _US Congress_ _... and to the world ... claiming our equal rights as_ _American_ _citizens,_\n",
       "_Puerto Rico_ _Gov._ _Ricardo Rossello_ _said in a news release. @highlight_ _Puerto Rico_ _voted Sunday in_\n",
       "_favor of US statehood_\n",
       "**Query** For one, they can truthfully say, “Don’t blame me, I didn’t vote for them, ” when discussing the\n",
       "<placeholder> presidency **Correct Entities:** `US`\n",
       "\n",
       "\n",
       "**Text:** _Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44,_\n",
       "_according to the Christopher Reeve Foundation._\n",
       "**Hypothesis:** _Christopher Reeve had an accident._ **Entailment:** `False`\n",
       "\n",
       "\n",
       "**Context 1:** _Room and board._ **Context 2:** _He nailed boards across the windows._\n",
       "\n",
       "**Sense match:** `False`\n",
       "\n",
       "\n",
       "**Text:** _Mark told_ _Pete_ _many lies about himself, which Pete included in his book._ _He_ _should have been_\n",
       "_more truthful._ **Coreference:** `False`\n",
       "\n",
       "\n",
       "**COPA** (Choice of Plausible Alternatives, Roemmele et al., 2011) is a causal reasoning task in which\n",
       "a system is given a premise sentence and must determine either the cause or effect of the premise\n",
       "from two possible choices. All examples are handcrafted and focus on topics from blogs and a\n",
       "photography-related encyclopedia. Following the original work, we evaluate using accuracy.\n",
       "\n",
       "\n",
       "**MultiRC** (Multi-Sentence Reading Comprehension, Khashabi et al., 2018) is a QA task where each\n",
       "example consists of a context paragraph, a question about that paragraph, and a list of possible\n",
       "answers. The system must predict which answers are true and which are false. While many QA\n",
       "tasks exist, we use MultiRC because of a number of desirable properties: (i) each question can have\n",
       "multiple possible correct answers, so each question-answer pair must be evaluated independent of\n",
       "other pairs, (ii) the questions are designed such that answering each question requires drawing facts\n",
       "from multiple context sentences, and (iii) the question-answer pair format more closely matches\n",
       "the API of other tasks in SuperGLUE than the more popular span-extractive QA format does. The\n",
       "paragraphs are drawn from seven domains including news, fiction, and historical text. The evaluation\n",
       "metrics are F1 over all answer-options (F1 _a_ ) and exact match of each question’s set of answers (EM).\n",
       "\n",
       "\n",
       "5\n",
       "\n",
       "\n",
       "**ReCoRD** (Reading Comprehension with Commonsense Reasoning Dataset, Zhang et al., 2018) is a\n",
       "multiple-choice QA task. Each example consists of a news article and a Cloze-style question about\n",
       "the article in which one entity is masked out. The system must predict the masked out entity from a\n",
       "given list of possible entities in the provided passage, where the same entity may be expressed using\n",
       "multiple different surface forms, all of which are considered correct. Articles are drawn from CNN\n",
       "and Daily Mail. Following the original work, we evaluate with max (over all mentions) token-level\n",
       "F1 and exact match (EM).\n",
       "\n",
       "\n",
       "**RTE** (Recognizing Textual Entailment) datasets come from a series of annual competitions on textual\n",
       "entailment. [2] RTE is included in GLUE, and we use the same data and format as GLUE: We merge data\n",
       "from RTE1 (Dagan et al., 2006), RTE2 (Bar Haim et al., 2006), RTE3 (Giampiccolo et al., 2007), and\n",
       "RTE5 (Bentivogli et al., 2009). [3] All datasets are combined and converted to two-class classification:\n",
       "_entailment_ and _not_entailment_ . Of all the GLUE tasks, RTE is among those that benefits from\n",
       "\n",
       "_∼_\n",
       "transfer learning the most, with performance jumping from near random-chance ( 56%) at the time\n",
       "of GLUE’s launch to 86.3% accuracy (Liu et al., 2019d; Yang et al., 2019) at the time of writing.\n",
       "Given the nearly eight point gap with respect to human performance, however, the task is not yet\n",
       "solved by machines, and we expect the remaining gap to be difficult to close.\n",
       "\n",
       "\n",
       "**WiC** (Word-in-Context, Pilehvar and Camacho-Collados, 2019) is a word sense disambiguation task\n",
       "cast as binary classification of sentence pairs. Given two text snippets and a polysemous word that\n",
       "appears in both sentences, the task is to determine whether the word is used with the same sense in\n",
       "both sentences. Sentences are drawn from WordNet (Miller, 1995), VerbNet (Schuler, 2005), and\n",
       "Wiktionary. We follow the original work and evaluate using accuracy.\n",
       "\n",
       "\n",
       "**WSC** (Winograd Schema Challenge, Levesque et al., 2012) is a coreference resolution task in\n",
       "which examples consist of a sentence with a pronoun and a list of noun phrases from the sentence.\n",
       "The system must determine the correct referrent of the pronoun from among the provided choices.\n",
       "Winograd schemas are designed to require everyday knowledge and commonsense reasoning to solve.\n",
       "\n",
       "\n",
       "GLUE includes a version of WSC recast as NLI, known as WNLI. Until very recently, no substantial\n",
       "progress had been made on WNLI, with many submissions opting to submit majority class predictions. [4] In the past few months, several works (Kocijan et al., 2019; Liu et al., 2019d) have made rapid\n",
       "progress via a hueristic data augmentation scheme, raising machine performance to 90.4% accuracy.\n",
       "Given estimated human performance of _∼_ 96%, there is still a gap between machine and human\n",
       "performance, which we expect will be relatively difficult to close. We therefore include a version of\n",
       "WSC cast as binary classification, where each example consists of a sentence with a marked pronoun\n",
       "and noun, and the task is to determine if the pronoun refers to that noun. The training and validation\n",
       "examples are drawn from the original WSC data (Levesque et al., 2012), as well as those distributed\n",
       "by the affiliated organization _Commonsense Reasoning_ . [5] The test examples are derived from fiction\n",
       "books and have been shared with us by the authors of the original dataset. We evaluate using accuracy.\n",
       "\n",
       "\n",
       "**3.3** **Scoring**\n",
       "\n",
       "\n",
       "As with GLUE, we seek to give a sense of aggregate system performance over all tasks by averaging\n",
       "scores of all tasks. Lacking a fair criterion with which to weight the contributions of each task to\n",
       "the overall score, we opt for the simple approach of weighing each task equally, and for tasks with\n",
       "multiple metrics, first averaging those metrics to get a task score.\n",
       "\n",
       "\n",
       "**3.4** **Tools for Model Analysis**\n",
       "\n",
       "\n",
       "**Analyzing Linguistic and World Knowledge in Models** GLUE includes an expert-constructed,\n",
       "diagnostic dataset that automatically tests models for a broad range of linguistic, commonsense, and\n",
       "world knowledge. Each example in this broad-coverage diagnostic is a sentence pair labeled with\n",
       "\n",
       "\n",
       "2 Textual entailment is also known as natural language inference, or NLI\n",
       "3 RTE4 is not publicly available, while RTE6 and RTE7 do not conform to the standard NLI task.\n",
       "4 WNLI is especially difficult due to an adversarial train/dev split: Premise sentences that appear in the\n",
       "training set often appear in the development set with a different hypothesis and a flipped label. If a system\n",
       "memorizes the training set, which was easy due to the small size of the training set, it could perform far _below_\n",
       "chance on the development set. We remove this adversarial design in our version of WSC by ensuring that no\n",
       "sentences are shared between the training, validation, and test sets.\n",
       "5 `[http://commonsensereasoning.org/disambiguation.html](http://commonsensereasoning.org/disambiguation.html)`\n",
       "\n",
       "\n",
       "6\n",
       "\n",
       "\n",
       "a three-way entailment relation ( _entailment_, _neutral_, or _contradiction_ ) and tagged with labels that\n",
       "indicate the phenomena that characterize the relationship between the two sentences. Submissions\n",
       "to the GLUE leaderboard are required to include predictions from the submission’s MultiNLI\n",
       "classifier on the diagnostic dataset, and analyses of the results were shown alongside the main\n",
       "leaderboard. Since this broad-coverage diagnostic task has proved difficult for top models, we retain\n",
       "it in SuperGLUE. However, since MultiNLI is not part of SuperGLUE, we collapse _contradiction_\n",
       "and _neutral_ into a single _not_entailment_ label, and request that submissions include predictions\n",
       "on the resulting set from the model used for the _RTE_ task. We collect non-expert annotations to\n",
       "estimate human performance, following the same procedure we use for the main benchmark tasks\n",
       "(Section 5.2). We estimate an accuracy of 88% and a Matthew’s correlation coefficient (MCC, the\n",
       "two-class variant of the _R_ 3 metric used in GLUE) of 0.77.\n",
       "\n",
       "\n",
       "**Analyzing Gender Bias in Models** Recent work has identified the presence and amplification\n",
       "of many social biases in data-driven machine learning models. (Lu et al., 2018; Zhao et al., 2018;\n",
       "Kiritchenko and Mohammad, 2018). To promote the detection of such biases, we include Winogender\n",
       "(Rudinger et al., 2018) as an additional diagnostic dataset. Winogender is designed to measure gender\n",
       "bias in coreference resolution systems. We use the Diverse Natural Language Inference Collection\n",
       "(DNC; Poliak et al., 2018) version that casts Winogender as a textual entailment task. [6] Each example\n",
       "consists of a premise sentence with a male or female pronoun and a hypothesis giving a possible\n",
       "antecedent of the pronoun. Examples occur in _minimal pairs_, where the only difference between\n",
       "an example and its pair is the gender of the pronoun in the premise. Performance on Winogender\n",
       "is measured with both accuracy and the _gender parity score_ : the percentage of minimal pairs for\n",
       "which the predictions are the same. We note that a system can trivially obtain a perfect gender parity\n",
       "score by guessing the same class for all examples, so a high gender parity score is meaningless unless\n",
       "accompanied by high accuracy. We collect non-expert annotations to estimate human performance,\n",
       "and observe an accuracy of 99.7% and a gender parity score of 0.99.\n",
       "\n",
       "\n",
       "Like any diagnostic, Winogender has limitations. It offers only positive predictive value: A poor\n",
       "bias score is clear evidence that a model exhibits gender bias, but a good score does not mean that\n",
       "the model is unbiased. More specifically, in the DNC version of the task, a low gender parity score\n",
       "means that a model’s prediction of textual entailment can be changed with a change in pronouns, all\n",
       "else equal. It is plausible that there are forms of bias that are relevant to target tasks of interest, but\n",
       "that do not surface in this setting (Gonen and Goldberg, 2019). In addition, Winogender does not\n",
       "cover all forms of social bias, or even all forms of gender. For instance, the version of the data used\n",
       "here offers no coverage of gender-neutral _they_ or non-binary pronouns. Despite these limitations, we\n",
       "believe that Winogender’s inclusion is worthwhile in providing a coarse sense of how social biases\n",
       "evolve with model performance and for keeping attention on the social ramifications of NLP models.\n",
       "\n",
       "\n",
       "**4** **Using SuperGLUE**\n",
       "\n",
       "\n",
       "**Software Tools** To facilitate using SuperGLUE, we release `jiant` (Wang et al., 2019b), [7] a modular\n",
       "software toolkit, built with PyTorch (Paszke et al., 2017), components from AllenNLP (Gardner\n",
       "et al., 2017), and the `pytorch-pretrained-bert` package. [8] `jiant` implements our baselines and\n",
       "supports the evaluation of custom models and training methods on the benchmark tasks. The toolkit\n",
       "includes support for existing popular pretrained models such as OpenAI GPT and BERT, as well as\n",
       "support for multistage and multitask learning of the kind seen in the strongest models on GLUE.\n",
       "\n",
       "\n",
       "**Eligibility** Any system or method that can produce predictions for the SuperGLUE tasks is eligible\n",
       "for submission to the leaderboard, subject to the data-use and submission frequency policies stated\n",
       "immediately below. There are no restrictions on the type of methods that may be used, and there is\n",
       "no requirement that any form of parameter sharing or shared initialization be used across the tasks in\n",
       "the benchmark. To limit overfitting to the private test data, users are limited to a maximum of two\n",
       "submissions per day and six submissions per month.\n",
       "\n",
       "\n",
       "6 We filter out 23 examples where the labels are ambiguous\n",
       "7 `[https://github.com/nyu-mll/jiant](https://github.com/nyu-mll/jiant)`\n",
       "8 `[https://github.com/huggingface/pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT)`\n",
       "\n",
       "\n",
       "7\n",
       "\n",
       "\n",
       "Table 3: Baseline performance on the SuperGLUE test sets and diagnostics. For CB we report\n",
       "accuracy and macro-average F1. For MultiRC we report F1 on all answer-options and exact match\n",
       "of each question’s set of correct answers. AX _b_ is the broad-coverage diagnostic task, scored using\n",
       "Matthews’ correlation (MCC). AX _g_ is the Winogender diagnostic, scored using accuracy and the\n",
       "gender parity score (GPS). All values are scaled by 100. The _Avg_ column is the overall benchmark\n",
       "score on non-AX _∗_ tasks. The bolded numbers reflect the best machine performance on task. *MultiRC\n",
       "has multiple test sets released on a staggered schedule, and these results evaluate on an installation of\n",
       "the test set that is a subset of ours.\n",
       "\n",
       "\n",
       "**Model** **Avg BoolQ** **CB** **COPA** **MultiRC** **ReCoRD RTE WiC WSC** **AX** _b_ **AX** _g_\n",
       "**Metrics** **Acc.** **F1/Acc.** **Acc.** **F1** _a_ **/EM** **F1/EM** **Acc. Acc.** **Acc.** **MCC GPS Acc.**\n",
       "\n",
       "\n",
       "Most Frequent 47.1 62.3 21.7/48.4 50.0 61.1 / 0.3 33.4/32.5 50.3 50.0 65.1 0.0 100.0/ 50.0\n",
       "CBoW 44.3 62.1 49.0/71.2 51.6 0.0 / 0.4 14.0/13.6 49.7 53.0 65.1 -0.4 100.0/ 50.0\n",
       "\n",
       "BERT 69.0 77.4 75.7/83.6 70.6 70.0 / 24.0 72.0/71.3 71.6 **69.5** **64.3** 23.0 97.8 / 51.7\n",
       "\n",
       "BERT++ **71.5** 79.0 **84.7** / **90.4** 73.8 70.0 / 24.1 72.0/71.3 79.0 **69.5** **64.3** 38.0 99.4 / 51.4\n",
       "\n",
       "Outside Best  - **80.4**  - / - **84.4** **70.4** */ **24.5**  - **74.8** / **73.0 82.7**  -  -  -  - /  \n",
       "\n",
       "Human (est.) 89.8 89.0 95.8/98.9 100.0 81.8*/51.9* 91.7/91.3 93.6 80.0 100.0 77.0 99.3 / 99.7\n",
       "\n",
       "\n",
       "**Data** Data for the tasks are available for download through the SuperGLUE site and through a\n",
       "download script included with the software toolkit. Each task comes with a standardized training set,\n",
       "development set, and _unlabeled_ test set. Submitted systems may use any public or private data when\n",
       "developing their systems, with a few exceptions: Systems may only use the SuperGLUE-distributed\n",
       "versions of the task datasets, as these use different train/validation/test splits from other public\n",
       "versions in some cases. Systems also may not use the unlabeled test data for the tasks in system\n",
       "development in any way, may not use the structured source data that was used to collect the WiC\n",
       "labels (sense-annotated example sentences from WordNet, VerbNet, and Wiktionary) in any way, and\n",
       "may not build systems that share information across separate _test_ examples in any way.\n",
       "\n",
       "\n",
       "We do not endorse the use of the benchmark data for _non-research_ applications, due to concerns\n",
       "about socially relevant biases (such as ethnicity–occupation associations) that may be undesirable\n",
       "or legally problematic in deployed systems. Because these biases are evident in texts from a wide\n",
       "variety of sources and collection methods (e.g., Rudinger et al., 2017), and because none of our task\n",
       "datasets directly mitigate them, one can reasonably presume that our training sets teach models these\n",
       "biases to some extent and that our evaluation sets similarly _reward_ models that learn these biases.\n",
       "\n",
       "\n",
       "To ensure reasonable credit assignment, because we build very directly on prior work, we ask the\n",
       "authors of submitted systems to directly name and cite the specific datasets that they use, _including the_\n",
       "_benchmark datasets_ . We will enforce this as a requirement for papers to be listed on the leaderboard.\n",
       "\n",
       "\n",
       "**5** **Experiments**\n",
       "\n",
       "\n",
       "**5.1** **Baselines**\n",
       "\n",
       "\n",
       "**BERT** Our main baselines are built around BERT, variants of which are among the most successful\n",
       "approach on GLUE at the time of writing. Specifically, we use the `bert-large-cased` variant.\n",
       "Following the practice recommended in Devlin et al. (2019), for each task, we use the simplest\n",
       "possible architecture on top of BERT. We fine-tune a copy of the pretrained BERT model separately\n",
       "for each task, and leave the development of multi-task learning models to future work. For training,\n",
       "we use the procedure specified in Devlin et al. (2019): We use Adam (Kingma and Ba, 2014) with an\n",
       "initial learning rate of 10 _[−]_ [5] and fine-tune for a maximum of 10 epochs.\n",
       "\n",
       "\n",
       "For classification tasks with sentence-pair inputs (BoolQ, CB, RTE, WiC), we concatenate the\n",
       "sentences with a [ SEP ] token, feed the fused input to BERT, and use a logistic regression classifier that\n",
       "sees the representation corresponding to [ CLS ]. For WiC only, we also concatenate the representation\n",
       "of the marked word to the [ CLS ] representation. For COPA, MultiRC, and ReCoRD, for each answer\n",
       "choice, we similarly concatenate the context with that answer choice and feed the resulting sequence\n",
       "into BERT to produce an answer representation. For COPA, we project these representations into a\n",
       "scalar, and take as the answer the choice with the highest associated scalar. For MultiRC, because\n",
       "each question can have more than one correct answer, we feed each answer representation into\n",
       "\n",
       "\n",
       "8\n",
       "\n",
       "\n",
       "a logistic regression classifier. For ReCoRD, we also evaluate the probability of each candidate\n",
       "independent of other candidates, and take the most likely candidate as the model’s prediction. For\n",
       "WSC, which is a span-based task, we use a model inspired by Tenney et al. (2019). Given the BERT\n",
       "representation for each word in the original sentence, we get span representations of the pronoun\n",
       "and noun phrase via a self-attention span-pooling operator (Lee et al., 2017), before feeding it into a\n",
       "logistic regression classifier.\n",
       "\n",
       "\n",
       "**BERT++** We also report results using BERT with additional training on related datasets before\n",
       "fine-tuning on the benchmark tasks, following the STILTs two-stage style of transfer learning (Phang\n",
       "et al., 2018). Given the productive use of MultiNLI in pretraining and intermediate fine-tuning of\n",
       "pretrained language models (Conneau et al., 2017; Phang et al., 2018, i.a.), for CB, RTE, and BoolQ,\n",
       "we use MultiNLI as a transfer task by first using the above procedure on MultiNLI. Similarly, given\n",
       "the similarity of COPA to SWAG (Zellers et al., 2018), we first fine-tune BERT on SWAG. These\n",
       "results are reported as BERT++. For all other tasks, we reuse the results of BERT fine-tuned on just\n",
       "that task.\n",
       "\n",
       "\n",
       "**Simple Baselines** We include a baseline where for each task we simply predict the majority class, [9]\n",
       "as well as a bag-of-words baseline where each input is represented as an average of its tokens’ GloVe\n",
       "word vectors (the 300D/840B release from Pennington et al., 2014).\n",
       "\n",
       "\n",
       "**Outside Best** We list the best known result on each task to date, except on tasks which we recast\n",
       "(WSC), resplit (CB), or achieve the best known result (WiC). The outside results for COPA, MultiRC,\n",
       "and RTE are from Sap et al. (2019), Trivedi et al. (2019), and Liu et al. (2019d) respectively.\n",
       "\n",
       "\n",
       "**5.2** **Human Performance**\n",
       "\n",
       "\n",
       "Pilehvar and Camacho-Collados (2019), Khashabi et al. (2018), Nangia and Bowman (2019), and\n",
       "Zhang et al. (2018) respectively provide estimates for human performance on WiC, MultiRC, RTE,\n",
       "and ReCoRD. For the remaining tasks, including the diagnostic set, we estimate human performance\n",
       "by hiring crowdworker annotators through Amazon’s Mechanical Turk platform to reannotate a\n",
       "sample of each test set. We follow a two step procedure where a crowd worker completes a short\n",
       "training phase before proceeding to the annotation phase, modeled after the method used by Nangia\n",
       "and Bowman (2019) for GLUE. For both phases and all tasks, the average pay rate is $23.75/hr. [10]\n",
       "\n",
       "\n",
       "In the training phase, workers are provided with instructions on the task, linked to an FAQ page, and\n",
       "are asked to annotate up to 30 examples from the development set. After answering each example,\n",
       "workers are also asked to check their work against the provided ground truth label. After the training\n",
       "phase is complete, we provide the qualification to work on the annotation phase to all workers\n",
       "who annotated a minimum of five examples, i.e. completed five HITs during training and achieved\n",
       "performance at, or above the median performance across all workers during training.\n",
       "\n",
       "\n",
       "In the annotation phase, workers are provided with the same instructions as the training phase, and\n",
       "are linked to the same FAQ page. The instructions for all tasks are provided in Appendix C. For the\n",
       "annotation phase we randomly sample 100 examples from the task’s test set, with the exception of\n",
       "WSC where we annotate the full test set. For each example, we collect annotations from five workers\n",
       "and take a majority vote to estimate human performance. For additional details, see Appendix C.3.\n",
       "\n",
       "\n",
       "**5.3** **Results**\n",
       "\n",
       "\n",
       "Table 3 shows results for all baselines. The simple baselines of predicting the most frequent class\n",
       "and CBOW do not perform well overall, achieving near chance performance for several of the tasks.\n",
       "Using BERT increases the average SuperGLUE score by 25 points, attaining significant gains on\n",
       "all of the benchmark tasks, particularly MultiRC, ReCoRD, and RTE. On WSC, BERT actually\n",
       "performs worse than the simple baselines, likely due to the small size of the dataset and the lack of\n",
       "data augmentation. Using MultiNLI as an additional source of supervision for BoolQ, CB, and RTE\n",
       "leads to a 2-5 point improvement on all tasks. Using SWAG as a transfer task for COPA sees an 8\n",
       "point improvement.\n",
       "\n",
       "\n",
       "9 For ReCoRD, we predict the entity that has the highest F1 with the other entity options.\n",
       "10 This estimate is taken from `[https://turkerview.com](https://turkerview.com)` .\n",
       "\n",
       "\n",
       "9\n",
       "\n",
       "\n",
       "Our best baselines still lag substantially behind human performance. On average, there is a nearly 20\n",
       "point gap between BERT++ and human performance. The largest gap is on WSC, with a 35 point\n",
       "difference between the best model and human performance. The smallest margins are on BoolQ,\n",
       "CB, RTE, and WiC, with gaps of around 10 points on each of these. We believe these gaps will be\n",
       "challenging to close: On WSC and COPA, human performance is perfect. On three other tasks, it is\n",
       "in the mid-to-high 90s. On the diagnostics, all models continue to lag significantly behind humans.\n",
       "Though all models obtain near perfect gender parity scores on Winogender, this is due to the fact that\n",
       "they are obtaining accuracy near that of random guessing.\n",
       "\n",
       "\n",
       "**6** **Conclusion**\n",
       "\n",
       "\n",
       "We present SuperGLUE, a new benchmark for evaluating general-purpose language understanding\n",
       "systems. SuperGLUE updates the GLUE benchmark by identifying a new set of challenging NLU\n",
       "tasks, as measured by the difference between human and machine baselines. The set of eight tasks in\n",
       "our benchmark emphasizes diverse task formats and low-data training data tasks, with nearly half the\n",
       "tasks having fewer than 1k examples and all but one of the tasks having fewer than 10k examples.\n",
       "\n",
       "\n",
       "We evaluate BERT-based baselines and find that they still lag behind humans by nearly 20 points.\n",
       "Given the difficulty of SuperGLUE for BERT, we expect that further progress in multi-task, transfer,\n",
       "and unsupervised/self-supervised learning techniques will be necessary to approach human-level performance on the benchmark. Overall, we argue that SuperGLUE offers a rich and challenging testbed\n",
       "for work developing new general-purpose machine learning methods for language understanding.\n",
       "\n",
       "\n",
       "**7** **Acknowledgments**\n",
       "\n",
       "\n",
       "We thank the original authors of the included datasets in SuperGLUE for their cooperation in the\n",
       "creation of the benchmark, as well as those who proposed tasks and datasets that we ultimately could\n",
       "not include.\n",
       "\n",
       "\n",
       "This work was made possible in part by a donation to NYU from Eric and Wendy Schmidt made by\n",
       "recommendation of the Schmidt Futures program. We gratefully acknowledge the support of the\n",
       "NVIDIA Corporation with the donation of a Titan V GPU used at NYU for this research. AW is\n",
       "supported by the National Science Foundation Graduate Research Fellowship Program under Grant\n",
       "No. DGE 1342536. Any opinions, findings, and conclusions or recommendations expressed in this\n",
       "material are those of the author(s) and do not necessarily reflect the views of the National Science\n",
       "Foundation.\n",
       "\n",
       "\n",
       "**References**\n",
       "\n",
       "\n",
       "Anonymous. Bam! Born-again multi-task networks for natural language understanding. Anonymous\n",
       "preprint under review, 2018. URL `[https://openreview.net/forum?id=SylnYlqKw4](https://openreview.net/forum?id=SylnYlqKw4)` .\n",
       "\n",
       "\n",
       "Stephen H. Bach, Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao, Cassandra Xia, Souvik\n",
       "Sen, Alexander Ratner, Braden Hancock, Houman Alborzi, Rahul Kuchhal, Christopher Ré, and\n",
       "Rob Malkin. Snorkel drybell: A case study in deploying weak supervision at industrial scale. In\n",
       "_SIGMOD_ . ACM, 2018.\n",
       "\n",
       "\n",
       "Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and\n",
       "Idan Szpektor. The second PASCAL recognising textual entailment challenge. In _Proceedings_\n",
       "_of the Second PASCAL Challenges Workshop on Recognising Textual Entailment_, 2006. URL\n",
       "`[http://u.cs.biu.ac.il/~nlp/RTE2/Proceedings/01.pdf](http://u.cs.biu.ac.il/~nlp/RTE2/Proceedings/01.pdf)` .\n",
       "\n",
       "\n",
       "Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. The\n",
       "fifth PASCAL recognizing textual entailment challenge. In _Textual Analysis Conference (TAC)_,\n",
       "2009. URL `[http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.232.1231](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.232.1231)` .\n",
       "\n",
       "\n",
       "Sven Buechel, Anneke Buffone, Barry Slaff, Lyle Ungar, and João Sedoc. Modeling empathy and\n",
       "distress in reaction to news stories. In _Proceedings of the 2018 Conference on Empirical Methods_\n",
       "_in Natural Language Processing (EMNLP)_, 2018.\n",
       "\n",
       "\n",
       "10\n",
       "\n",
       "\n",
       "Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluation the role of bleu in machine\n",
       "translation research. In _Proceedings of the Conference of the European Chapter of the Association_\n",
       "_for Computational Linguistics (EACL)_ . Association for Computational Linguistics, 2006. URL\n",
       "`[https://www.aclweb.org/anthology/E06-1032](https://www.aclweb.org/anthology/E06-1032)` .\n",
       "\n",
       "\n",
       "Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task\n",
       "1: Semantic textual similarity multilingual and crosslingual focused evaluation. In _Proceedings_\n",
       "_of the 11th International Workshop on Semantic Evaluation (SemEval-2017)_ . Association for\n",
       "Computational Linguistics, 2017. doi: 10.18653/v1/S17-2001. URL `[https://www.aclweb.](https://www.aclweb.org/anthology/S17-2001)`\n",
       "`[org/anthology/S17-2001](https://www.aclweb.org/anthology/S17-2001)` .\n",
       "\n",
       "\n",
       "Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke\n",
       "Zettlemoyer. QuAC: Question answering in context. In _Proceedings of the 2018 Conference on_\n",
       "_Empirical Methods in Natural Language Processing (EMNLP)_ . Association for Computational\n",
       "Linguistics, 2018a.\n",
       "\n",
       "\n",
       "Eunsol Choi, Omer Levy, Yejin Choi, and Luke Zettlemoyer. Ultra-fine entity typing. In _Proceedings_\n",
       "_of the Association for Computational Linguistics (ACL)_ . Association for Computational Linguistics,\n",
       "2018b. URL `[https://www.aclweb.org/anthology/P18-1009](https://www.aclweb.org/anthology/P18-1009)` .\n",
       "\n",
       "\n",
       "Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\n",
       "Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In _Proceedings_\n",
       "_of the 2019 Conference of the North American Chapter of the Association for Computational_\n",
       "_Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 2924–2936,\n",
       "2019.\n",
       "\n",
       "\n",
       "Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep\n",
       "neural networks with multitask learning. In _Proceedings of the 25th International Conference on_\n",
       "_Machine Learning (ICML)_ . Association for Computing Machinery, 2008. URL `[https://dl.acm.](https://dl.acm.org/citation.cfm?id=1390177)`\n",
       "`[org/citation.cfm?id=1390177](https://dl.acm.org/citation.cfm?id=1390177)` .\n",
       "\n",
       "\n",
       "Alexis Conneau and Douwe Kiela. SentEval: An evaluation toolkit for universal sentence representations. In _Proceedings of the 11th Language Resources and Evaluation Conference_ . European Language Resource Association, 2018. URL `[https://www.aclweb.org/anthology/L18-1269](https://www.aclweb.org/anthology/L18-1269)` .\n",
       "\n",
       "\n",
       "Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes. Supervised learning of universal sentence representations from natural language inference data. In\n",
       "_Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing_\n",
       "_(EMNLP)_ . Association for Computational Linguistics, 2017. doi: 10.18653/v1/D17-1070. URL\n",
       "\n",
       "`[https://www.aclweb.org/anthology/D17-1070](https://www.aclweb.org/anthology/D17-1070)` .\n",
       "\n",
       "\n",
       "Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment challenge. In _Machine Learning Challenges. Evaluating Predictive Uncertainty, Vi-_\n",
       "_sual Object Classification, and Recognising Textual Entailment_ . Springer, 2006. URL `[https:](https://link.springer.com/chapter/10.1007/11736790_9)`\n",
       "`[//link.springer.com/chapter/10.1007/11736790_9](https://link.springer.com/chapter/10.1007/11736790_9)` .\n",
       "\n",
       "\n",
       "Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In _Advances in Neural_\n",
       "_Information Processing Systems (NeurIPS)_ . Curran Associates, Inc., 2015. URL `[http://papers.](http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf)`\n",
       "`[nips.cc/paper/5949-semi-supervised-sequence-learning.pdf](http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf)` .\n",
       "\n",
       "\n",
       "Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The CommitmentBank:\n",
       "Investigating projection in naturally occurring discourse. 2019. To appear in _Proceedings of Sinn_\n",
       "_und Bedeutung 23_ . Data can be found at `[https://github.com/mcdm/CommitmentBank/](https://github.com/mcdm/CommitmentBank/)` .\n",
       "\n",
       "\n",
       "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\n",
       "bidirectional transformers for language understanding. In _Proceedings of the Conference of the_\n",
       "_North American Chapter of the Association for Computational Linguistics: Human Language_\n",
       "_Technologies (NAACL-HLT)_ . Association for Computational Linguistics, 2019. URL `[https:](https://arxiv.org/abs/1810.04805)`\n",
       "`[//arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)` .\n",
       "\n",
       "\n",
       "William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.\n",
       "In _Proceedings of IWP_, 2005.\n",
       "\n",
       "\n",
       "11\n",
       "\n",
       "\n",
       "Manaal Faruqui and Dipanjan Das. Identifying well-formed natural language questions. In _Pro-_\n",
       "_ceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)_ .\n",
       "Association for Computational Linguistics, 2018. URL `[https://www.aclweb.org/anthology/](https://www.aclweb.org/anthology/D18-1091)`\n",
       "`[D18-1091](https://www.aclweb.org/anthology/D18-1091)` .\n",
       "\n",
       "\n",
       "Tommaso Furlanello, Zachary C Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar.\n",
       "Born again neural networks. _International Conference on Machine Learning (ICML)_, 2018. URL\n",
       "`[http://proceedings.mlr.press/v80/furlanello18a.html](http://proceedings.mlr.press/v80/furlanello18a.html)` .\n",
       "\n",
       "\n",
       "Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew\n",
       "Peters, Michael Schmitz, and Luke S. Zettlemoyer. AllenNLP: A deep semantic natural language\n",
       "processing platform. In _Proceedings of Workshop for NLP Open Source Software_, 2017. URL\n",
       "`[https://www.aclweb.org/anthology/W18-2501](https://www.aclweb.org/anthology/W18-2501)` .\n",
       "\n",
       "\n",
       "Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing\n",
       "textual entailment challenge. In _Proceedings of the ACL-PASCAL Workshop on Textual Entailment_\n",
       "_and Paraphrasing_ . Association for Computational Linguistics, 2007.\n",
       "\n",
       "\n",
       "Hila Gonen and Yoav Goldberg. Lipstick on a pig: Debiasing methods cover up systematic\n",
       "gender biases in word embeddings but do not remove them. In _Proceedings of the 2019_\n",
       "_Conference of the North American Chapter of the Association for Computational Linguistics:_\n",
       "_Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 609–614, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. URL `[https:](https://www.aclweb.org/anthology/N19-1061)`\n",
       "`[//www.aclweb.org/anthology/N19-1061](https://www.aclweb.org/anthology/N19-1061)` .\n",
       "\n",
       "\n",
       "Felix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences\n",
       "from unlabelled data. In _Proceedings of the Conference of the North American Chapter of_\n",
       "_the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)_ .\n",
       "Association for Computational Linguistics, 2016. doi: 10.18653/v1/N16-1162. URL `[https:](https://www.aclweb.org/anthology/N16-1162)`\n",
       "`[//www.aclweb.org/anthology/N16-1162](https://www.aclweb.org/anthology/N16-1162)` .\n",
       "\n",
       "\n",
       "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. _arXiv_\n",
       "_preprint 1503.02531_, 2015. URL `[https://arxiv.org/abs/1503.02531](https://arxiv.org/abs/1503.02531)` .\n",
       "\n",
       "\n",
       "Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems. In\n",
       "_Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)_ .\n",
       "Association for Computational Linguistics, 2017. doi: 10.18653/v1/D17-1215. URL `[https:](https://www.aclweb.org/anthology/D17-1215)`\n",
       "`[//www.aclweb.org/anthology/D17-1215](https://www.aclweb.org/anthology/D17-1215)` .\n",
       "\n",
       "\n",
       "Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking\n",
       "beyond the surface: A challenge set for reading comprehension over multiple sentences. In\n",
       "_Proceedings of the Conference of the North American Chapter of the Association for Computa-_\n",
       "_tional Linguistics: Human Language Technologies (NAACL-HLT)_ . Association for Computational\n",
       "Linguistics, 2018. URL `[https://www.aclweb.org/anthology/papers/N/N18/N18-1023/](https://www.aclweb.org/anthology/papers/N/N18/N18-1023/)` .\n",
       "\n",
       "\n",
       "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint_\n",
       "_1412.6980_, 2014. URL `[https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)` .\n",
       "\n",
       "\n",
       "Svetlana Kiritchenko and Saif Mohammad. Examining gender and race bias in two hundred sentiment\n",
       "analysis systems. In _Proceedings of the Seventh Joint Conference on Lexical and Computational_\n",
       "_Semantics_ . Association for Computational Linguistics, 2018. doi: 10.18653/v1/S18-2005. URL\n",
       "`[https://www.aclweb.org/anthology/S18-2005](https://www.aclweb.org/anthology/S18-2005)` .\n",
       "\n",
       "\n",
       "Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba,\n",
       "and Sanja Fidler. Skip-thought vectors. In _Advances in neural information processing systems_,\n",
       "2015.\n",
       "\n",
       "\n",
       "Nikita Kitaev and Dan Klein. Multilingual constituency parsing with self-attention and pre-training.\n",
       "_arXiv preprint 1812.11760_, 2018. URL `[https://arxiv.org/abs/1812.11760](https://arxiv.org/abs/1812.11760)` .\n",
       "\n",
       "\n",
       "Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas Lukasiewicz.\n",
       "A surprisingly robust trick for winograd schema challenge. _arXiv preprint 1905.06290_, 2019.\n",
       "\n",
       "\n",
       "12\n",
       "\n",
       "\n",
       "Kenton Lee, Luheng He, Mike Lewis, and Luke Zettlemoyer. End-to-end neural coreference\n",
       "resolution. In _Proceedings of the 2017 Conference on Empirical Methods in Natural Language_\n",
       "_Processing_ . Association for Computational Linguistics, September 2017. doi: 10.18653/v1/\n",
       "D17-1018. URL `[https://www.aclweb.org/anthology/D17-1018](https://www.aclweb.org/anthology/D17-1018)` .\n",
       "\n",
       "\n",
       "Hector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge. In\n",
       "_Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning_,\n",
       "2012. URL `[http://dl.acm.org/citation.cfm?id=3031843.3031909](http://dl.acm.org/citation.cfm?id=3031843.3031909)` .\n",
       "\n",
       "\n",
       "Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, and Joelle Pineau.\n",
       "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics\n",
       "for dialogue response generation. In _Proceedings of the 2016 Conference on Empirical Methods in_\n",
       "_Natural Language Processing_ . Association for Computational Linguistics, 2016. doi: 10.18653/\n",
       "v1/D16-1230. URL `[https://www.aclweb.org/anthology/D16-1230](https://www.aclweb.org/anthology/D16-1230)` .\n",
       "\n",
       "\n",
       "Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. Linguistic\n",
       "knowledge and transferability of contextual representations. In _Proceedings of the Conference of_\n",
       "_the North American Chapter of the Association for Computational Linguistics: Human Language_\n",
       "_Technologies (NAACL-HLT)_ . Association for Computational Linguistics, 2019a. URL `[https:](https://arxiv.org/abs/1903.08855)`\n",
       "`[//arxiv.org/abs/1903.08855](https://arxiv.org/abs/1903.08855)` .\n",
       "\n",
       "\n",
       "Nelson F. Liu, Roy Schwartz, and Noah A. Smith. Inoculation by fine-tuning: A method for\n",
       "analyzing challenge datasets. In _Proceedings of the Conference of the North American Chapter_\n",
       "_of the Association for Computational Linguistics: Human Language Technologies (NAACL-_\n",
       "_HLT)_ . Association for Computational Linguistics, 2019b. URL `[https://arxiv.org/abs/1904.](https://arxiv.org/abs/1904.02668)`\n",
       "`[02668](https://arxiv.org/abs/1904.02668)` .\n",
       "\n",
       "\n",
       "Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Improving multi-task deep neural\n",
       "networks via knowledge distillation for natural language understanding. _arXiv preprint 1904.09482_,\n",
       "2019c. URL `[http://arxiv.org/abs/1904.09482](http://arxiv.org/abs/1904.09482)` .\n",
       "\n",
       "\n",
       "Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for\n",
       "natural language understanding. _arXiv preprint 1901.11504_, 2019d.\n",
       "\n",
       "\n",
       "Kaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Amancharla, and Anupam Datta. Gender bias in\n",
       "neural natural language processing. _arXiv preprint 1807.11714_, 2018. URL `[http://arxiv.org/](http://arxiv.org/abs/1807.11714)`\n",
       "`[abs/1807.11714](http://arxiv.org/abs/1807.11714)` .\n",
       "\n",
       "\n",
       "Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation: Contextualized word vectors. In _Advances in Neural Information Processing Sys-_\n",
       "_tems (NeurIPS)_ . Curran Associates, Inc., 2017. URL `[http://papers.nips.cc/paper/](http://papers.nips.cc/paper/7209-learned-in-translation-contextualized-word-vectors.pdf)`\n",
       "`[7209-learned-in-translation-contextualized-word-vectors.pdf](http://papers.nips.cc/paper/7209-learned-in-translation-contextualized-word-vectors.pdf)` .\n",
       "\n",
       "\n",
       "Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language\n",
       "decathlon: Multitask learning as question answering. _arXiv preprint 1806.08730_, 2018. URL\n",
       "`[https://arxiv.org/abs/1806.08730](https://arxiv.org/abs/1806.08730)` .\n",
       "\n",
       "\n",
       "R. Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic\n",
       "heuristics in natural language inference. In _Proceedings of the Association for Computational_\n",
       "_Linguistics (ACL)_ . Association for Computational Linguistics, 2019. URL `[https://arxiv.org/](https://arxiv.org/abs/1902.01007)`\n",
       "`[abs/1902.01007](https://arxiv.org/abs/1902.01007)` .\n",
       "\n",
       "\n",
       "Richard T. McCoy and Tal Linzen. Non-entailed subsequences as a challenge for natural language\n",
       "inference. In _Proceedings of the Society for Computational in Linguistics (SCiL) 2019_, 2019. URL\n",
       "`[https://scholarworks.umass.edu/scil/vol2/iss1/46/](https://scholarworks.umass.edu/scil/vol2/iss1/46/)` .\n",
       "\n",
       "\n",
       "George A Miller. WordNet: a lexical database for english. _Communications of the ACM_, 1995. URL\n",
       "\n",
       "`[https://www.aclweb.org/anthology/H94-1111](https://www.aclweb.org/anthology/H94-1111)` .\n",
       "\n",
       "\n",
       "Aakanksha Naik, Abhilasha Ravichander, Norman M. Sadeh, Carolyn Penstein Rosé, and Graham\n",
       "Neubig. Stress test evaluation for natural language inference. In _International Conference on_\n",
       "_Computational Linguistics (COLING)_, 2018.\n",
       "\n",
       "\n",
       "13\n",
       "\n",
       "\n",
       "Nikita Nangia and Samuel R. Bowman. Human vs. Muppet: A conservative estimate of human performance on the GLUE benchmark. In _Proceedings of the Association of Compu-_\n",
       "_tational Linguistics (ACL)_ . Association for Computational Linguistics, 2019. URL `[https:](https://woollysocks.github.io/assets/GLUE_Human_Baseline.pdf)`\n",
       "`[//woollysocks.github.io/assets/GLUE_Human_Baseline.pdf](https://woollysocks.github.io/assets/GLUE_Human_Baseline.pdf)` .\n",
       "\n",
       "\n",
       "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\n",
       "Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in\n",
       "PyTorch. In _Advances in Neural Information Processing Systems (NeurIPS)_ . Curran Associates,\n",
       "Inc., 2017. URL `[https://openreview.net/pdf?id=BJJsrmfCZ](https://openreview.net/pdf?id=BJJsrmfCZ)` .\n",
       "\n",
       "\n",
       "Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word\n",
       "representation. In _Proceedings of the Conference on Empirical Methods in Natural Language Pro-_\n",
       "_cessing (EMNLP)_ . Association for Computational Linguistics, 2014. doi: 10.3115/v1/D14-1162.\n",
       "URL `[https://www.aclweb.org/anthology/D14-1162](https://www.aclweb.org/anthology/D14-1162)` .\n",
       "\n",
       "\n",
       "Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\n",
       "Luke Zettlemoyer. Deep contextualized word representations. In _Proceedings of the Conference of_\n",
       "_the North American Chapter of the Association for Computational Linguistics: Human Language_\n",
       "_Technologies (NAACL-HLT)_ . Association for Computational Linguistics, 2018. doi: 10.18653/v1/\n",
       "N18-1202. URL `[https://www.aclweb.org/anthology/N18-1202](https://www.aclweb.org/anthology/N18-1202)` .\n",
       "\n",
       "\n",
       "Jason Phang, Thibault Févry, and Samuel R Bowman. Sentence encoders on STILTs: Supplementary\n",
       "training on intermediate labeled-data tasks. _arXiv preprint 1811.01088_, 2018. URL `[https:](https://arxiv.org/abs/1811.01088)`\n",
       "`[//arxiv.org/abs/1811.01088](https://arxiv.org/abs/1811.01088)` .\n",
       "\n",
       "\n",
       "Mohammad Taher Pilehvar and Jose Camacho-Collados. WiC: The word-in-context dataset for\n",
       "evaluating context-sensitive meaning representations. In _Proceedings of the Conference of the_\n",
       "_North American Chapter of the Association for Computational Linguistics: Human Language_\n",
       "_Technologies (NAACL-HLT)_ . Association for Computational Linguistics, 2019. URL `[https:](https://arxiv.org/abs/1808.09121)`\n",
       "`[//arxiv.org/abs/1808.09121](https://arxiv.org/abs/1808.09121)` .\n",
       "\n",
       "\n",
       "Adam Poliak, Aparajita Haldar, Rachel Rudinger, J. Edward Hu, Ellie Pavlick, Aaron Steven White,\n",
       "and Benjamin Van Durme. Collecting diverse natural language inference problems for sentence\n",
       "representation evaluation. In _Proceedings of the 2018 Conference on Empirical Methods in_\n",
       "_Natural Language Processing_ . Association for Computational Linguistics, 2018. URL `[https:](https://www.aclweb.org/anthology/D18-1007)`\n",
       "`[//www.aclweb.org/anthology/D18-1007](https://www.aclweb.org/anthology/D18-1007)` .\n",
       "\n",
       "\n",
       "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training, 2018. Unpublished ms. available through a link at\n",
       "`[https://blog.openai.com/language-unsupervised/](https://blog.openai.com/language-unsupervised/)` .\n",
       "\n",
       "\n",
       "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions\n",
       "for machine comprehension of text. In _Proceedings of the Conference on Empirical Methods in_\n",
       "_Natural Language Processing (EMNLP)_ . Association for Computational Linguistics, 2016. doi:\n",
       "10.18653/v1/D16-1264. URL `[http://aclweb.org/anthology/D16-1264](http://aclweb.org/anthology/D16-1264)` .\n",
       "\n",
       "\n",
       "Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S. Gordon. Choice of plausible alternatives:\n",
       "An evaluation of commonsense causal reasoning. In _2011 AAAI Spring Symposium Series_, 2011.\n",
       "\n",
       "\n",
       "Rachel Rudinger, Chandler May, and Benjamin Van Durme. Social bias in elicited natural language\n",
       "inferences. In _Proceedings of the First ACL Workshop on Ethics in Natural Language Processing_ .\n",
       "Association for Computational Linguistics, 2017. doi: 10.18653/v1/W17-1609. URL `[https:](https://www.aclweb.org/anthology/W17-1609)`\n",
       "`[//www.aclweb.org/anthology/W17-1609](https://www.aclweb.org/anthology/W17-1609)` .\n",
       "\n",
       "\n",
       "Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias in\n",
       "coreference resolution. In _Proceedings of the 2018 Conference of the North American Chapter_\n",
       "_of the Association for Computational Linguistics: Human Language Technologies_ . Association\n",
       "for Computational Linguistics, 2018. doi: 10.18653/v1/N18-2002. URL `[https://www.aclweb.](https://www.aclweb.org/anthology/N18-2002)`\n",
       "`[org/anthology/N18-2002](https://www.aclweb.org/anthology/N18-2002)` .\n",
       "\n",
       "\n",
       "Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense\n",
       "reasoning about social interactions. _arXiv preprint 1904.09728_, 2019. URL `[https://arxiv.](https://arxiv.org/abs/1904.09728)`\n",
       "`[org/abs/1904.09728](https://arxiv.org/abs/1904.09728)` .\n",
       "\n",
       "\n",
       "14\n",
       "\n",
       "\n",
       "Nathan Schneider and Noah A Smith. A corpus and model integrating multiword expressions and\n",
       "supersenses. In _Proceedings of the Conference of the North American Chapter of the Association_\n",
       "_for Computational Linguistics: Human Language Technologies (NAACL-HLT)_ . Association for\n",
       "Computational Linguistics, 2015. URL `[https://www.aclweb.org/anthology/N15-1177](https://www.aclweb.org/anthology/N15-1177)` .\n",
       "\n",
       "\n",
       "Karin Kipper Schuler. _Verbnet: A Broad-coverage, Comprehensive Verb Lexicon_ . PhD thesis, 2005.\n",
       "URL `[http://verbs.colorado.edu/~kipper/Papers/dissertation.pdf](http://verbs.colorado.edu/~kipper/Papers/dissertation.pdf)` .\n",
       "\n",
       "\n",
       "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng,\n",
       "and Christopher. Potts. Recursive deep models for semantic compositionality over a sentiment\n",
       "treebank. In _Proceedings of the Conference on Empirical Methods in Natural Language Processing_\n",
       "_(EMNLP)_ . Association for Computational Linguistics, 2013. URL `[https://www.aclweb.org/](https://www.aclweb.org/anthology/D13-1170)`\n",
       "`[anthology/D13-1170](https://www.aclweb.org/anthology/D13-1170)` .\n",
       "\n",
       "\n",
       "Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim,\n",
       "Benjamin Van Durme, Sam Bowman, Dipanjan Das, and Ellie Pavlick. What do you learn from\n",
       "context? probing for sentence structure in contextualized word representations. 2019. URL\n",
       "`[https://openreview.net/forum?id=SJzSgnRcKX](https://openreview.net/forum?id=SJzSgnRcKX)` .\n",
       "\n",
       "\n",
       "Harsh Trivedi, Heeyoung Kwon, Tushar Khot, Ashish Sabharwal, and Niranjan Balasubramanian.\n",
       "Repurposing entailment for multi-hop question answering tasks, 2019. URL `[https://arxiv.](https://arxiv.org/abs/1904.09380)`\n",
       "`[org/abs/1904.09380](https://arxiv.org/abs/1904.09380)` .\n",
       "\n",
       "\n",
       "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\n",
       "GLUE: A multi-task benchmark and analysis platform for natural language understanding. In\n",
       "_International Conference on Learning Representations_, 2019a. URL `[https://openreview.](https://openreview.net/forum?id=rJ4km2R5t7)`\n",
       "`[net/forum?id=rJ4km2R5t7](https://openreview.net/forum?id=rJ4km2R5t7)` .\n",
       "\n",
       "\n",
       "Alex Wang, Ian F. Tenney, Yada Pruksachatkun, Katherin Yu, Jan Hula, Patrick Xia, Raghu Pappagari,\n",
       "Shuning Jin, R. Thomas McCoy, Roma Patel, Yinghui Huang, Jason Phang, Edouard Grave,\n",
       "Najoung Kim, Phu Mon Htut, Thibault F’evry, Berlin Chen, Nikita Nangia, Haokun Liu,, Anhad\n",
       "Mohananey, Shikha Bordia, Ellie Pavlick, and Samuel R. Bowman. jiant 1.0: A software toolkit\n",
       "for research on general-purpose text understanding models. `[http://jiant.info/](http://jiant.info/)`, 2019b.\n",
       "\n",
       "\n",
       "Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments.\n",
       "_arXiv preprint 1805.12471_, 2018. URL `[https://arxiv.org/abs/1805.12471](https://arxiv.org/abs/1805.12471)` .\n",
       "\n",
       "\n",
       "Kellie Webster, Marta Recasens, Vera Axelrod, and Jason Baldridge. Mind the GAP: A balanced\n",
       "corpus of gendered ambiguous pronouns. _Transactions of the Association for Computational_\n",
       "_Linguistics (TACL)_, 2018. URL `[https://www.aclweb.org/anthology/Q18-1042](https://www.aclweb.org/anthology/Q18-1042)` .\n",
       "\n",
       "\n",
       "Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for\n",
       "sentence understanding through inference. In _Proceedings of the Conference of the North American_\n",
       "_Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-_\n",
       "_HLT)_ . Association for Computational Linguistics, 2018. URL `[http://aclweb.org/anthology/](http://aclweb.org/anthology/N18-1101)`\n",
       "`[N18-1101](http://aclweb.org/anthology/N18-1101)` .\n",
       "\n",
       "\n",
       "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V.\n",
       "Le. Xlnet: Generalized autoregressive pretraining for language understanding. _arXiv preprint_\n",
       "_1906.0823_, 2019.\n",
       "\n",
       "\n",
       "Fabio Massimo Zanzotto and Lorenzo Ferrone. Have you lost the thread? discovering ongoing\n",
       "conversations in scattered dialog blocks. _ACM Transactions on Interactive Intelligent Systems_\n",
       "_(TiiS)_, 2017.\n",
       "\n",
       "\n",
       "Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. SWAG: A large-scale adversarial dataset\n",
       "for grounded commonsense inference. 2018. URL `[https://www.aclweb.org/anthology/](https://www.aclweb.org/anthology/D18-1009)`\n",
       "`[D18-1009](https://www.aclweb.org/anthology/D18-1009)` .\n",
       "\n",
       "\n",
       "Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme.\n",
       "Record: Bridging the gap between human and machine commonsense reading comprehension.\n",
       "_arXiv preprint 1810.12885_, 2018.\n",
       "\n",
       "\n",
       "15\n",
       "\n",
       "\n",
       "Yuan Zhang, Jason Baldridge, and Luheng He. PAWS: Paraphrase adversaries from word scrambling.\n",
       "_arXiv preprint 1904.01130_, 2019. URL `[https://arxiv.org/abs/1904.01130](https://arxiv.org/abs/1904.01130)` .\n",
       "\n",
       "\n",
       "Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Gender bias in\n",
       "coreference resolution: Evaluation and debiasing methods. In _Proceedings of the 2018 Conference_\n",
       "_of the North American Chapter of the Association for Computational Linguistics: Human Language_\n",
       "_Technologies_ . Association for Computational Linguistics, 2018. doi: 10.18653/v1/N18-2003. URL\n",
       "\n",
       "`[https://www.aclweb.org/anthology/N18-2003](https://www.aclweb.org/anthology/N18-2003)` .\n",
       "\n",
       "\n",
       "16\n",
       "\n",
       "\n",
       "Table 4: Baseline performance on the SuperGLUE development.\n",
       "\n",
       "\n",
       "**Model** **Avg** **BoolQ** **CB** **COPA** **MultiRC** **ReCoRD** **RTE** **WiC** **WSC**\n",
       "**Metrics** **Acc.** **Acc./F1** **Acc.** **F1** _a_ **/EM** **F1/EM** **Acc.** **Acc.** **Acc.**\n",
       "\n",
       "\n",
       "Most Frequent Class 47.7 62.2 50 /22.2 55 59.9/ 0.8 32.4/31.5 52.7 50.0 63.5\n",
       "CBOW 47.7 62.4 71.4/49.6 63.0 20.3/ 0.3 14.4/13.8 54.2 55.3 61.5\n",
       "\n",
       "BERT 72.2 77.7 94.6/93.7 69.0 70.5/24.7 70.6/69.8 75.8 74.9 68.3\n",
       "\n",
       "BERT++ 74.6 80.1 96.4/95.0 78.0 70.5/24.7 70.6/69.8 82.3 74.9 68.3\n",
       "\n",
       "\n",
       "**A** **Development Set Results**\n",
       "\n",
       "\n",
       "In Table 4, we present results of the baselines on the SuperGLUE tasks development sets.\n",
       "\n",
       "\n",
       "**B** **Performance on GLUE Diagnostics**\n",
       "\n",
       "\n",
       "Figure 2 shows the performance on the GLUE diagnostics dataset for systems submitted to the public\n",
       "leaderboard.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "|80<br>60<br>40<br>20<br>0<br>20<br>40<br>60<br>Disjun|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|\n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "|~~Disju~~<br>60<br>40<br>20<br>0<br>20<br>40<br>60<br>80||||||||||\n",
       "|~~Disju~~<br>60<br>40<br>20<br>0<br>20<br>40<br>60<br>80||||||||||\n",
       "|~~Disju~~<br>60<br>40<br>20<br>0<br>20<br>40<br>60<br>80||||||||||\n",
       "|~~Disju~~<br>60<br>40<br>20<br>0<br>20<br>40<br>60<br>80||||||||||\n",
       "|~~Disju~~<br>60<br>40<br>20<br>0<br>20<br>40<br>60<br>80||||||||||\n",
       "|~~Disju~~<br>60<br>40<br>20<br>0<br>20<br>40<br>60<br>80|||||Chan<br>~~BiLS~~|Chan<br>~~BiLS~~||||\n",
       "|~~Disju~~<br>60<br>40<br>20<br>0<br>20<br>40<br>60<br>80|||||Chan<br>~~BiLS~~|Chan<br>~~BiLS~~|ce<br>~~M+ELMo+Attn~~|BERT <br>~~SemB~~|BAM<br>~~RT~~|\n",
       "|~~Disju~~<br>60<br>40<br>20<br>0<br>20<br>40<br>60<br>80|||||Open<br>BERT<br>|Open<br>BERT<br>|AI GPT<br> + Single~~-~~task Ad<br>|apters<br><br>Snorke<br>ALICE <br>|l MeTaL<br>(Large)<br>|\n",
       "|~~Disju~~<br>60<br>40<br>20<br>0<br>20<br>40<br>60<br>80||||~~ction~~<br>~~Downwar~~|~~Monotone~~<br>~~Restri~~<br>~~BERT~~<br>BERT|~~Monotone~~<br>~~Restri~~<br>~~BERT~~<br>BERT|~~(Large)~~<br> on STILTs|~~MT-DN~~<br>XLNet~~-~~|~~N (ensemble)~~<br>Large (ensemble)|\n",
       "|~~Disju~~<br>60<br>40<br>20<br>0<br>20<br>40<br>60<br>80||||~~ction~~<br>~~Downwar~~|~~Monotone~~<br>~~Restri~~<br>~~BERT~~<br>BERT|~~Monotone~~<br>~~Restri~~<br>~~BERT~~<br>BERT|~~tivity~~<br>~~Doubl~~|~~Negation~~<br>~~Preposition~~|~~l Phrases~~|\n",
       "\n",
       "\n",
       "Figure 2: Performance of GLUE submissions on selected diagnostic categories, reported using the\n",
       "_R_ 3 metric scaled up by 100, as in Wang et al. (2019a, see paper for a description of the categories).\n",
       "Some initially difficult categories, like double negation, saw gains from advances on GLUE, but\n",
       "others remain hard (restrictivity) or even adversarial (disjunction, downward monotone).\n",
       "\n",
       "\n",
       "**C** **Instructions to Crowd Workers**\n",
       "\n",
       "\n",
       "**C.1** **Training Phase Instructions**\n",
       "\n",
       "\n",
       "For collecting data to establish human performance on the SuperGLUE tasks, we follow a two\n",
       "step procedure where we first provide some training to the crowd workers before they proceed to\n",
       "annotation. In the training step, we provide workers with brief instructions about the training phase.\n",
       "An example of these instructions is given Table 5. These training instructions are the same across\n",
       "tasks, only the task name in the instructions is changed.\n",
       "\n",
       "\n",
       "17\n",
       "\n",
       "\n",
       "**C.2** **Task Instructions**\n",
       "\n",
       "\n",
       "During training and annotation for each task, we provide workers with brief instructions tailored to\n",
       "the task. We also link workers to an FAQ page for the task. Tables 6, 7, 8, and 9, show the instructions\n",
       "we used for all four tasks: COPA, CommitmentBank, WSC, and BoolQ respectively. The instructions\n",
       "given to crowd workers for annotations on the diagnostic and bias diagnostic datasets are shown in\n",
       "Table 11.\n",
       "\n",
       "\n",
       "We collected data to produce conservative estimates for human performance on several tasks that\n",
       "we did not ultimately include in our benchmark, including GAP (Webster et al., 2018), PAWS\n",
       "(Zhang et al., 2019), Quora Insincere Questions, [11] Ultrafine Entity Typing (Choi et al., 2018b), and\n",
       "Empathetic Reactions datasets (Buechel et al., 2018). The instructions we used for these tasks are\n",
       "shown in Tables 12, 13, 14, 15, and 16.\n",
       "\n",
       "\n",
       "**C.3** **Task Specific Details**\n",
       "\n",
       "\n",
       "For WSC and COPA we provide annotators with a two way classification problem. We then use\n",
       "majority vote across annotations to calculate human performance.\n",
       "\n",
       "\n",
       "**CommitmentBank** We follow the authors in providing annotators with a 7-way classification\n",
       "problem. We then collapse the annotations into 3 classes by using the same ranges for bucketing used\n",
       "by De Marneffe et al. (2019). We then use majority vote to get human performance numbers on the\n",
       "task.\n",
       "\n",
       "\n",
       "Furthermore, for training on CommitmentBank we randomly sample examples from the low interannotator agreement portion of the CommitmentBank data that is not included in the benchmark\n",
       "version of the task. These low agreement examples are generally harder to classify since they are\n",
       "more ambiguous.\n",
       "\n",
       "\n",
       "**Diagnostic Dataset** Since the diagnostic dataset does not come with accompanying training data,\n",
       "we train our workers on examples from RTE’s development set. RTE is also a textual entailment\n",
       "task and is the most closely related task in the main benchmark. Providing the crowd workers with\n",
       "training on RTE enables them to learn label definitions which should generalize to the diagnostic\n",
       "dataset.\n",
       "\n",
       "\n",
       "**Ultrafine Entity Typing** We cast the task into a binary classification problem to make it an easier\n",
       "task for non-expert crowd workers. We work in cooperation with the authors of the dataset (Choi\n",
       "et al., 2018b) to do this reformulation: We give workers one possible tag for a word or phrase and\n",
       "asked them to classify the tag as being applicable or not.\n",
       "\n",
       "\n",
       "The authors used WordNet (Miller, 1995) to expand the set of labels to include synonyms and\n",
       "hypernyms from WordNet. They then asked five annotators to validate these tags. The tags from this\n",
       "validation had high agreement, and were included in the publicly available Ultrafine Entity Typing\n",
       "dataset, [12] This constitutes our set of positive examples. The rest of the tags from the validation\n",
       "procedure that are not in the public dataset constitute our negative examples.\n",
       "\n",
       "\n",
       "**GAP** For the Gendered Ambiguous Pronoun Coreference task (GAP, Webster et al., 2018), we\n",
       "simplified the task by providing noun phrase spans as part of the input, thus reducing the original\n",
       "structure prediction task to a classification task. This task was presented to crowd workers as a three\n",
       "way classification problem: Choose span A, B, or neither.\n",
       "\n",
       "\n",
       "**D** **Excluded Tasks**\n",
       "\n",
       "\n",
       "In this section we provide some examples of tasks that we evaluated for inclusion but ultimately could\n",
       "not include. We report on these excluded tasks only with the permission of their authors. We turned\n",
       "down many medical text datasets because they are usually only accessible with explicit permission\n",
       "and credentials from the data owners.\n",
       "\n",
       "\n",
       "11 `[https://www.kaggle.com/c/quora-insincere-questions-classification/data](https://www.kaggle.com/c/quora-insincere-questions-classification/data)`\n",
       "12 `[https://homes.cs.washington.edu/~eunsol/open_entity.html](https://homes.cs.washington.edu/~eunsol/open_entity.html)`\n",
       "\n",
       "\n",
       "18\n",
       "\n",
       "\n",
       "Tasks like QuAC (Choi et al., 2018a) and STREUSLE (Schneider and Smith, 2015) differed substantially from the format of other tasks in our benchmark, which we worried would incentivize users\n",
       "to spend significant effort on task-specific model designs, rather than focusing on general-purpose\n",
       "techniques. It was challenging to train annotators to do well on Quora Insincere Questions [13], Empathetic Reactions (Buechel et al., 2018), and a recast version of Ultra-Fine Entity Typing (Choi et al.,\n",
       "2018b, see Appendix C.3 for details), leading to low human performance. BERT achieved very high\n",
       "or superhuman performance on Query Well-Formedness (Faruqui and Das, 2018), PAWS (Zhang\n",
       "et al., 2019), Discovering Ongoing Conversations (Zanzotto and Ferrone, 2017), and GAP (Webster\n",
       "et al., 2018).\n",
       "\n",
       "\n",
       "During the process of selecting tasks for our benchmark, we collected human performance baselines\n",
       "and run BERT-based machine baselines for some tasks that we ultimately excluded from our task\n",
       "list. We chose to exclude these tasks because our BERT baseline performs better than our human\n",
       "performance baseline or if the gap between human and machine performance is small.\n",
       "\n",
       "\n",
       "On Quora Insincere Questions our BERT baseline outperforms our human baseline by a small margin:\n",
       "an F1 score of 67.2 versus 66.7 for BERT and human baselines respectively. Similarly, on the\n",
       "Empathetic Reactions dataset, BERT outperforms our human baseline, where BERT’s predictions\n",
       "have a Pearson correlation of 0.45 on empathy and 0.55 on distress, compared to 0.45 and 0.35 for\n",
       "our human baseline. For PAWS-Wiki, we report that BERT achieves an accuracy of 91.9%, while our\n",
       "human baseline achieved 84% accuracy. These three tasks are excluded from the benchmark since\n",
       "our, admittedly conservative, human baselines are worse than machine performance. Our human\n",
       "performance baselines are subject to the clarity of our instructions (all instructions can be found in\n",
       "Appendix C), and crowd workers engagement and ability.\n",
       "\n",
       "\n",
       "For the Query Well-Formedness task, the authors set an estimate human performance at 88.4%\n",
       "accuracy. Our BERT baseline model reaches an accuracy of 82.3%. While there is a positive gap on\n",
       "this task, the gap was smaller than we were were willing to tolerate. Similarly, on our recast version\n",
       "of the Ultrafine Entity Typing, we observe too small a gap between human (60.2 F1) and machine\n",
       "performance (55.0 F1). Our recasting for this task is described in Appendix C.2. On GAP, when\n",
       "taken as a classification problem without the related task of span selection (details in C.2), BERT\n",
       "performs (91.0 F1) comparably to our human baseline (94.9 F1). Given this small margin, we also\n",
       "exclude GAP.\n",
       "\n",
       "\n",
       "On Discovering Ongoing Conversations, our BERT baseline achieves an F1 of 51.9 on a version of\n",
       "the task cast as sentence pair classification (given two snippets of texts from plays, determine if the\n",
       "second snippet is a continuation of the first). This dataset is very class imbalanced (90% negative), so\n",
       "we also experimented with a class-balanced version on which our BERT baselines achieves 88.4\n",
       "F1. Qualitatively, we also found the task challenging for humans as there was little context for the\n",
       "text snippets and the examples were drawn from plays using early English. Given this fairly high\n",
       "machine performance and challenging nature for humans, we exclude this task from our benchmark.\n",
       "\n",
       "\n",
       "_Instructions tables begin on the following page._\n",
       "\n",
       "\n",
       "13 `[https://www.kaggle.com/c/quora-insincere-questions-classification/data](https://www.kaggle.com/c/quora-insincere-questions-classification/data)`\n",
       "\n",
       "\n",
       "19\n",
       "\n",
       "\n",
       "Table 5: The instructions given to crowd-sourced worker describing the training phase for the Choice\n",
       "of Plausible Answers (COPA) task.\n",
       "\n",
       "\n",
       "The New York University Center for Data Science is collecting your answers for use in research\n",
       "on computer understanding of English. Thank you for your help!\n",
       "\n",
       "\n",
       "This project is a **training task** that needs to be completed before working on the main project\n",
       "on AMT named Human Performance: Plausible Answer. Once you are done with the training,\n",
       "please proceed to the main task! The qualification approval is not immediate but we will add\n",
       "you to our qualified workers list within a day.\n",
       "\n",
       "\n",
       "In this training, you must answer the question on the page and then, to see how you did, click\n",
       "the **Check Work** button at the bottom of the page before hitting Submit. The Check Work\n",
       "button will reveal the true label. Please use this training and the provided answers to build\n",
       "an understanding of what the answers to these questions look like (the main project, Human\n",
       "Performance: Plausible Answer, does not have the answers on the page).\n",
       "\n",
       "\n",
       "Table 6: Task-specific instructions for Choice of Plausible Alternatives (COPA). These instructions\n",
       "were provided during both training and annotation phases.\n",
       "\n",
       "\n",
       "**Plausible Answer Instructions**\n",
       "\n",
       "\n",
       "The New York University Center for Data Science is collecting your answers for use in research\n",
       "on computer understanding of English. Thank you for your help!\n",
       "\n",
       "\n",
       "We will present you with a prompt sentence and a question. The question will either be about\n",
       "what caused the situation described in the prompt, or what a possible effect of that situation is.\n",
       "We will also give you two possible answers to this question. Your job is to decide, given the\n",
       "situation described in the prompt, which of the two options is a more plausible answer to the\n",
       "question:\n",
       "\n",
       "\n",
       "In the following example, option 1. is a more plausible answer to the question about what caused\n",
       "the situation described in the prompt,\n",
       "\n",
       "\n",
       "_The girl received a trophy._\n",
       "_What’s the CAUSE for this?_\n",
       "\n",
       "\n",
       "1. _She won a spelling bee._\n",
       "2. _She made a new friend._\n",
       "\n",
       "\n",
       "In the following example, option 2. is a more plausible answer the question about what happened\n",
       "because of the situation described in the prompt,\n",
       "\n",
       "\n",
       "_The police aimed their weapons at the fugitive._\n",
       "_What happened as a RESULT?_\n",
       "\n",
       "\n",
       "1. _The fugitive fell to the ground._\n",
       "2. _The fugitive dropped his gun._\n",
       "\n",
       "\n",
       "[If you have any more questions, please refer to our FAQ page.](https://nyu-mll.github.io/SuperGLUE-human/copa-faq)\n",
       "\n",
       "\n",
       "20\n",
       "\n",
       "\n",
       "Table 7: Task-specific instructions for Commitment Bank. These instructions were provided during\n",
       "both training and annotation phases.\n",
       "\n",
       "\n",
       "**Speaker Commitment Instructions**\n",
       "\n",
       "\n",
       "The New York University Center for Data Science is collecting your answers for use in research\n",
       "on computer understanding of English. Thank you for your help!\n",
       "\n",
       "\n",
       "We will present you with a prompt taken from a piece of dialogue, this could be a single sentence,\n",
       "a few sentences, or a short exchange between people. Your job is to figure out, based on this\n",
       "first prompt (on top), how certain the speaker is about the truthfulness of the second prompt\n",
       "(on the bottom). You can choose from a 7 point scale ranging from (1) completely certain that\n",
       "the second prompt is true to (7) completely certain that the second prompt is false. Here are\n",
       "examples for a few of the labels:\n",
       "\n",
       "\n",
       "Choose 1 (certain that it is true) if the speaker from the first prompt definitely believes or knows\n",
       "that the second prompt is true. For example,\n",
       "\n",
       "\n",
       "_\"What fun to hear Artemis laugh. She’s such a serious child. I didn’t know_\n",
       "_she had a sense of humor.\"_\n",
       "_\"Artemis had a sense of humor\"_\n",
       "\n",
       "\n",
       "Choose 4 (not certain if it is true or false) if the speaker from the first prompt is uncertain if the\n",
       "second prompt is true or false. For example,\n",
       "\n",
       "\n",
       "_\"Tess is committed to track. She’s always trained with all her heart and soul._\n",
       "_One can only hope that she has recovered from the flu and will cross the finish_\n",
       "_line.\"_\n",
       "\n",
       "_\"Tess crossed the finish line.\"_\n",
       "\n",
       "\n",
       "Choose 7 (certain that it is false) if the speaker from the first prompt definitely believes or knows\n",
       "that the second prompt is false. For example,\n",
       "\n",
       "\n",
       "_\"Did you hear about Olivia’s chemistry test? She studied really hard. But_\n",
       "_even after putting in all that time and energy, she didn’t manage to pass the_\n",
       "_test\"._\n",
       "\n",
       "_\"Olivia passed the test.\"_\n",
       "\n",
       "\n",
       "[If you have any more questions, please refer to our FAQ page.](https://nyu-mll.github.io/SuperGLUE-human/commit-faq)\n",
       "\n",
       "\n",
       "21\n",
       "\n",
       "\n",
       "Table 8: Task-specific instructions for Winograd Schema Challenge (WSC). These instructions were\n",
       "provided during both training and annotation phases.\n",
       "\n",
       "\n",
       "**Winograd Schema Instructions**\n",
       "\n",
       "\n",
       "The New York University Center for Data Science is collecting your answers for use in research\n",
       "on computer understanding of English. Thank you for your help!\n",
       "\n",
       "\n",
       "We will present you with a sentence that someone wrote, with one bolded pronoun. We will then\n",
       "ask if you if the pronoun refers to a specific word or phrase in the sentence. Your job is to figure\n",
       "out, based on the sentence, if the bolded pronoun refers to this selected word or phrase:\n",
       "\n",
       "\n",
       "Choose Yes if the pronoun refers to the selected word or phrase. For example,\n",
       "\n",
       "\n",
       "_\"I put the cake away in the refrigerator. It has a lot of butter in it.\"_\n",
       "_Does_ _**It**_ _in \"It has a lot\" refer to_ _**cake**_ _?_\n",
       "\n",
       "\n",
       "Choose No if the pronoun does not refer to the selected word or phrase. For example,\n",
       "\n",
       "\n",
       "_\"The large ball crashed right through the table because it was made of_\n",
       "_styrofoam.\"_\n",
       "_Does_ _**it**_ _in \"it was made\" refer to_ _**ball**_ _?_\n",
       "\n",
       "\n",
       "[If you have any more questions, please refer to our FAQ page.](https://nyu-mll.github.io/SuperGLUE-human/wsc-faq)\n",
       "\n",
       "\n",
       "22\n",
       "\n",
       "\n",
       "Table 9: Task-specific instructions for BoolQ (continued in Table 10). These instructions were\n",
       "provided during both training and annotation phases.\n",
       "\n",
       "\n",
       "**Question-Answering Instructions**\n",
       "\n",
       "\n",
       "The New York University Center for Data Science is collecting your answers for use in research\n",
       "on computer understanding of English. Thank you for your help!\n",
       "\n",
       "\n",
       "We will present you with a passage taken from a Wikipedia article and a relevant question. Your\n",
       "job is to decide, given the information provided in the passage, if the answer to the question is\n",
       "Yes or No. For example,\n",
       "\n",
       "\n",
       "**In the following examples the correct answer is Yes,**\n",
       "\n",
       "\n",
       "_The thirteenth season of Criminal Minds was ordered on April 7, 2017, by_\n",
       "_CBS with an order of 22 episodes. The season premiered on September 27,_\n",
       "_2017 in a new time slot at 10:00PM on Wednesday when it had previously_\n",
       "_been at 9:00PM on Wednesday since its inception. The season concluded on_\n",
       "_April 18, 2018 with a two-part season finale._\n",
       "\n",
       "_will there be a 13th season of criminal minds?_\n",
       "(In the above example, the first line of the passage says that the 13th season of\n",
       "the show was ordered.)\n",
       "\n",
       "\n",
       "_As of 8 August 2016, the FDA extended its regulatory power to include e-_\n",
       "_cigarettes. Under this ruling the FDA will evaluate certain issues, including_\n",
       "_ingredients, product features and health risks, as well their appeal to minors_\n",
       "_and non-users. The FDA rule also bans access to minors. A photo ID is_\n",
       "_required to buy e-cigarettes, and their sale in all-ages vending machines is not_\n",
       "_permitted. The FDA in September 2016 has sent warning letters for unlawful_\n",
       "_underage sales to online retailers and retailers of e-cigarettes._\n",
       "_is vaping illegal if you are under 18?_\n",
       "(In the above example, the passage states that the \"FDA rule also bans access\n",
       "to minors.\" The question uses the word \"vaping,\" which is a synonym for\n",
       "e-cigrattes.)\n",
       "\n",
       "\n",
       "**In the following examples the correct answer is No,**\n",
       "\n",
       "\n",
       "_Badgers are short-legged omnivores in the family Mustelidae, which also_\n",
       "_includes the otters, polecats, weasels, and wolverines. They belong to the_\n",
       "_caniform suborder of carnivoran mammals. The 11 species of badgers are_\n",
       "_grouped in three subfamilies: Melinae (Eurasian badgers), Mellivorinae (the_\n",
       "_honey badger or ratel), and Taxideinae (the American badger). The Asiatic_\n",
       "_stink badgers of the genus Mydaus were formerly included within Melinae_\n",
       "_(and thus Mustelidae), but recent genetic evidence indicates these are actually_\n",
       "_members of the skunk family, placing them in the taxonomic family Mephitidae._\n",
       "_is a wolverine the same as a badger?_\n",
       "(In the above example, the passage says that badgers and wolverines are in\n",
       "the same family, Mustelidae, which does not mean they are the same animal.)\n",
       "\n",
       "\n",
       "23\n",
       "\n",
       "\n",
       "Table 10: Continuation from Table 9 of task-specific instructions for BoolQ. These instructions were\n",
       "provided during both training and annotation phases.\n",
       "\n",
       "\n",
       "_More famously, Harley-Davidson attempted to register as a trademark the_\n",
       "_distinctive “chug” of a Harley-Davidson motorcycle engine. On February_\n",
       "_1, 1994, the company filed its application with the following description:_\n",
       "_“The mark consists of the exhaust sound of applicant’s motorcycles, produced_\n",
       "_by V-twin, common crankpin motorcycle engines when the goods are in use.”_\n",
       "_Nine of Harley-Davidson’s competitors filed oppositions against the applica-_\n",
       "_tion, arguing that cruiser-style motorcycles of various brands use the same_\n",
       "_crankpin V-twin engine which produces the same sound. After six years of_\n",
       "_litigation, with no end in sight, in early 2000, Harley-Davidson withdrew their_\n",
       "_application._\n",
       "_does harley davidson have a patent on their sound?_\n",
       "(In the above example, the passage states that Harley-Davidson applied for a\n",
       "patent but then withdrew, so they do not have a patent on the sound.)\n",
       "\n",
       "\n",
       "[If you have any more questions, please refer to our FAQ page.](https://nyu-mll.github.io/SuperGLUE-human/boolq-faq)\n",
       "\n",
       "\n",
       "24\n",
       "\n",
       "\n",
       "Table 11: Task-specific instructions for the diagnostic and the bias diagnostic datasets. These\n",
       "instructions were provided during both training and annotation phases.\n",
       "\n",
       "\n",
       "**Textual Entailment Instructions**\n",
       "\n",
       "\n",
       "The New York University Center for Data Science is collecting your answers for use in research\n",
       "on computer understanding of English. Thank you for your help!\n",
       "\n",
       "\n",
       "We will present you with a prompt taken from an article someone wrote. Your job is to figure out,\n",
       "based on this correct prompt (the first prompt, on top), if another prompt (the second prompt, on\n",
       "bottom) is also necessarily true:\n",
       "\n",
       "\n",
       "Choose True if the event or situation described by the first prompt definitely implies that the\n",
       "second prompt, on bottom, must also be true. For example,\n",
       "\n",
       "\n",
       "_• \"Murphy recently decided to move to London.\"_\n",
       "\n",
       "_\"Murphy recently decided to move to England.\"_\n",
       "(The above example is True because London is in England and therefore prompt 2 is\n",
       "clearly implied by prompt 1.)\n",
       "\n",
       "_•_ _\"Russian cosmonaut Valery Polyakov set the record for the longest continuous amount_\n",
       "_of time spent in space, a staggering 438 days, between 1994 and 1995.\"_\n",
       "_\"Russians hold record for longest stay in space.\"_\n",
       "(The above example is True because the information in the second prompt is contained\n",
       "in the first prompt: Valery is Russian and she set the record for longest stay in space.)\n",
       "\n",
       "_•_ _\"She does not disgree with her brother’s opinion, but she believes he’s too aggresive in_\n",
       "_his defense\"_\n",
       "_\"She agrees with her brother’s opinion, but she believes he’s too aggresive in his_\n",
       "_defense\"_\n",
       "(The above example is True because the second prompt is an exact paraphrase of the\n",
       "first prompt, with exactly the same meaning.)\n",
       "\n",
       "\n",
       "Choose False if the event or situation described with the first prompt on top does not necessarily\n",
       "imply that this second prompt must also be true. For example,\n",
       "\n",
       "\n",
       "_• \"This method was developed at Columbia and applied to data processing at CERN.\"_\n",
       "\n",
       "_\"This method was developed at Columbia and applied to data processing at CERN_\n",
       "_with limited success.\"_\n",
       "\n",
       "(The above example is False because the second prompt is introducing new information\n",
       "not implied in the first prompt: The first prompt does not give us any knowledge of\n",
       "how succesful the application of the method at CERN was.)\n",
       "\n",
       "_• \"This building is very tall.\"_\n",
       "\n",
       "_\"This is the tallest building in New York.\"_\n",
       "(The above example is False because a building being tall does not mean it must be the\n",
       "tallest building, nor that it is in New York.)\n",
       "\n",
       "_•_ _\"Hours earlier, Yasser Arafat called for an end to attacks against Israeli civilians in_\n",
       "_the two weeks before Israeli elections.\"_\n",
       "_\"Arafat condemned suicide bomb attacks inside Israel.\"_\n",
       "(The above example is False because from the first prompt we only know that Arafat\n",
       "called for an end to attacks against Israeli citizens, we do not know what kind of attacks\n",
       "he may have been condemning.)\n",
       "\n",
       "\n",
       "You do not have to worry about whether the writing style is maintained between the two prompts.\n",
       "\n",
       "\n",
       "[If you have any more questions, please refer to our FAQ page.](https://nyu-mll.github.io/SuperGLUE-human/diagnostic-faq)\n",
       "\n",
       "\n",
       "25\n",
       "\n",
       "\n",
       "Table 12: Task-specific instructions for the Gendered Ambiguous Pronoun Coreference (GAP) task.\n",
       "These instructions were provided during both training and annotation phases.\n",
       "\n",
       "\n",
       "**GAP Instructions**\n",
       "\n",
       "\n",
       "The New York University Center for Data Science is collecting your answers for use in research\n",
       "on computer understanding of English. Thank you for your help!\n",
       "\n",
       "\n",
       "We will present you with an extract from a Wikipedia article, with one bolded pronoun. We will\n",
       "also give you two names from the text that this pronoun could refer to. Your job is to figure out,\n",
       "based on the extract, if the pronoun refers to option A, options B, or neither:\n",
       "\n",
       "\n",
       "Choose A if the pronoun refers to option A. For example,\n",
       "\n",
       "\n",
       "_\"In 2010 Ella Kabambe was not the official Miss Malawi; this was Faith_\n",
       "_Chibale, but Kabambe represented the country in the Miss World pageant._\n",
       "_At the 2012 Miss World, Susan Mtegha pushed Miss New Zealand, Collette_\n",
       "_Lochore, during the opening headshot of the pageant, claiming that Miss New_\n",
       "_Zealand was in her space.\"_\n",
       "_Does_ _**her**_ _refer to option A or B below?_\n",
       "\n",
       "\n",
       "A _Susan Mtegha_\n",
       "\n",
       "\n",
       "B _Collette Lochore_\n",
       "\n",
       "C _Neither_\n",
       "\n",
       "\n",
       "Choose B if the pronoun refers to option B. For example,\n",
       "\n",
       "\n",
       "_\"In 1650 he started his career as advisor in the ministerium of finances in Den_\n",
       "_Haag. After he became a minister he went back to Amsterdam, and took place_\n",
       "_as a sort of chairing mayor of this city. After the death of his brother Cornelis,_\n",
       "_De Graeff became the strong leader of the republicans. He held this position_\n",
       "_until the rampjaar.\"_\n",
       "_Does_ _**He**_ _refer to option A or B below?_\n",
       "\n",
       "\n",
       "A _Cornelis_\n",
       "\n",
       "\n",
       "B _De Graeff_\n",
       "\n",
       "C _Neither_\n",
       "\n",
       "\n",
       "Choose C if the pronoun refers to neither option. For example,\n",
       "\n",
       "\n",
       "_\"Reb Chaim Yaakov’s wife is the sister of Rabbi Moishe Sternbuch, as is_\n",
       "_the wife of Rabbi Meshulam Dovid Soloveitchik, making the two Rabbis his_\n",
       "_uncles. Reb Asher’s brother Rabbi Shlomo Arieli is the author of a critical_\n",
       "_edition of the novallae of Rabbi Akiva Eiger. Before his marriage, Rabbi Arieli_\n",
       "_studied in the Ponevezh Yeshiva headed by Rabbi Shmuel Rozovsky, and he_\n",
       "_later studied under his father-in-law in the Mirrer Yeshiva.\"_\n",
       "_Does_ _**his**_ _refer to option A or B below?_\n",
       "\n",
       "\n",
       "A _Reb Asher_\n",
       "\n",
       "\n",
       "B _Akiva Eiger_\n",
       "\n",
       "C _Neither_\n",
       "\n",
       "\n",
       "[If you have any more questions, please refer to our FAQ page.](https://nyu-mll.github.io/SuperGLUE-human/gap-faq)\n",
       "\n",
       "\n",
       "26\n",
       "\n",
       "\n",
       "Table 13: Task-specific instructions for the Paraphrase Adversaries from Word Scrambling (PAWS)\n",
       "task. These instructions were provided during both training and annotation phases.\n",
       "\n",
       "\n",
       "**Paraphrase Detection Instructions**\n",
       "\n",
       "\n",
       "The New York University Center for Data Science is collecting your answers for use in research\n",
       "on computer understanding of English. Thank you for your help!\n",
       "\n",
       "\n",
       "We will present you with two similar sentences taken from Wikipedia articles. Your job is to\n",
       "figure out if these two sentences are paraphrases of each other, and convey exactly the same\n",
       "meaning:\n",
       "\n",
       "\n",
       "Choose Yes if the sentences are paraphrases and have the exact same meaning. For example,\n",
       "\n",
       "\n",
       "_\"Hastings Ndlovu was buried with Hector Pieterson at Avalon Cemetery in_\n",
       "_Johannesburg.\"_\n",
       "_\"Hastings Ndlovu, together with Hector Pieterson, was buried at the Avalon_\n",
       "_cemetery in Johannesburg .\"_\n",
       "\n",
       "\n",
       "_\"The complex of the Trabzon World Trade Center is close to Trabzon Airport_\n",
       "_.\"_\n",
       "\n",
       "_\"The complex of World Trade Center Trabzon is situated close to Trabzon_\n",
       "_Airport .\"_\n",
       "\n",
       "\n",
       "Choose No if the two sentences are not exact paraphrases and mean different things. For\n",
       "example,\n",
       "\n",
       "\n",
       "_\"She was only a few months in French service when she met some British_\n",
       "_frigates in 1809 .\"_\n",
       "_\"She was only in British service for a few months, when in 1809, she_\n",
       "_encountered some French frigates .\"_\n",
       "\n",
       "\n",
       "_\"This work caused him to trigger important reflections on the practices of_\n",
       "_molecular genetics and genomics at a time when this was not considered_\n",
       "_ethical .\"_\n",
       "\n",
       "_\"This work led him to trigger ethical reflections on the practices of molecular_\n",
       "_genetics and genomics at a time when this was not considered important .\"_\n",
       "\n",
       "\n",
       "[If you have any more questions, please refer to our FAQ page.](https://nyu-mll.github.io/SuperGLUE-human/paws-faq)\n",
       "\n",
       "\n",
       "27\n",
       "\n",
       "\n",
       "Table 14: Task-specific instructions for the Quora Insincere Questions task. These instructions were\n",
       "provided during both training and annotation phases.\n",
       "\n",
       "\n",
       "**Insincere Questions Instructions**\n",
       "\n",
       "\n",
       "The New York University Center for Data Science is collecting your answers for use in research\n",
       "on computer understanding of English. Thank you for your help!\n",
       "\n",
       "\n",
       "We will present you with a question that someone posted on Quora. Your job is to figure out\n",
       "whether or not this is a sincere question. An insincere question is defined as a question intended\n",
       "to make a statement rather than look for helpful answers. Some characteristics that can signify\n",
       "that a question is insincere:\n",
       "\n",
       "\n",
       "_•_ Has a non-neutral tone\n",
       "\n",
       "\n",
       "**–**\n",
       "Has an exaggerated tone to underscore a point about a group of people\n",
       "\n",
       "**–**\n",
       "Is rhetorical and meant to imply a statement about a group of people\n",
       "\n",
       "_•_ Is disparaging or inflammatory\n",
       "\n",
       "\n",
       "**–**\n",
       "Suggests a discriminatory idea against a protected class of people, or seeks\n",
       "confirmation of a stereotype\n",
       "\n",
       "**–**\n",
       "Makes disparaging attacks/insults against a specific person or group of people\n",
       "\n",
       "**–**\n",
       "Based on an outlandish premise about a group of people\n",
       "\n",
       "**–**\n",
       "Disparages against a characteristic that is not fixable and not measurable\n",
       "\n",
       "_•_ Isn’t grounded in reality\n",
       "\n",
       "\n",
       "**–**\n",
       "Based on false information, or contains absurd assumptions\n",
       "\n",
       "**–**\n",
       "Uses sexual content (incest, bestiality, pedophilia) for shock value, and not to seek\n",
       "genuine answers\n",
       "\n",
       "\n",
       "Please note that there are far fewer insincere questions than there are sincere questions! So you\n",
       "should expect to label most questions as sincere.\n",
       "\n",
       "\n",
       "**Examples,**\n",
       "\n",
       "\n",
       "Choose Sincere if you believe the person asking the question was genuinely seeking an answer\n",
       "from the forum. For example,\n",
       "\n",
       "\n",
       "_\"How do DNA and RNA compare and contrast?\"_\n",
       "_\"Are there any sports that you don’t like?_ \"\n",
       "_\"What is the main purpose of penance?\"_\n",
       "\n",
       "\n",
       "Choose Insincere if you believe the person asking the question was not really seeking an answer\n",
       "but was being inflammatory, extremely rhetorical, or absurd. For example,\n",
       "\n",
       "\n",
       "_\"How do I sell Pakistan? I need lots of money so I decided to sell Pakistan_\n",
       "_any one wanna buy?\"_\n",
       "_\"If Hispanics are so proud of their countries, why do they move out?\"_\n",
       "_\"Why Chinese people are always not welcome in all countries?\"_\n",
       "\n",
       "\n",
       "[If you have any more questions, please refer to our FAQ page.](https://nyu-mll.github.io/SuperGLUE-human/quora-faq)\n",
       "\n",
       "\n",
       "28\n",
       "\n",
       "\n",
       "Table 15: Task-specific instructions for the Ultrafine Entity Typing task. These instructions were\n",
       "provided during both training and annotation phases.\n",
       "\n",
       "\n",
       "**Entity Typing Instructions**\n",
       "\n",
       "\n",
       "The New York University Center for Data Science is collecting your answers for use in research\n",
       "on computer understanding of English. Thank you for your help!\n",
       "\n",
       "\n",
       "We will provide you with a sentence with on bolded word or phrase. We will also give you a\n",
       "possible tag for this bolded word or phrase. Your job is to decide, in the context of the sentence,\n",
       "if this tag is correct and applicable to the bolded word or phrase:\n",
       "\n",
       "\n",
       "Choose Yes if the tag is applicable and accurately describes the selected word or phrase. For\n",
       "example,\n",
       "\n",
       "\n",
       "_“Spain was the gold line.\"_ _**It**_ _started out with zero gold in 1937, and by 1945_\n",
       "_it had 65.5 tons._\n",
       "\n",
       "_Tag: nation_\n",
       "\n",
       "\n",
       "Choose No if the tag is not applicable and does not describes the selected word or phrase. For\n",
       "example,\n",
       "\n",
       "\n",
       "_**Iraqi museum workers**_ _are starting to assess the damage to Iraq’s history._\n",
       "\n",
       "_Tag: organism_\n",
       "\n",
       "\n",
       "[If you have any more questions, please refer to our FAQ page.](https://nyu-mll.github.io/SuperGLUE-human/ultra-faq)\n",
       "\n",
       "\n",
       "29\n",
       "\n",
       "\n",
       "Table 16: Task-specific instructions for the Empathetic Reaction task. These instructions were\n",
       "provided during both training and annotation phases.\n",
       "\n",
       "\n",
       "**Empathy and Distress Analysis Instructions**\n",
       "\n",
       "\n",
       "The New York University Center for Data Science is collecting your answers for use in research\n",
       "on computer understanding of English. Thank you for your help!\n",
       "\n",
       "\n",
       "We will present you with a message someone wrote after reading an article. Your job is to figure\n",
       "out, based on this message, how disressed and empathetic the author was feeling. Empathy is\n",
       "defined as feeling warm, tender, sympathetic, moved, or compassionate. Distressed is defined as\n",
       "feeling worried, upset, troubled, perturbed, grieved, distrubed, or alarmed.\n",
       "\n",
       "\n",
       "**Examples,**\n",
       "The author of the following message was not feeling empathetic at all with an empathy score of 1,\n",
       "and was very distressed with a distress score of 7,\n",
       "\n",
       "\n",
       "_\"I really hate ISIS. They continue to be the stain on society by committing_\n",
       "_atrocities condemned by every nation in the world. They must be stopped at_\n",
       "_all costs and they must be destroyed so that they wont hurt another soul. These_\n",
       "_poor people who are trying to survive get killed, imprisoned, or brainwashed_\n",
       "_into joining and there seems to be no way to stop them.\"_\n",
       "\n",
       "\n",
       "The author of the following message is feeling very empathetic with an empathy score of 7 and\n",
       "also very distressed with a distress score of 7,\n",
       "\n",
       "\n",
       "_\"All of you know that I love birds. This article was hard for me to read because_\n",
       "_of that. Wind turbines are killing a lot of birds, including eagles. It’s really_\n",
       "_very sad. It makes me feel awful. I am all for wind turbines and renewable_\n",
       "_sources of energy because of global warming and coal, but this is awful. I_\n",
       "_don’t want these poor birds to die like this. Read this article and you’ll see_\n",
       "_why.\"_\n",
       "\n",
       "\n",
       "The author of the following message is feeling moderately empathetic with an\n",
       "empathy score of 4 and moderately distressed with a distress score of 4,\n",
       "\n",
       "\n",
       "_\"I just read an article about wild fires sending a smokey haze across the state_\n",
       "_near the Appalachian mountains. Can you imagine how big the fire must be_\n",
       "_to spread so far and wide? And the people in the area obviously suffer the_\n",
       "_most. What if you have asthma or some other condition that restricts your_\n",
       "_breathing?\"_\n",
       "\n",
       "\n",
       "The author of the following message is feeling very empathetic with an empathy score of 7 and\n",
       "mildly distressed with a distress score of 2,\n",
       "\n",
       "\n",
       "_\"This is a very sad article. Being of of the first female fighter pilots must_\n",
       "_have given her and her family great honor. I think that there should be more_\n",
       "_training for all pilots who deal in these acrobatic flying routines. I also think_\n",
       "_that women have just as much of a right to become a fighter pilot as men.\"_\n",
       "\n",
       "\n",
       "[If you have any more questions, please refer to our FAQ page.](https://nyu-mll.github.io/SuperGLUE-human/empathy-faq)\n",
       "\n",
       "\n",
       "30\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bbc8839-f9fa-4b36-b32a-cf2f5c78f534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pathlib\n",
    "# pathlib.Path(\"output.md\").write_bytes(doc.encode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af302923-9287-48a5-a16b-6c4542d664a8",
   "metadata": {},
   "source": [
    "### PyPDF experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ff18eac-fc7f-4aa8-a2a5-c79f9b1557d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_with_metadata_from_pdfs(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    num_pages = len(reader.pages)\n",
    "    data = []\n",
    "    for i in range(num_pages):\n",
    "        data.append({\n",
    "            \"file_type\": \"pdf\",\n",
    "            \"file_name\": file_path,\n",
    "            \"marker\": i+1, # To identify where in the document this chunk is from\n",
    "            \"text\": reader.pages[i].extract_text()\n",
    "        })\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3fdcd92-0b49-497b-8043-0a705289ab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "lec_data = read_text_with_metadata_from_pdfs(lec_doc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9727d34c-e965-4715-883a-4c9f834f2f1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 1,\n",
       "  'text': 'SuperGLUE: A Stickier Benchmark for\\nGeneral-Purpose Language Understanding Systems\\nAlex Wang∗\\nNew York University\\nYada Pruksachatkun∗\\nNew York University\\nNikita Nangia∗\\nNew York University\\nAmanpreet Singh∗\\nFacebook AI Research\\nJulian Michael\\nUniversity of Washington\\nFelix Hill\\nDeepMind\\nOmer Levy\\nFacebook AI Research\\nSamuel R. Bowman\\nNew York University\\nAbstract\\nIn the last year, new models and methods for pretraining and transfer learning have\\ndriven striking performance improvements across a range of language understand-\\ning tasks. The GLUE benchmark, introduced a little over one year ago, offers\\na single-number metric that summarizes progress on a diverse set of such tasks,\\nbut performance on the benchmark has recently surpassed the level of non-expert\\nhumans, suggesting limited headroom for further research. In this paper we present\\nSuperGLUE, a new benchmark styled after GLUE with a new set of more difﬁ-\\ncult language understanding tasks, a software toolkit, and a public leaderboard.\\nSuperGLUE is available at super.gluebenchmark.com.\\n1 Introduction\\nIn the past year, there has been notable progress across many natural language processing (NLP)\\ntasks, led by methods such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018),\\nand BERT (Devlin et al., 2019). The common thread connecting these methods is that they couple\\nself-supervised learning from massive unlabelled text corpora with a recipe for effectively adapting\\nthe resulting model to target tasks. The tasks that have proven amenable to this general approach\\ninclude question answering, sentiment analysis, textual entailment, and parsing, among many others\\n(Devlin et al., 2019; Kitaev and Klein, 2018, i.a.).\\nIn this context, the GLUE benchmark (Wang et al., 2019a) has become a prominent evaluation\\nframework for research towards general-purpose language understanding technologies. GLUE is\\na collection of nine language understanding tasks built on existing public datasets, together with\\nprivate test data, an evaluation server, a single-number target metric, and an accompanying expert-\\nconstructed diagnostic set. GLUE was designed to provide a general-purpose evaluation of language\\nunderstanding that covers a range of training data volumes, task genres, and task formulations. We\\nbelieve it was these aspects that made GLUE particularly appropriate for exhibiting the transfer-\\nlearning potential of approaches like OpenAI GPT and BERT.\\nThe progress of the last twelve months has eroded headroom on the GLUE benchmark dramatically.\\nWhile some tasks (Figure 1) and some linguistic phenomena (Figure 2 in Appendix B) measured\\nin GLUE remain difﬁcult, the current state of the art GLUE Score as of early July 2019 (88.4 from\\nYang et al., 2019) surpasses human performance (87.1 from Nangia and Bowman, 2019) by 1.3\\n∗Equal contribution. Correspondence: glue-benchmark-admin@googlegroups.com\\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 2,\n",
       "  'text': 'BiLSTM+ELMo+Attn\\nOpenAI GPT\\nBERT + Single-task Adapters\\nBERT (Large)\\nBERT on STILTs\\nBERT + BAM\\nSemBERT\\nSnorkel MeTaL\\nALICE (Large)\\nMT-DNN (ensemble)\\nXLNet-Large (ensemble)0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0\\n1.1\\n1.2\\nGLUE Score\\nHuman Performance\\nCoLA\\nSST-2\\nMRPC\\nSTS-B\\nQQP\\nMNLI\\nQNLI\\nRTE\\nWNLI\\nFigure 1: GLUE benchmark performance for submitted systems, rescaled to set human performance\\nto 1.0, shown as a single number score, and broken down into the nine constituent task performances.\\nFor tasks with multiple metrics, we use an average of the metrics. More information on the tasks\\nincluded in GLUE can be found in Wang et al. (2019a) and in Warstadt et al. (2018, CoLA), Socher\\net al. (2013, SST-2), Dolan and Brockett (2005, MRPC), Cer et al. (2017, STS-B), and Williams et al.\\n(2018, MNLI), and Rajpurkar et al. (2016, the original data source for QNLI).\\npoints, and in fact exceeds this human performance estimate on four tasks. Consequently, while there\\nremains substantial scope for improvement towards GLUE’s high-level goals, the original version of\\nthe benchmark is no longer a suitable metric for quantifying such progress.\\nIn response, we introduce SuperGLUE, a new benchmark designed to pose a more rigorous test of\\nlanguage understanding. SuperGLUE has the same high-level motivation as GLUE: to provide a\\nsimple, hard-to-game measure of progress toward general-purpose language understanding technolo-\\ngies for English. We anticipate that signiﬁcant progress on SuperGLUE should require substantive\\ninnovations in a number of core areas of machine learning, including sample-efﬁcient, transfer,\\nmultitask, and unsupervised or self-supervised learning.\\nSuperGLUE follows the basic design of GLUE: It consists of a public leaderboard built around\\neight language understanding tasks, drawing on existing data, accompanied by a single-number\\nperformance metric, and an analysis toolkit. However, it improves upon GLUE in several ways:\\n• More challenging tasks: SuperGLUE retains the two hardest tasks in GLUE. The remain-\\ning tasks were identiﬁed from those submitted to an open call for task proposals and were\\nselected based on difﬁculty for current NLP approaches.\\n• More diverse task formats: The task formats in GLUE are limited to sentence- and\\nsentence-pair classiﬁcation. We expand the set of task formats in SuperGLUE to include\\ncoreference resolution and question answering (QA).\\n• Comprehensive human baselines: We include human performance estimates for all bench-\\nmark tasks, which verify that substantial headroom exists between a strong BERT-based\\nbaseline and human performance.\\n• Improved code support: SuperGLUE is distributed with a new, modular toolkit for work\\non pretraining, multi-task learning, and transfer learning in NLP, built around standard tools\\nincluding PyTorch (Paszke et al., 2017) and AllenNLP (Gardner et al., 2017).\\n• Reﬁned usage rules: The conditions for inclusion on the SuperGLUE leaderboard have\\nbeen revamped to ensure fair competition, an informative leaderboard, and full credit\\nassignment to data and task creators.\\nThe SuperGLUE leaderboard, data, and software tools are available at super.gluebenchmark.com.\\n2'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 3,\n",
       "  'text': '2 Related Work\\nMuch work prior to GLUE demonstrated that training neural models with large amounts of available\\nsupervision can produce representations that effectively transfer to a broad range of NLP tasks\\n(Collobert and Weston, 2008; Dai and Le, 2015; Kiros et al., 2015; Hill et al., 2016; Conneau and\\nKiela, 2018; McCann et al., 2017; Peters et al., 2018). GLUE was presented as a formal challenge\\naffording straightforward comparison between such task-agnostic transfer learning techniques. Other\\nsimilarly-motivated benchmarks include SentEval (Conneau and Kiela, 2018), which speciﬁcally\\nevaluates ﬁxed-size sentence embeddings, and DecaNLP (McCann et al., 2018), which recasts a set\\nof target tasks into a general question-answering format and prohibits task-speciﬁc parameters. In\\ncontrast, GLUE provides a lightweight classiﬁcation API and no restrictions on model architecture or\\nparameter sharing, which seems to have been well-suited to recent work in this area.\\nSince its release, GLUE has been used as a testbed and showcase by the developers of several\\ninﬂuential models, including GPT (Radford et al., 2018) and BERT (Devlin et al., 2019). As shown\\nin Figure 1, progress on GLUE since its release has been striking. On GLUE, GPT and BERT\\nachieved scores of 72.8 and 80.2 respectively, relative to 66.5 for an ELMo-based model (Peters\\net al., 2018) and 63.7 for the strongest baseline with no multitask learning or pretraining above the\\nword level. Recent models (Liu et al., 2019d; Yang et al., 2019) have clearly surpassed estimates of\\nnon-expert human performance on GLUE (Nangia and Bowman, 2019). The success of these models\\non GLUE has been driven by ever-increasing model capacity, compute power, and data quantity, as\\nwell as innovations in model expressivity (from recurrent to bidirectional recurrent to multi-headed\\ntransformer encoders) and degree of contextualization (from learning representation of words in\\nisolation to using uni-directional contexts and ultimately to leveraging bidirectional contexts).\\nIn parallel to work scaling up pretrained models, several studies have focused on complementary\\nmethods for augmenting performance of pretrained models. Phang et al. (2018) show that BERT can\\nbe improved using two-stage pretraining, i.e., ﬁne-tuning the pretrained model on an intermediate\\ndata-rich supervised task before ﬁne-tuning it again on a data-poor target task. Liu et al. (2019d,c) and\\nBach et al. (2018) get further improvements respectively via multi-task ﬁnetuning and using massive\\namounts of weak supervision. Anonymous (2018) demonstrate that knowledge distillation (Hinton\\net al., 2015; Furlanello et al., 2018) can lead to student networks that outperform their teachers.\\nOverall, the quantity and quality of research contributions aimed at the challenges posed by GLUE\\nunderline the utility of this style of benchmark for machine learning researchers looking to evaluate\\nnew application-agnostic methods on language understanding.\\nLimits to current approaches are also apparent via the GLUE suite. Performance on the GLUE\\ndiagnostic entailment dataset, at 0.42 R3, falls far below the average human performance of 0.80\\nR3 reported in the original GLUE publication, with models performing near, or even below, chance\\non some linguistic phenomena (Figure 2, Appendix B). While some initially difﬁcult categories\\nsaw gains from advances on GLUE (e.g., double negation), others remain hard (restrictivity) or\\neven adversarial (disjunction, downward monotonicity). This suggests that even as unsupervised\\npretraining produces ever-better statistical summaries of text, it remains difﬁcult to extract many\\ndetails crucial to semantics without the right kind of supervision. Much recent work has made similar\\nobservations about the limitations of existing pretrained models (Jia and Liang, 2017; Naik et al.,\\n2018; McCoy and Linzen, 2019; McCoy et al., 2019; Liu et al., 2019a,b).\\n3 SuperGLUE Overview\\n3.1 Design Process\\nThe goal of SuperGLUE is to provide a simple, robust evaluation metric of any method capable of\\nbeing applied to a broad range of language understanding tasks. To that end, in designing SuperGLUE,\\nwe identify the following desiderata of tasks in the benchmark:\\n• Task substance: Tasks should test a system’s ability to understand and reason about texts\\nwritten in English.\\n• Task difﬁculty: Tasks should be beyond the scope of current state-of-the-art systems,\\nbut solvable by most college-educated English speakers. We exclude tasks that require\\ndomain-speciﬁc knowledge, e.g. medical notes or scientiﬁc papers.\\n3'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 4,\n",
       "  'text': 'Table 1: The tasks included in SuperGLUE. WSD stands for word sense disambiguation, NLI is\\nnatural language inference, coref. is coreference resolution, and QA is question answering. For\\nMultiRC, we list the number of total answers for 456/83/166 train/dev/test questions. The metrics for\\nMultiRC are binary F1 on all answer-options and exact match.\\nCorpus |Train| | Dev| | Test| Task Metrics Text Sources\\nBoolQ 9427 3270 3245 QA acc. Google queries, Wikipedia\\nCB 250 57 250 NLI acc./F1 various\\nCOPA 400 100 500 QA acc. blogs, photography encyclopedia\\nMultiRC 5100 953 1800 QA F1 a/EM various\\nReCoRD 101k 10k 10k QA F1/EM news (CNN, Daily Mail)\\nRTE 2500 278 300 NLI acc. news, Wikipedia\\nWiC 6000 638 1400 WSD acc. WordNet, VerbNet, Wiktionary\\nWSC 554 104 146 coref. acc. ﬁction books\\n• Evaluability: Tasks must have an automatic performance metric that corresponds well to\\nhuman judgments of output quality. Certain text generation tasks fail to meet this criteria\\ndue to issues surrounding automatic metrics like ROUGE and BLEU (Callison-Burch et al.,\\n2006; Liu et al., 2016, i.a.).\\n• Public data: We require that tasks haveexisting public training data in order to minimize\\nthe risks involved in newly-created datasets. We also prefer tasks for which we have access\\nto (or could create) a test set with private labels.\\n• Task format: We prefer tasks that had relatively simple input and output formats, to avoid\\nincentivizing the users of the benchmark to create complex task-speciﬁc model architectures.\\nNevertheless, while GLUE is restricted to tasks involving single sentence or sentence pair\\ninputs, for SuperGLUE we expand the scope to consider tasks with longer inputs. This\\nyields a set of tasks that requires understanding individual tokens in context, complete\\nsentences, inter-sentence relations, and entire paragraphs.\\n• License: We require that task data be available under licences that allow use and redistribu-\\ntion for research purposes.\\nTo identify possible tasks for SuperGLUE, we disseminated a public call for task proposals to the\\nNLP community, and received approximately 30 proposals. We ﬁltered these proposals according\\nto our criteria. Many proposals were not suitable due to licensing issues, complex formats, and\\ninsufﬁcient headroom; we provide examples of such tasks in Appendix D. For each of the remaining\\ntasks, we ran a BERT-based baseline and a human baseline, and ﬁltered out tasks which were either\\ntoo challenging for humans without extensive training or too easy for our machine baselines.\\n3.2 Selected Tasks\\nFollowing this process, we arrived at eight tasks to use in SuperGLUE. See Tables 1 and 2 for details\\nand speciﬁc examples of each task.\\nBoolQ (Boolean Questions, Clark et al., 2019) is a QA task where each example consists of a short\\npassage and a yes/no question about the passage. The questions are provided anonymously and\\nunsolicited by users of the Google search engine, and afterwards paired with a paragraph from a\\nWikipedia article containing the answer. Following the original work, we evaluate with accuracy.\\nCB (CommitmentBank, De Marneffe et al., 2019) is a corpus of short texts in which at least one\\nsentence contains an embedded clause. Each of these embedded clauses is annotated with the degree\\nto which it appears the person who wrote the text iscommitted to the truth of the clause. The resulting\\ntask framed as three-class textual entailment on examples that are drawn from the Wall Street Journal,\\nﬁction from the British National Corpus, and Switchboard. Each example consists of a premise\\ncontaining an embedded clause and the corresponding hypothesis is the extraction of that clause.\\nWe use a subset of the data that had inter-annotator agreement above80%. The data is imbalanced\\n(relatively fewer neutral examples), so we evaluate using accuracy and F1, where for multi-class F1\\nwe compute the unweighted average of the F1 per class.\\n4'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 5,\n",
       "  'text': 'Table 2: Development set examples from the tasks in SuperGLUE. Bold text represents part of the\\nexample format for each task. Text in italics is part of the model input. Underlined text is specially\\nmarked in the input. Text in a monospaced font represents the expected model output.\\nBoolQ\\nPassage: Barq’s – Barq’s is an American soft drink. Its brand of root beer is notable for having caffeine.\\nBarq’s, created by Edward Barq and bottled since the turn of the 20th century, is owned by the Barq\\nfamily but bottled by the Coca-Cola Company. It was known as Barq’s Famous Olde Tyme Root Beer\\nuntil 2012.\\nQuestion: is barq’s root beer a pepsi product Answer: No\\nCB\\nText: B: And yet, uh, I we-, I hope to see employer based, you know, helping out. You know, child, uh,\\ncare centers at the place of employment and things like that, that will help out. A: Uh-huh. B: What do\\nyou think, do you think we are, setting a trend?\\nHypothesis: they are setting a trend Entailment: Unknown\\nCOPA\\nPremise: My body cast a shadow over the grass. Question: What’s the CAUSE for this?\\nAlternative 1: The sun was rising. Alternative 2: The grass was cut.\\nCorrect Alternative: 1\\nMultiRC\\nParagraph: Susan wanted to have a birthday party. She called all of her friends. She has ﬁve friends.\\nHer mom said that Susan can invite them all to the party. Her ﬁrst friend could not go to the party\\nbecause she was sick. Her second friend was going out of town. Her third friend was not so sure if her\\nparents would let her. The fourth friend said maybe. The ﬁfth friend could go to the party for sure. Susan\\nwas a little sad. On the day of the party, all ﬁve friends showed up. Each friend had a present for Susan.\\nSusan was happy and sent each friend a thank you card the next week\\nQuestion: Did Susan’s sick friend recover? Candidate answers: Yes, she recovered (T), No (F), Yes\\n(T), No, she didn’t recover(F), Yes, she was at Susan’s party(T)\\nReCoRD\\nParagraph: (CNN) Puerto Rico on Sunday overwhelmingly voted for statehood. But Congress, the only\\nbody that can approve new states, will ultimately decide whether the status of the US commonwealth\\nchanges. Ninety-seven percent of the votes in the nonbinding referendum favored statehood, an increase\\nover the results of a 2012 referendum, ofﬁcial results from the State Electorcal Commission show. It\\nwas the ﬁfth such vote on statehood. \"Today, we the people of Puerto Rico are sending a strong and\\nclear message to the US Congress ... and to the world ... claiming our equal rights as American citizens,\\nPuerto Rico Gov. Ricardo Rossello said in a news release. @highlight Puerto Rico voted Sunday in\\nfavor of US statehood\\nQuery For one, they can truthfully say, “Don’t blame me, I didn’t vote for them, ” when discussing the\\n<placeholder> presidency Correct Entities: US\\nRTE\\nText: Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44,\\naccording to the Christopher Reeve Foundation.\\nHypothesis: Christopher Reeve had an accident. Entailment: False\\nWiC\\nContext 1: Room and board. Context 2: He nailed boards across the windows.\\nSense match: False\\nWSC\\nText: Mark told Pete many lies about himself, which Pete included in his book. He should have been\\nmore truthful. Coreference: False\\nCOPA (Choice of Plausible Alternatives, Roemmele et al., 2011) is a causal reasoning task in which\\na system is given a premise sentence and must determine either the cause or effect of the premise\\nfrom two possible choices. All examples are handcrafted and focus on topics from blogs and a\\nphotography-related encyclopedia. Following the original work, we evaluate using accuracy.\\nMultiRC (Multi-Sentence Reading Comprehension, Khashabi et al., 2018) is a QA task where each\\nexample consists of a context paragraph, a question about that paragraph, and a list of possible\\nanswers. The system must predict which answers are true and which are false. While many QA\\ntasks exist, we use MultiRC because of a number of desirable properties: (i) each question can have\\nmultiple possible correct answers, so each question-answer pair must be evaluated independent of\\nother pairs, (ii) the questions are designed such that answering each question requires drawing facts\\nfrom multiple context sentences, and (iii) the question-answer pair format more closely matches\\nthe API of other tasks in SuperGLUE than the more popular span-extractive QA format does. The\\nparagraphs are drawn from seven domains including news, ﬁction, and historical text. The evaluation\\nmetrics are F1 over all answer-options (F1a) and exact match of each question’s set of answers (EM).\\n5'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 6,\n",
       "  'text': 'ReCoRD (Reading Comprehension with Commonsense Reasoning Dataset, Zhang et al., 2018) is a\\nmultiple-choice QA task. Each example consists of a news article and a Cloze-style question about\\nthe article in which one entity is masked out. The system must predict the masked out entity from a\\ngiven list of possible entities in the provided passage, where the same entity may be expressed using\\nmultiple different surface forms, all of which are considered correct. Articles are drawn from CNN\\nand Daily Mail. Following the original work, we evaluate with max (over all mentions) token-level\\nF1 and exact match (EM).\\nRTE (Recognizing Textual Entailment) datasets come from a series of annual competitions on textual\\nentailment.2 RTE is included in GLUE, and we use the same data and format as GLUE: We merge data\\nfrom RTE1 (Dagan et al., 2006), RTE2 (Bar Haim et al., 2006), RTE3 (Giampiccolo et al., 2007), and\\nRTE5 (Bentivogli et al., 2009).3 All datasets are combined and converted to two-class classiﬁcation:\\nentailment and not_entailment. Of all the GLUE tasks, RTE is among those that beneﬁts from\\ntransfer learning the most, with performance jumping from near random-chance (∼56%) at the time\\nof GLUE’s launch to 86.3% accuracy (Liu et al., 2019d; Yang et al., 2019) at the time of writing.\\nGiven the nearly eight point gap with respect to human performance, however, the task is not yet\\nsolved by machines, and we expect the remaining gap to be difﬁcult to close.\\nWiC (Word-in-Context, Pilehvar and Camacho-Collados, 2019) is a word sense disambiguation task\\ncast as binary classiﬁcation of sentence pairs. Given two text snippets and a polysemous word that\\nappears in both sentences, the task is to determine whether the word is used with the same sense in\\nboth sentences. Sentences are drawn from WordNet (Miller, 1995), VerbNet (Schuler, 2005), and\\nWiktionary. We follow the original work and evaluate using accuracy.\\nWSC (Winograd Schema Challenge, Levesque et al., 2012) is a coreference resolution task in\\nwhich examples consist of a sentence with a pronoun and a list of noun phrases from the sentence.\\nThe system must determine the correct referrent of the pronoun from among the provided choices.\\nWinograd schemas are designed to require everyday knowledge and commonsense reasoning to solve.\\nGLUE includes a version of WSC recast as NLI, known as WNLI. Until very recently, no substantial\\nprogress had been made on WNLI, with many submissions opting to submit majority class predic-\\ntions.4 In the past few months, several works (Kocijan et al., 2019; Liu et al., 2019d) have made rapid\\nprogress via a hueristic data augmentation scheme, raising machine performance to 90.4% accuracy.\\nGiven estimated human performance of ∼96%, there is still a gap between machine and human\\nperformance, which we expect will be relatively difﬁcult to close. We therefore include a version of\\nWSC cast as binary classiﬁcation, where each example consists of a sentence with a marked pronoun\\nand noun, and the task is to determine if the pronoun refers to that noun. The training and validation\\nexamples are drawn from the original WSC data (Levesque et al., 2012), as well as those distributed\\nby the afﬁliated organization Commonsense Reasoning.5 The test examples are derived from ﬁction\\nbooks and have been shared with us by the authors of the original dataset. We evaluate using accuracy.\\n3.3 Scoring\\nAs with GLUE, we seek to give a sense of aggregate system performance over all tasks by averaging\\nscores of all tasks. Lacking a fair criterion with which to weight the contributions of each task to\\nthe overall score, we opt for the simple approach of weighing each task equally, and for tasks with\\nmultiple metrics, ﬁrst averaging those metrics to get a task score.\\n3.4 Tools for Model Analysis\\nAnalyzing Linguistic and World Knowledge in Models GLUE includes an expert-constructed,\\ndiagnostic dataset that automatically tests models for a broad range of linguistic, commonsense, and\\nworld knowledge. Each example in this broad-coverage diagnostic is a sentence pair labeled with\\n2Textual entailment is also known as natural language inference, or NLI\\n3RTE4 is not publicly available, while RTE6 and RTE7 do not conform to the standard NLI task.\\n4WNLI is especially difﬁcult due to an adversarial train/dev split: Premise sentences that appear in the\\ntraining set often appear in the development set with a different hypothesis and a ﬂipped label. If a system\\nmemorizes the training set, which was easy due to the small size of the training set, it could perform far below\\nchance on the development set. We remove this adversarial design in our version of WSC by ensuring that no\\nsentences are shared between the training, validation, and test sets.\\n5http://commonsensereasoning.org/disambiguation.html\\n6'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 7,\n",
       "  'text': 'a three-way entailment relation (entailment, neutral, or contradiction) and tagged with labels that\\nindicate the phenomena that characterize the relationship between the two sentences. Submissions\\nto the GLUE leaderboard are required to include predictions from the submission’s MultiNLI\\nclassiﬁer on the diagnostic dataset, and analyses of the results were shown alongside the main\\nleaderboard. Since this broad-coverage diagnostic task has proved difﬁcult for top models, we retain\\nit in SuperGLUE. However, since MultiNLI is not part of SuperGLUE, we collapse contradiction\\nand neutral into a single not_entailment label, and request that submissions include predictions\\non the resulting set from the model used for the RTE task. We collect non-expert annotations to\\nestimate human performance, following the same procedure we use for the main benchmark tasks\\n(Section 5.2). We estimate an accuracy of 88% and a Matthew’s correlation coefﬁcient (MCC, the\\ntwo-class variant of the R3 metric used in GLUE) of 0.77.\\nAnalyzing Gender Bias in Models Recent work has identiﬁed the presence and ampliﬁcation\\nof many social biases in data-driven machine learning models. (Lu et al., 2018; Zhao et al., 2018;\\nKiritchenko and Mohammad, 2018). To promote the detection of such biases, we include Winogender\\n(Rudinger et al., 2018) as an additional diagnostic dataset. Winogender is designed to measure gender\\nbias in coreference resolution systems. We use the Diverse Natural Language Inference Collection\\n(DNC; Poliak et al., 2018) version that casts Winogender as a textual entailment task.6 Each example\\nconsists of a premise sentence with a male or female pronoun and a hypothesis giving a possible\\nantecedent of the pronoun. Examples occur in minimal pairs, where the only difference between\\nan example and its pair is the gender of the pronoun in the premise. Performance on Winogender\\nis measured with both accuracy and the gender parity score: the percentage of minimal pairs for\\nwhich the predictions are the same. We note that a system can trivially obtain a perfect gender parity\\nscore by guessing the same class for all examples, so a high gender parity score is meaningless unless\\naccompanied by high accuracy. We collect non-expert annotations to estimate human performance,\\nand observe an accuracy of 99.7% and a gender parity score of 0.99.\\nLike any diagnostic, Winogender has limitations. It offers only positive predictive value: A poor\\nbias score is clear evidence that a model exhibits gender bias, but a good score does not mean that\\nthe model is unbiased. More speciﬁcally, in the DNC version of the task, a low gender parity score\\nmeans that a model’s prediction of textual entailment can be changed with a change in pronouns, all\\nelse equal. It is plausible that there are forms of bias that are relevant to target tasks of interest, but\\nthat do not surface in this setting (Gonen and Goldberg, 2019). In addition, Winogender does not\\ncover all forms of social bias, or even all forms of gender. For instance, the version of the data used\\nhere offers no coverage of gender-neutral they or non-binary pronouns. Despite these limitations, we\\nbelieve that Winogender’s inclusion is worthwhile in providing a coarse sense of how social biases\\nevolve with model performance and for keeping attention on the social ramiﬁcations of NLP models.\\n4 Using SuperGLUE\\nSoftware Tools To facilitate using SuperGLUE, we releasejiant (Wang et al., 2019b),7 a modular\\nsoftware toolkit, built with PyTorch (Paszke et al., 2017), components from AllenNLP (Gardner\\net al., 2017), and the pytorch-pretrained-bert package.8 jiant implements our baselines and\\nsupports the evaluation of custom models and training methods on the benchmark tasks. The toolkit\\nincludes support for existing popular pretrained models such as OpenAI GPT and BERT, as well as\\nsupport for multistage and multitask learning of the kind seen in the strongest models on GLUE.\\nEligibility Any system or method that can produce predictions for the SuperGLUE tasks is eligible\\nfor submission to the leaderboard, subject to the data-use and submission frequency policies stated\\nimmediately below. There are no restrictions on the type of methods that may be used, and there is\\nno requirement that any form of parameter sharing or shared initialization be used across the tasks in\\nthe benchmark. To limit overﬁtting to the private test data, users are limited to a maximum of two\\nsubmissions per day and six submissions per month.\\n6We ﬁlter out 23 examples where the labels are ambiguous\\n7https://github.com/nyu-mll/jiant\\n8https://github.com/huggingface/pytorch-pretrained-BERT\\n7'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 8,\n",
       "  'text': 'Table 3: Baseline performance on the SuperGLUE test sets and diagnostics. For CB we report\\naccuracy and macro-average F1. For MultiRC we report F1 on all answer-options and exact match\\nof each question’s set of correct answers. AXb is the broad-coverage diagnostic task, scored using\\nMatthews’ correlation (MCC). AXg is the Winogender diagnostic, scored using accuracy and the\\ngender parity score (GPS). All values are scaled by 100. The Avg column is the overall benchmark\\nscore on non-AX∗ tasks. The bolded numbers reﬂect the best machine performance on task. *MultiRC\\nhas multiple test sets released on a staggered schedule, and these results evaluate on an installation of\\nthe test set that is a subset of ours.\\nModel Avg BoolQ CB COPA MultiRC ReCoRD RTE WiC WSC AX b AXg\\nMetrics Acc. F1/Acc. Acc. F1 a/EM F1/EM Acc. Acc. Acc. MCC GPS Acc.\\nMost Frequent 47.1 62.3 21.7/48.4 50.0 61.1 / 0.3 33.4/32.5 50.3 50.0 65.1 0.0 100.0/ 50.0\\nCBoW 44.3 62.1 49.0/71.2 51.6 0.0 / 0.4 14.0/13.6 49.7 53.0 65.1 -0.4 100.0/ 50.0\\nBERT 69.0 77.4 75.7/83.6 70.6 70.0 / 24.0 72.0/71.3 71.6 69.5 64.3 23.0 97.8 / 51.7\\nBERT++ 71.5 79.0 84.7/90.4 73.8 70.0 / 24.1 72.0/71.3 79.0 69.5 64.3 38.0 99.4 / 51.4\\nOutside Best - 80.4 - / - 84.4 70.4 */24.5* 74.8/73.0 82.7 - - - - / -\\nHuman (est.) 89.8 89.0 95.8/98.9 100.0 81.8*/51.9* 91.7/91.3 93.6 80.0 100.0 77.0 99.3 / 99.7\\nData Data for the tasks are available for download through the SuperGLUE site and through a\\ndownload script included with the software toolkit. Each task comes with a standardized training set,\\ndevelopment set, and unlabeled test set. Submitted systems may use any public or private data when\\ndeveloping their systems, with a few exceptions: Systems may only use the SuperGLUE-distributed\\nversions of the task datasets, as these use different train/validation/test splits from other public\\nversions in some cases. Systems also may not use the unlabeled test data for the tasks in system\\ndevelopment in any way, may not use the structured source data that was used to collect the WiC\\nlabels (sense-annotated example sentences from WordNet, VerbNet, and Wiktionary) in any way, and\\nmay not build systems that share information across separate test examples in any way.\\nWe do not endorse the use of the benchmark data for non-research applications, due to concerns\\nabout socially relevant biases (such as ethnicity–occupation associations) that may be undesirable\\nor legally problematic in deployed systems. Because these biases are evident in texts from a wide\\nvariety of sources and collection methods (e.g., Rudinger et al., 2017), and because none of our task\\ndatasets directly mitigate them, one can reasonably presume that our training sets teach models these\\nbiases to some extent and that our evaluation sets similarly reward models that learn these biases.\\nTo ensure reasonable credit assignment, because we build very directly on prior work, we ask the\\nauthors of submitted systems to directly name and cite the speciﬁc datasets that they use,including the\\nbenchmark datasets. We will enforce this as a requirement for papers to be listed on the leaderboard.\\n5 Experiments\\n5.1 Baselines\\nBERT Our main baselines are built around BERT, variants of which are among the most successful\\napproach on GLUE at the time of writing. Speciﬁcally, we use the bert-large-cased variant.\\nFollowing the practice recommended in Devlin et al. (2019), for each task, we use the simplest\\npossible architecture on top of BERT. We ﬁne-tune a copy of the pretrained BERT model separately\\nfor each task, and leave the development of multi-task learning models to future work. For training,\\nwe use the procedure speciﬁed in Devlin et al. (2019): We use Adam (Kingma and Ba, 2014) with an\\ninitial learning rate of 10−5 and ﬁne-tune for a maximum of 10 epochs.\\nFor classiﬁcation tasks with sentence-pair inputs (BoolQ, CB, RTE, WiC), we concatenate the\\nsentences with a [SEP ] token, feed the fused input to BERT, and use a logistic regression classiﬁer that\\nsees the representation corresponding to [CLS ]. For WiC only, we also concatenate the representation\\nof the marked word to the [CLS ] representation. For COPA, MultiRC, and ReCoRD, for each answer\\nchoice, we similarly concatenate the context with that answer choice and feed the resulting sequence\\ninto BERT to produce an answer representation. For COPA, we project these representations into a\\nscalar, and take as the answer the choice with the highest associated scalar. For MultiRC, because\\neach question can have more than one correct answer, we feed each answer representation into\\n8'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 9,\n",
       "  'text': 'a logistic regression classiﬁer. For ReCoRD, we also evaluate the probability of each candidate\\nindependent of other candidates, and take the most likely candidate as the model’s prediction. For\\nWSC, which is a span-based task, we use a model inspired by Tenney et al. (2019). Given the BERT\\nrepresentation for each word in the original sentence, we get span representations of the pronoun\\nand noun phrase via a self-attention span-pooling operator (Lee et al., 2017), before feeding it into a\\nlogistic regression classiﬁer.\\nBERT++ We also report results using BERT with additional training on related datasets before\\nﬁne-tuning on the benchmark tasks, following the STILTs two-stage style of transfer learning (Phang\\net al., 2018). Given the productive use of MultiNLI in pretraining and intermediate ﬁne-tuning of\\npretrained language models (Conneau et al., 2017; Phang et al., 2018, i.a.), for CB, RTE, and BoolQ,\\nwe use MultiNLI as a transfer task by ﬁrst using the above procedure on MultiNLI. Similarly, given\\nthe similarity of COPA to SWAG (Zellers et al., 2018), we ﬁrst ﬁne-tune BERT on SWAG. These\\nresults are reported as BERT++. For all other tasks, we reuse the results of BERT ﬁne-tuned on just\\nthat task.\\nSimple Baselines We include a baseline where for each task we simply predict the majority class,9\\nas well as a bag-of-words baseline where each input is represented as an average of its tokens’ GloVe\\nword vectors (the 300D/840B release from Pennington et al., 2014).\\nOutside Best We list the best known result on each task to date, except on tasks which we recast\\n(WSC), resplit (CB), or achieve the best known result (WiC). The outside results for COPA, MultiRC,\\nand RTE are from Sap et al. (2019), Trivedi et al. (2019), and Liu et al. (2019d) respectively.\\n5.2 Human Performance\\nPilehvar and Camacho-Collados (2019), Khashabi et al. (2018), Nangia and Bowman (2019), and\\nZhang et al. (2018) respectively provide estimates for human performance on WiC, MultiRC, RTE,\\nand ReCoRD. For the remaining tasks, including the diagnostic set, we estimate human performance\\nby hiring crowdworker annotators through Amazon’s Mechanical Turk platform to reannotate a\\nsample of each test set. We follow a two step procedure where a crowd worker completes a short\\ntraining phase before proceeding to the annotation phase, modeled after the method used by Nangia\\nand Bowman (2019) for GLUE. For both phases and all tasks, the average pay rate is $23.75/hr.10\\nIn the training phase, workers are provided with instructions on the task, linked to an FAQ page, and\\nare asked to annotate up to 30 examples from the development set. After answering each example,\\nworkers are also asked to check their work against the provided ground truth label. After the training\\nphase is complete, we provide the qualiﬁcation to work on the annotation phase to all workers\\nwho annotated a minimum of ﬁve examples, i.e. completed ﬁve HITs during training and achieved\\nperformance at, or above the median performance across all workers during training.\\nIn the annotation phase, workers are provided with the same instructions as the training phase, and\\nare linked to the same FAQ page. The instructions for all tasks are provided in Appendix C. For the\\nannotation phase we randomly sample 100 examples from the task’s test set, with the exception of\\nWSC where we annotate the full test set. For each example, we collect annotations from ﬁve workers\\nand take a majority vote to estimate human performance. For additional details, see Appendix C.3.\\n5.3 Results\\nTable 3 shows results for all baselines. The simple baselines of predicting the most frequent class\\nand CBOW do not perform well overall, achieving near chance performance for several of the tasks.\\nUsing BERT increases the average SuperGLUE score by 25 points, attaining signiﬁcant gains on\\nall of the benchmark tasks, particularly MultiRC, ReCoRD, and RTE. On WSC, BERT actually\\nperforms worse than the simple baselines, likely due to the small size of the dataset and the lack of\\ndata augmentation. Using MultiNLI as an additional source of supervision for BoolQ, CB, and RTE\\nleads to a 2-5 point improvement on all tasks. Using SWAG as a transfer task for COPA sees an 8\\npoint improvement.\\n9For ReCoRD, we predict the entity that has the highest F1 with the other entity options.\\n10This estimate is taken from https://turkerview.com.\\n9'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 10,\n",
       "  'text': 'Our best baselines still lag substantially behind human performance. On average, there is a nearly 20\\npoint gap between BERT++ and human performance. The largest gap is on WSC, with a 35 point\\ndifference between the best model and human performance. The smallest margins are on BoolQ,\\nCB, RTE, and WiC, with gaps of around 10 points on each of these. We believe these gaps will be\\nchallenging to close: On WSC and COPA, human performance is perfect. On three other tasks, it is\\nin the mid-to-high 90s. On the diagnostics, all models continue to lag signiﬁcantly behind humans.\\nThough all models obtain near perfect gender parity scores on Winogender, this is due to the fact that\\nthey are obtaining accuracy near that of random guessing.\\n6 Conclusion\\nWe present SuperGLUE, a new benchmark for evaluating general-purpose language understanding\\nsystems. SuperGLUE updates the GLUE benchmark by identifying a new set of challenging NLU\\ntasks, as measured by the difference between human and machine baselines. The set of eight tasks in\\nour benchmark emphasizes diverse task formats and low-data training data tasks, with nearly half the\\ntasks having fewer than 1k examples and all but one of the tasks having fewer than 10k examples.\\nWe evaluate BERT-based baselines and ﬁnd that they still lag behind humans by nearly 20 points.\\nGiven the difﬁculty of SuperGLUE for BERT, we expect that further progress in multi-task, transfer,\\nand unsupervised/self-supervised learning techniques will be necessary to approach human-level per-\\nformance on the benchmark. Overall, we argue that SuperGLUE offers a rich and challenging testbed\\nfor work developing new general-purpose machine learning methods for language understanding.\\n7 Acknowledgments\\nWe thank the original authors of the included datasets in SuperGLUE for their cooperation in the\\ncreation of the benchmark, as well as those who proposed tasks and datasets that we ultimately could\\nnot include.\\nThis work was made possible in part by a donation to NYU from Eric and Wendy Schmidt made by\\nrecommendation of the Schmidt Futures program. We gratefully acknowledge the support of the\\nNVIDIA Corporation with the donation of a Titan V GPU used at NYU for this research. AW is\\nsupported by the National Science Foundation Graduate Research Fellowship Program under Grant\\nNo. DGE 1342536. Any opinions, ﬁndings, and conclusions or recommendations expressed in this\\nmaterial are those of the author(s) and do not necessarily reﬂect the views of the National Science\\nFoundation.\\nReferences\\nAnonymous. Bam! Born-again multi-task networks for natural language understanding. Anonymous\\npreprint under review, 2018. URL https://openreview.net/forum?id=SylnYlqKw4.\\nStephen H. Bach, Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao, Cassandra Xia, Souvik\\nSen, Alexander Ratner, Braden Hancock, Houman Alborzi, Rahul Kuchhal, Christopher Ré, and\\nRob Malkin. Snorkel drybell: A case study in deploying weak supervision at industrial scale. In\\nSIGMOD. ACM, 2018.\\nRoy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and\\nIdan Szpektor. The second PASCAL recognising textual entailment challenge. In Proceedings\\nof the Second PASCAL Challenges Workshop on Recognising Textual Entailment, 2006. URL\\nhttp://u.cs.biu.ac.il/~nlp/RTE2/Proceedings/01.pdf.\\nLuisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. The\\nﬁfth PASCAL recognizing textual entailment challenge. In Textual Analysis Conference (TAC),\\n2009. URL http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.232.1231.\\nSven Buechel, Anneke Buffone, Barry Slaff, Lyle Ungar, and João Sedoc. Modeling empathy and\\ndistress in reaction to news stories. In Proceedings of the 2018 Conference on Empirical Methods\\nin Natural Language Processing (EMNLP), 2018.\\n10'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 11,\n",
       "  'text': 'Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluation the role of bleu in machine\\ntranslation research. In Proceedings of the Conference of the European Chapter of the Association\\nfor Computational Linguistics (EACL). Association for Computational Linguistics, 2006. URL\\nhttps://www.aclweb.org/anthology/E06-1032.\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task\\n1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings\\nof the 11th International Workshop on Semantic Evaluation (SemEval-2017) . Association for\\nComputational Linguistics, 2017. doi: 10.18653/v1/S17-2001. URL https://www.aclweb.\\norg/anthology/S17-2001.\\nEunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke\\nZettlemoyer. QuAC: Question answering in context. In Proceedings of the 2018 Conference on\\nEmpirical Methods in Natural Language Processing (EMNLP). Association for Computational\\nLinguistics, 2018a.\\nEunsol Choi, Omer Levy, Yejin Choi, and Luke Zettlemoyer. Ultra-ﬁne entity typing. In Proceedings\\nof the Association for Computational Linguistics (ACL). Association for Computational Linguistics,\\n2018b. URL https://www.aclweb.org/anthology/P18-1009.\\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\\nToutanova. Boolq: Exploring the surprising difﬁculty of natural yes/no questions. In Proceedings\\nof the 2019 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924–2936,\\n2019.\\nRonan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: Deep\\nneural networks with multitask learning. In Proceedings of the 25th International Conference on\\nMachine Learning (ICML). Association for Computing Machinery, 2008. URL https://dl.acm.\\norg/citation.cfm?id=1390177.\\nAlexis Conneau and Douwe Kiela. SentEval: An evaluation toolkit for universal sentence representa-\\ntions. In Proceedings of the 11th Language Resources and Evaluation Conference. European Lan-\\nguage Resource Association, 2018. URL https://www.aclweb.org/anthology/L18-1269.\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes. Super-\\nvised learning of universal sentence representations from natural language inference data. In\\nProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing\\n(EMNLP). Association for Computational Linguistics, 2017. doi: 10.18653/v1/D17-1070. URL\\nhttps://www.aclweb.org/anthology/D17-1070.\\nIdo Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entail-\\nment challenge. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Vi-\\nsual Object Classiﬁcation, and Recognising Textual Entailment . Springer, 2006. URL https:\\n//link.springer.com/chapter/10.1007/11736790_9.\\nAndrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in Neural\\nInformation Processing Systems (NeurIPS). Curran Associates, Inc., 2015. URL http://papers.\\nnips.cc/paper/5949-semi-supervised-sequence-learning.pdf .\\nMarie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The CommitmentBank:\\nInvestigating projection in naturally occurring discourse. 2019. To appear in Proceedings of Sinn\\nund Bedeutung 23. Data can be found at https://github.com/mcdm/CommitmentBank/.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\\nbidirectional transformers for language understanding. In Proceedings of the Conference of the\\nNorth American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies (NAACL-HLT). Association for Computational Linguistics, 2019. URL https:\\n//arxiv.org/abs/1810.04805.\\nWilliam B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.\\nIn Proceedings of IWP, 2005.\\n11'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 12,\n",
       "  'text': 'Manaal Faruqui and Dipanjan Das. Identifying well-formed natural language questions. In Pro-\\nceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP) .\\nAssociation for Computational Linguistics, 2018. URLhttps://www.aclweb.org/anthology/\\nD18-1091.\\nTommaso Furlanello, Zachary C Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar.\\nBorn again neural networks. International Conference on Machine Learning (ICML), 2018. URL\\nhttp://proceedings.mlr.press/v80/furlanello18a.html.\\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew\\nPeters, Michael Schmitz, and Luke S. Zettlemoyer. AllenNLP: A deep semantic natural language\\nprocessing platform. In Proceedings of Workshop for NLP Open Source Software, 2017. URL\\nhttps://www.aclweb.org/anthology/W18-2501.\\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing\\ntextual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment\\nand Paraphrasing. Association for Computational Linguistics, 2007.\\nHila Gonen and Yoav Goldberg. Lipstick on a pig: Debiasing methods cover up systematic\\ngender biases in word embeddings but do not remove them. In Proceedings of the 2019\\nConference of the North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies, Volume 1 (Long and Short Papers) , pages 609–614, Min-\\nneapolis, Minnesota, June 2019. Association for Computational Linguistics. URL https:\\n//www.aclweb.org/anthology/N19-1061.\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences\\nfrom unlabelled data. In Proceedings of the Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).\\nAssociation for Computational Linguistics, 2016. doi: 10.18653/v1/N16-1162. URL https:\\n//www.aclweb.org/anthology/N16-1162.\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv\\npreprint 1503.02531, 2015. URL https://arxiv.org/abs/1503.02531.\\nRobin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems. In\\nProceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).\\nAssociation for Computational Linguistics, 2017. doi: 10.18653/v1/D17-1215. URL https:\\n//www.aclweb.org/anthology/D17-1215.\\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking\\nbeyond the surface: A challenge set for reading comprehension over multiple sentences. In\\nProceedings of the Conference of the North American Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies (NAACL-HLT). Association for Computational\\nLinguistics, 2018. URL https://www.aclweb.org/anthology/papers/N/N18/N18-1023/.\\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\\n1412.6980, 2014. URL https://arxiv.org/abs/1412.6980.\\nSvetlana Kiritchenko and Saif Mohammad. Examining gender and race bias in two hundred sentiment\\nanalysis systems. In Proceedings of the Seventh Joint Conference on Lexical and Computational\\nSemantics. Association for Computational Linguistics, 2018. doi: 10.18653/v1/S18-2005. URL\\nhttps://www.aclweb.org/anthology/S18-2005.\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. Skip-thought vectors. In Advances in neural information processing systems,\\n2015.\\nNikita Kitaev and Dan Klein. Multilingual constituency parsing with self-attention and pre-training.\\narXiv preprint 1812.11760, 2018. URL https://arxiv.org/abs/1812.11760.\\nVid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas Lukasiewicz.\\nA surprisingly robust trick for winograd schema challenge. arXiv preprint 1905.06290, 2019.\\n12'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 13,\n",
       "  'text': 'Kenton Lee, Luheng He, Mike Lewis, and Luke Zettlemoyer. End-to-end neural coreference\\nresolution. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language\\nProcessing. Association for Computational Linguistics, September 2017. doi: 10.18653/v1/\\nD17-1018. URL https://www.aclweb.org/anthology/D17-1018.\\nHector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge. In\\nThirteenth International Conference on the Principles of Knowledge Representation and Reasoning,\\n2012. URL http://dl.acm.org/citation.cfm?id=3031843.3031909.\\nChia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, and Joelle Pineau.\\nHow not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics\\nfor dialogue response generation. In Proceedings of the 2016 Conference on Empirical Methods in\\nNatural Language Processing. Association for Computational Linguistics, 2016. doi: 10.18653/\\nv1/D16-1230. URL https://www.aclweb.org/anthology/D16-1230.\\nNelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. Linguistic\\nknowledge and transferability of contextual representations. In Proceedings of the Conference of\\nthe North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies (NAACL-HLT). Association for Computational Linguistics, 2019a. URL https:\\n//arxiv.org/abs/1903.08855.\\nNelson F. Liu, Roy Schwartz, and Noah A. Smith. Inoculation by ﬁne-tuning: A method for\\nanalyzing challenge datasets. In Proceedings of the Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Technologies (NAACL-\\nHLT). Association for Computational Linguistics, 2019b. URL https://arxiv.org/abs/1904.\\n02668.\\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Improving multi-task deep neural\\nnetworks via knowledge distillation for natural language understanding.arXiv preprint 1904.09482,\\n2019c. URL http://arxiv.org/abs/1904.09482.\\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for\\nnatural language understanding. arXiv preprint 1901.11504, 2019d.\\nKaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Amancharla, and Anupam Datta. Gender bias in\\nneural natural language processing. arXiv preprint 1807.11714, 2018. URL http://arxiv.org/\\nabs/1807.11714.\\nBryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in transla-\\ntion: Contextualized word vectors. In Advances in Neural Information Processing Sys-\\ntems (NeurIPS) . Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/\\n7209-learned-in-translation-contextualized-word-vectors.pdf .\\nBryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language\\ndecathlon: Multitask learning as question answering. arXiv preprint 1806.08730, 2018. URL\\nhttps://arxiv.org/abs/1806.08730.\\nR. Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic\\nheuristics in natural language inference. In Proceedings of the Association for Computational\\nLinguistics (ACL). Association for Computational Linguistics, 2019. URL https://arxiv.org/\\nabs/1902.01007.\\nRichard T. McCoy and Tal Linzen. Non-entailed subsequences as a challenge for natural language\\ninference. In Proceedings of the Society for Computational in Linguistics (SCiL) 2019, 2019. URL\\nhttps://scholarworks.umass.edu/scil/vol2/iss1/46/.\\nGeorge A Miller. WordNet: a lexical database for english. Communications of the ACM, 1995. URL\\nhttps://www.aclweb.org/anthology/H94-1111.\\nAakanksha Naik, Abhilasha Ravichander, Norman M. Sadeh, Carolyn Penstein Rosé, and Graham\\nNeubig. Stress test evaluation for natural language inference. In International Conference on\\nComputational Linguistics (COLING), 2018.\\n13'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 14,\n",
       "  'text': 'Nikita Nangia and Samuel R. Bowman. Human vs. Muppet: A conservative estimate of hu-\\nman performance on the GLUE benchmark. In Proceedings of the Association of Compu-\\ntational Linguistics (ACL) . Association for Computational Linguistics, 2019. URL https:\\n//woollysocks.github.io/assets/GLUE_Human_Baseline.pdf.\\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\\nZeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in\\nPyTorch. In Advances in Neural Information Processing Systems (NeurIPS). Curran Associates,\\nInc., 2017. URL https://openreview.net/pdf?id=BJJsrmfCZ.\\nJeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word\\nrepresentation. In Proceedings of the Conference on Empirical Methods in Natural Language Pro-\\ncessing (EMNLP). Association for Computational Linguistics, 2014. doi: 10.3115/v1/D14-1162.\\nURL https://www.aclweb.org/anthology/D14-1162.\\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\\nLuke Zettlemoyer. Deep contextualized word representations. In Proceedings of the Conference of\\nthe North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies (NAACL-HLT). Association for Computational Linguistics, 2018. doi: 10.18653/v1/\\nN18-1202. URL https://www.aclweb.org/anthology/N18-1202.\\nJason Phang, Thibault Févry, and Samuel R Bowman. Sentence encoders on STILTs: Supplementary\\ntraining on intermediate labeled-data tasks. arXiv preprint 1811.01088 , 2018. URL https:\\n//arxiv.org/abs/1811.01088.\\nMohammad Taher Pilehvar and Jose Camacho-Collados. WiC: The word-in-context dataset for\\nevaluating context-sensitive meaning representations. In Proceedings of the Conference of the\\nNorth American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies (NAACL-HLT). Association for Computational Linguistics, 2019. URL https:\\n//arxiv.org/abs/1808.09121.\\nAdam Poliak, Aparajita Haldar, Rachel Rudinger, J. Edward Hu, Ellie Pavlick, Aaron Steven White,\\nand Benjamin Van Durme. Collecting diverse natural language inference problems for sentence\\nrepresentation evaluation. In Proceedings of the 2018 Conference on Empirical Methods in\\nNatural Language Processing. Association for Computational Linguistics, 2018. URL https:\\n//www.aclweb.org/anthology/D18-1007.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language un-\\nderstanding by generative pre-training, 2018. Unpublished ms. available through a link at\\nhttps://blog.openai.com/language-unsupervised/.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions\\nfor machine comprehension of text. In Proceedings of the Conference on Empirical Methods in\\nNatural Language Processing (EMNLP). Association for Computational Linguistics, 2016. doi:\\n10.18653/v1/D16-1264. URL http://aclweb.org/anthology/D16-1264.\\nMelissa Roemmele, Cosmin Adrian Bejan, and Andrew S. Gordon. Choice of plausible alternatives:\\nAn evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series, 2011.\\nRachel Rudinger, Chandler May, and Benjamin Van Durme. Social bias in elicited natural language\\ninferences. In Proceedings of the First ACL Workshop on Ethics in Natural Language Processing.\\nAssociation for Computational Linguistics, 2017. doi: 10.18653/v1/W17-1609. URL https:\\n//www.aclweb.org/anthology/W17-1609.\\nRachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias in\\ncoreference resolution. In Proceedings of the 2018 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Technologies. Association\\nfor Computational Linguistics, 2018. doi: 10.18653/v1/N18-2002. URL https://www.aclweb.\\norg/anthology/N18-2002.\\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense\\nreasoning about social interactions. arXiv preprint 1904.09728, 2019. URL https://arxiv.\\norg/abs/1904.09728.\\n14'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 15,\n",
       "  'text': 'Nathan Schneider and Noah A Smith. A corpus and model integrating multiword expressions and\\nsupersenses. In Proceedings of the Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies (NAACL-HLT). Association for\\nComputational Linguistics, 2015. URL https://www.aclweb.org/anthology/N15-1177.\\nKarin Kipper Schuler. Verbnet: A Broad-coverage, Comprehensive Verb Lexicon. PhD thesis, 2005.\\nURL http://verbs.colorado.edu/~kipper/Papers/dissertation.pdf.\\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng,\\nand Christopher. Potts. Recursive deep models for semantic compositionality over a sentiment\\ntreebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing\\n(EMNLP). Association for Computational Linguistics, 2013. URL https://www.aclweb.org/\\nanthology/D13-1170.\\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim,\\nBenjamin Van Durme, Sam Bowman, Dipanjan Das, and Ellie Pavlick. What do you learn from\\ncontext? probing for sentence structure in contextualized word representations. 2019. URL\\nhttps://openreview.net/forum?id=SJzSgnRcKX.\\nHarsh Trivedi, Heeyoung Kwon, Tushar Khot, Ashish Sabharwal, and Niranjan Balasubramanian.\\nRepurposing entailment for multi-hop question answering tasks, 2019. URL https://arxiv.\\norg/abs/1904.09380.\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. In\\nInternational Conference on Learning Representations , 2019a. URL https://openreview.\\nnet/forum?id=rJ4km2R5t7.\\nAlex Wang, Ian F. Tenney, Yada Pruksachatkun, Katherin Yu, Jan Hula, Patrick Xia, Raghu Pappagari,\\nShuning Jin, R. Thomas McCoy, Roma Patel, Yinghui Huang, Jason Phang, Edouard Grave,\\nNajoung Kim, Phu Mon Htut, Thibault F’evry, Berlin Chen, Nikita Nangia, Haokun Liu, , Anhad\\nMohananey, Shikha Bordia, Ellie Pavlick, and Samuel R. Bowman. jiant 1.0: A software toolkit\\nfor research on general-purpose text understanding models. http://jiant.info/, 2019b.\\nAlex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments.\\narXiv preprint 1805.12471, 2018. URL https://arxiv.org/abs/1805.12471.\\nKellie Webster, Marta Recasens, Vera Axelrod, and Jason Baldridge. Mind the GAP: A balanced\\ncorpus of gendered ambiguous pronouns. Transactions of the Association for Computational\\nLinguistics (TACL), 2018. URL https://www.aclweb.org/anthology/Q18-1042.\\nAdina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for\\nsentence understanding through inference. In Proceedings of the Conference of the North American\\nChapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-\\nHLT). Association for Computational Linguistics, 2018. URLhttp://aclweb.org/anthology/\\nN18-1101.\\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V .\\nLe. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint\\n1906.0823, 2019.\\nFabio Massimo Zanzotto and Lorenzo Ferrone. Have you lost the thread? discovering ongoing\\nconversations in scattered dialog blocks. ACM Transactions on Interactive Intelligent Systems\\n(TiiS), 2017.\\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. SW AG: A large-scale adversarial dataset\\nfor grounded commonsense inference. 2018. URL https://www.aclweb.org/anthology/\\nD18-1009.\\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme.\\nRecord: Bridging the gap between human and machine commonsense reading comprehension.\\narXiv preprint 1810.12885, 2018.\\n15'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 16,\n",
       "  'text': 'Yuan Zhang, Jason Baldridge, and Luheng He. PAWS: Paraphrase adversaries from word scrambling.\\narXiv preprint 1904.01130, 2019. URL https://arxiv.org/abs/1904.01130.\\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Gender bias in\\ncoreference resolution: Evaluation and debiasing methods. In Proceedings of the 2018 Conference\\nof the North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies. Association for Computational Linguistics, 2018. doi: 10.18653/v1/N18-2003. URL\\nhttps://www.aclweb.org/anthology/N18-2003.\\n16'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 17,\n",
       "  'text': 'Table 4: Baseline performance on the SuperGLUE development.\\nModel Avg BoolQ CB COPA MultiRC ReCoRD RTE WiC WSC\\nMetrics Acc. Acc./F1 Acc. F1 a/EM F1/EM Acc. Acc. Acc.\\nMost Frequent Class 47.7 62.2 50 /22.2 55 59.9/ 0.8 32.4/31.5 52.7 50.0 63.5\\nCBOW 47.7 62.4 71.4/49.6 63.0 20.3/ 0.3 14.4/13.8 54.2 55.3 61.5\\nBERT 72.2 77.7 94.6/93.7 69.0 70.5/24.7 70.6/69.8 75.8 74.9 68.3\\nBERT++ 74.6 80.1 96.4/95.0 78.0 70.5/24.7 70.6/69.8 82.3 74.9 68.3\\nA Development Set Results\\nIn Table 4, we present results of the baselines on the SuperGLUE tasks development sets.\\nB Performance on GLUE Diagnostics\\nFigure 2 shows the performance on the GLUE diagnostics dataset for systems submitted to the public\\nleaderboard.\\nDisjunction Downward Monotone Restrictivity Double Negation Prepositional Phrases\\n60\\n40\\n20\\n0\\n20\\n40\\n60\\n80\\nChance\\nBiLSTM+ELMo+Attn\\nOpenAI GPT\\nBERT + Single-task Adapters\\nBERT (Large)\\nBERT on STILTs\\nBERT + BAM\\nSemBERT\\nSnorkel MeTaL\\nALICE (Large)\\nMT-DNN (ensemble)\\nXLNet-Large (ensemble)\\nFigure 2: Performance of GLUE submissions on selected diagnostic categories, reported using the\\nR3 metric scaled up by 100, as in Wang et al. (2019a, see paper for a description of the categories).\\nSome initially difﬁcult categories, like double negation, saw gains from advances on GLUE, but\\nothers remain hard (restrictivity) or even adversarial (disjunction, downward monotone).\\nC Instructions to Crowd Workers\\nC.1 Training Phase Instructions\\nFor collecting data to establish human performance on the SuperGLUE tasks, we follow a two\\nstep procedure where we ﬁrst provide some training to the crowd workers before they proceed to\\nannotation. In the training step, we provide workers with brief instructions about the training phase.\\nAn example of these instructions is given Table 5. These training instructions are the same across\\ntasks, only the task name in the instructions is changed.\\n17'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 18,\n",
       "  'text': 'C.2 Task Instructions\\nDuring training and annotation for each task, we provide workers with brief instructions tailored to\\nthe task. We also link workers to an FAQ page for the task. Tables 6, 7, 8, and 9, show the instructions\\nwe used for all four tasks: COPA, CommitmentBank, WSC, and BoolQ respectively. The instructions\\ngiven to crowd workers for annotations on the diagnostic and bias diagnostic datasets are shown in\\nTable 11.\\nWe collected data to produce conservative estimates for human performance on several tasks that\\nwe did not ultimately include in our benchmark, including GAP (Webster et al., 2018), PAWS\\n(Zhang et al., 2019), Quora Insincere Questions,11 Ultraﬁne Entity Typing (Choi et al., 2018b), and\\nEmpathetic Reactions datasets (Buechel et al., 2018). The instructions we used for these tasks are\\nshown in Tables 12, 13, 14, 15, and 16.\\nC.3 Task Speciﬁc Details\\nFor WSC and COPA we provide annotators with a two way classiﬁcation problem. We then use\\nmajority vote across annotations to calculate human performance.\\nCommitmentBank We follow the authors in providing annotators with a 7-way classiﬁcation\\nproblem. We then collapse the annotations into 3 classes by using the same ranges for bucketing used\\nby De Marneffe et al. (2019). We then use majority vote to get human performance numbers on the\\ntask.\\nFurthermore, for training on CommitmentBank we randomly sample examples from the low inter-\\nannotator agreement portion of the CommitmentBank data that is not included in the benchmark\\nversion of the task. These low agreement examples are generally harder to classify since they are\\nmore ambiguous.\\nDiagnostic Dataset Since the diagnostic dataset does not come with accompanying training data,\\nwe train our workers on examples from RTE’s development set. RTE is also a textual entailment\\ntask and is the most closely related task in the main benchmark. Providing the crowd workers with\\ntraining on RTE enables them to learn label deﬁnitions which should generalize to the diagnostic\\ndataset.\\nUltraﬁne Entity Typing We cast the task into a binary classiﬁcation problem to make it an easier\\ntask for non-expert crowd workers. We work in cooperation with the authors of the dataset (Choi\\net al., 2018b) to do this reformulation: We give workers one possible tag for a word or phrase and\\nasked them to classify the tag as being applicable or not.\\nThe authors used WordNet (Miller, 1995) to expand the set of labels to include synonyms and\\nhypernyms from WordNet. They then asked ﬁve annotators to validate these tags. The tags from this\\nvalidation had high agreement, and were included in the publicly available Ultraﬁne Entity Typing\\ndataset,12 This constitutes our set of positive examples. The rest of the tags from the validation\\nprocedure that are not in the public dataset constitute our negative examples.\\nGAP For the Gendered Ambiguous Pronoun Coreference task (GAP, Webster et al., 2018), we\\nsimpliﬁed the task by providing noun phrase spans as part of the input, thus reducing the original\\nstructure prediction task to a classiﬁcation task. This task was presented to crowd workers as a three\\nway classiﬁcation problem: Choose span A, B, or neither.\\nD Excluded Tasks\\nIn this section we provide some examples of tasks that we evaluated for inclusion but ultimately could\\nnot include. We report on these excluded tasks only with the permission of their authors. We turned\\ndown many medical text datasets because they are usually only accessible with explicit permission\\nand credentials from the data owners.\\n11https://www.kaggle.com/c/quora-insincere-questions-classification/data\\n12https://homes.cs.washington.edu/~eunsol/open_entity.html\\n18'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 19,\n",
       "  'text': 'Tasks like QuAC (Choi et al., 2018a) and STREUSLE (Schneider and Smith, 2015) differed substan-\\ntially from the format of other tasks in our benchmark, which we worried would incentivize users\\nto spend signiﬁcant effort on task-speciﬁc model designs, rather than focusing on general-purpose\\ntechniques. It was challenging to train annotators to do well on Quora Insincere Questions 13, Empa-\\nthetic Reactions (Buechel et al., 2018), and a recast version of Ultra-Fine Entity Typing (Choi et al.,\\n2018b, see Appendix C.3 for details), leading to low human performance. BERT achieved very high\\nor superhuman performance on Query Well-Formedness (Faruqui and Das, 2018), PAWS (Zhang\\net al., 2019), Discovering Ongoing Conversations (Zanzotto and Ferrone, 2017), and GAP (Webster\\net al., 2018).\\nDuring the process of selecting tasks for our benchmark, we collected human performance baselines\\nand run BERT-based machine baselines for some tasks that we ultimately excluded from our task\\nlist. We chose to exclude these tasks because our BERT baseline performs better than our human\\nperformance baseline or if the gap between human and machine performance is small.\\nOn Quora Insincere Questions our BERT baseline outperforms our human baseline by a small margin:\\nan F1 score of 67.2 versus 66.7 for BERT and human baselines respectively. Similarly, on the\\nEmpathetic Reactions dataset, BERT outperforms our human baseline, where BERT’s predictions\\nhave a Pearson correlation of 0.45 on empathy and 0.55 on distress, compared to 0.45 and 0.35 for\\nour human baseline. For PAWS-Wiki, we report that BERT achieves an accuracy of 91.9%, while our\\nhuman baseline achieved 84% accuracy. These three tasks are excluded from the benchmark since\\nour, admittedly conservative, human baselines are worse than machine performance. Our human\\nperformance baselines are subject to the clarity of our instructions (all instructions can be found in\\nAppendix C), and crowd workers engagement and ability.\\nFor the Query Well-Formedness task, the authors set an estimate human performance at 88.4%\\naccuracy. Our BERT baseline model reaches an accuracy of 82.3%. While there is a positive gap on\\nthis task, the gap was smaller than we were were willing to tolerate. Similarly, on our recast version\\nof the Ultraﬁne Entity Typing, we observe too small a gap between human (60.2 F1) and machine\\nperformance (55.0 F1). Our recasting for this task is described in Appendix C.2. On GAP, when\\ntaken as a classiﬁcation problem without the related task of span selection (details in C.2), BERT\\nperforms (91.0 F1) comparably to our human baseline (94.9 F1). Given this small margin, we also\\nexclude GAP.\\nOn Discovering Ongoing Conversations, our BERT baseline achieves an F1 of 51.9 on a version of\\nthe task cast as sentence pair classiﬁcation (given two snippets of texts from plays, determine if the\\nsecond snippet is a continuation of the ﬁrst). This dataset is very class imbalanced (90% negative), so\\nwe also experimented with a class-balanced version on which our BERT baselines achieves 88.4\\nF1. Qualitatively, we also found the task challenging for humans as there was little context for the\\ntext snippets and the examples were drawn from plays using early English. Given this fairly high\\nmachine performance and challenging nature for humans, we exclude this task from our benchmark.\\nInstructions tables begin on the following page.\\n13https://www.kaggle.com/c/quora-insincere-questions-classification/data\\n19'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 20,\n",
       "  'text': 'Table 5: The instructions given to crowd-sourced worker describing the training phase for the Choice\\nof Plausible Answers (COPA) task.\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nThis project is a training task that needs to be completed before working on the main project\\non AMT named Human Performance: Plausible Answer. Once you are done with the training,\\nplease proceed to the main task! The qualiﬁcation approval is not immediate but we will add\\nyou to our qualiﬁed workers list within a day.\\nIn this training, you must answer the question on the page and then, to see how you did, click\\nthe Check Work button at the bottom of the page before hitting Submit. The Check Work\\nbutton will reveal the true label. Please use this training and the provided answers to build\\nan understanding of what the answers to these questions look like (the main project, Human\\nPerformance: Plausible Answer, does not have the answers on the page).\\nTable 6: Task-speciﬁc instructions for Choice of Plausible Alternatives (COPA). These instructions\\nwere provided during both training and annotation phases.\\nPlausible Answer Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will present you with a prompt sentence and a question. The question will either be about\\nwhat caused the situation described in the prompt, or what a possible effect of that situation is.\\nWe will also give you two possible answers to this question. Your job is to decide, given the\\nsituation described in the prompt, which of the two options is a more plausible answer to the\\nquestion:\\nIn the following example, option 1. is a more plausible answer to the question about what caused\\nthe situation described in the prompt,\\nThe girl received a trophy.\\nWhat’s the CAUSE for this?\\n1. She won a spelling bee.\\n2. She made a new friend.\\nIn the following example, option2. is a more plausible answer the question about what happened\\nbecause of the situation described in the prompt,\\nThe police aimed their weapons at the fugitive.\\nWhat happened as a RESULT?\\n1. The fugitive fell to the ground.\\n2. The fugitive dropped his gun.\\nIf you have any more questions, please refer to our FAQpage.\\n20'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 21,\n",
       "  'text': 'Table 7: Task-speciﬁc instructions for Commitment Bank. These instructions were provided during\\nboth training and annotation phases.\\nSpeaker Commitment Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will present you with a prompt taken from a piece of dialogue, this could be a single sentence,\\na few sentences, or a short exchange between people. Your job is to ﬁgure out, based on this\\nﬁrst prompt (on top), how certain the speaker is about the truthfulness of the second prompt\\n(on the bottom). You can choose from a 7 point scale ranging from (1) completely certain that\\nthe second prompt is true to (7) completely certain that the second prompt is false. Here are\\nexamples for a few of the labels:\\nChoose 1 (certain that it is true) if the speaker from the ﬁrst prompt deﬁnitely believes or knows\\nthat the second prompt is true. For example,\\n\"What fun to hear Artemis laugh. She’s such a serious child. I didn’t know\\nshe had a sense of humor.\"\\n\"Artemis had a sense of humor\"\\nChoose 4 (not certain if it is true or false) if the speaker from the ﬁrst prompt is uncertain if the\\nsecond prompt is true or false. For example,\\n\"Tess is committed to track. She’s always trained with all her heart and soul.\\nOne can only hope that she has recovered from the ﬂu and will cross the ﬁnish\\nline.\"\\n\"Tess crossed the ﬁnish line.\"\\nChoose 7 (certain that it is false) if the speaker from the ﬁrst prompt deﬁnitely believes or knows\\nthat the second prompt is false. For example,\\n\"Did you hear about Olivia’s chemistry test? She studied really hard. But\\neven after putting in all that time and energy, she didn’t manage to pass the\\ntest\".\\n\"Olivia passed the test.\"\\nIf you have any more questions, please refer to our FAQpage.\\n21'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 22,\n",
       "  'text': 'Table 8: Task-speciﬁc instructions for Winograd Schema Challenge (WSC). These instructions were\\nprovided during both training and annotation phases.\\nWinograd Schema Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will present you with a sentence that someone wrote, with one bolded pronoun. We will then\\nask if you if the pronoun refers to a speciﬁc word or phrase in the sentence. Your job is to ﬁgure\\nout, based on the sentence, if the bolded pronoun refers to this selected word or phrase:\\nChoose Yes if the pronoun refers to the selected word or phrase. For example,\\n\"I put the cake away in the refrigerator. It has a lot of butter in it.\"\\nDoes It in \"It has a lot\" refer to cake?\\nChoose No if the pronoun does not refer to the selected word or phrase. For example,\\n\"The large ball crashed right through the table because it was made of\\nstyrofoam.\"\\nDoes it in \"it was made\" refer to ball?\\nIf you have any more questions, please refer to our FAQpage.\\n22'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 23,\n",
       "  'text': 'Table 9: Task-speciﬁc instructions for BoolQ (continued in Table 10). These instructions were\\nprovided during both training and annotation phases.\\nQuestion-Answering Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will present you with a passage taken from a Wikipedia article and a relevant question. Your\\njob is to decide, given the information provided in the passage, if the answer to the question is\\nYes or No. For example,\\nIn the following examples the correct answer is Yes,\\nThe thirteenth season of Criminal Minds was ordered on April 7, 2017, by\\nCBS with an order of 22 episodes. The season premiered on September 27,\\n2017 in a new time slot at 10:00PM on Wednesday when it had previously\\nbeen at 9:00PM on Wednesday since its inception. The season concluded on\\nApril 18, 2018 with a two-part season ﬁnale.\\nwill there be a 13th season of criminal minds?\\n(In the above example, the ﬁrst line of the passage says that the 13th season of\\nthe show was ordered.)\\nAs of 8 August 2016, the FDA extended its regulatory power to include e-\\ncigarettes. Under this ruling the FDA will evaluate certain issues, including\\ningredients, product features and health risks, as well their appeal to minors\\nand non-users. The FDA rule also bans access to minors. A photo ID is\\nrequired to buy e-cigarettes, and their sale in all-ages vending machines is not\\npermitted. The FDA in September 2016 has sent warning letters for unlawful\\nunderage sales to online retailers and retailers of e-cigarettes.\\nis vaping illegal if you are under 18?\\n(In the above example, the passage states that the \"FDA rule also bans access\\nto minors.\" The question uses the word \"vaping,\" which is a synonym for\\ne-cigrattes.)\\nIn the following examples the correct answer is No,\\nBadgers are short-legged omnivores in the family Mustelidae, which also\\nincludes the otters, polecats, weasels, and wolverines. They belong to the\\ncaniform suborder of carnivoran mammals. The 11 species of badgers are\\ngrouped in three subfamilies: Melinae (Eurasian badgers), Mellivorinae (the\\nhoney badger or ratel), and Taxideinae (the American badger). The Asiatic\\nstink badgers of the genus Mydaus were formerly included within Melinae\\n(and thus Mustelidae), but recent genetic evidence indicates these are actually\\nmembers of the skunk family, placing them in the taxonomic family Mephitidae.\\nis a wolverine the same as a badger?\\n(In the above example, the passage says that badgers and wolverines are in\\nthe same family, Mustelidae, which does not mean they are the same animal.)\\n23'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 24,\n",
       "  'text': 'Table 10: Continuation from Table 9 of task-speciﬁc instructions for BoolQ. These instructions were\\nprovided during both training and annotation phases.\\nMore famously, Harley-Davidson attempted to register as a trademark the\\ndistinctive “chug” of a Harley-Davidson motorcycle engine. On February\\n1, 1994, the company ﬁled its application with the following description:\\n“The mark consists of the exhaust sound of applicant’s motorcycles, produced\\nby V-twin, common crankpin motorcycle engines when the goods are in use. ”\\nNine of Harley-Davidson’s competitors ﬁled oppositions against the applica-\\ntion, arguing that cruiser-style motorcycles of various brands use the same\\ncrankpin V-twin engine which produces the same sound. After six years of\\nlitigation, with no end in sight, in early 2000, Harley-Davidson withdrew their\\napplication.\\ndoes harley davidson have a patent on their sound?\\n(In the above example, the passage states that Harley-Davidson applied for a\\npatent but then withdrew, so they do not have a patent on the sound.)\\nIf you have any more questions, please refer to our FAQpage.\\n24'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 25,\n",
       "  'text': 'Table 11: Task-speciﬁc instructions for the diagnostic and the bias diagnostic datasets. These\\ninstructions were provided during both training and annotation phases.\\nTextual Entailment Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will present you with a prompt taken from an article someone wrote. Your job is to ﬁgure out,\\nbased on this correct prompt (the ﬁrst prompt, on top), if another prompt (the second prompt, on\\nbottom) is also necessarily true:\\nChoose True if the event or situation described by the ﬁrst prompt deﬁnitely implies that the\\nsecond prompt, on bottom, must also be true. For example,\\n• \"Murphy recently decided to move to London.\"\\n\"Murphy recently decided to move to England.\"\\n(The above example is True because London is in England and therefore prompt 2 is\\nclearly implied by prompt 1.)\\n• \"Russian cosmonaut Valery Polyakov set the record for the longest continuous amount\\nof time spent in space, a staggering 438 days, between 1994 and 1995.\"\\n\"Russians hold record for longest stay in space.\"\\n(The above example is True because the information in the second prompt is contained\\nin the ﬁrst prompt: Valery is Russian and she set the record for longest stay in space.)\\n• \"She does not disgree with her brother’s opinion, but she believes he’s too aggresive in\\nhis defense\"\\n\"She agrees with her brother’s opinion, but she believes he’s too aggresive in his\\ndefense\"\\n(The above example is True because the second prompt is an exact paraphrase of the\\nﬁrst prompt, with exactly the same meaning.)\\nChoose False if the event or situation described with the ﬁrst prompt on top does not necessarily\\nimply that this second prompt must also be true. For example,\\n• \"This method was developed at Columbia and applied to data processing at CERN.\"\\n\"This method was developed at Columbia and applied to data processing at CERN\\nwith limited success.\"\\n(The above example is False because the second prompt is introducing new information\\nnot implied in the ﬁrst prompt: The ﬁrst prompt does not give us any knowledge of\\nhow succesful the application of the method at CERN was.)\\n• \"This building is very tall.\"\\n\"This is the tallest building in New York.\"\\n(The above example is False because a building being tall does not mean it must be the\\ntallest building, nor that it is in New York.)\\n• \"Hours earlier, Yasser Arafat called for an end to attacks against Israeli civilians in\\nthe two weeks before Israeli elections.\"\\n\"Arafat condemned suicide bomb attacks inside Israel.\"\\n(The above example is False because from the ﬁrst prompt we only know that Arafat\\ncalled for an end to attacks against Israeli citizens, we do not know what kind of attacks\\nhe may have been condemning.)\\nYou do not have to worry about whether the writing style is maintained between the two prompts.\\nIf you have any more questions, please refer to our FAQpage.\\n25'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 26,\n",
       "  'text': 'Table 12: Task-speciﬁc instructions for the Gendered Ambiguous Pronoun Coreference (GAP) task.\\nThese instructions were provided during both training and annotation phases.\\nGAP Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will present you with an extract from a Wikipedia article, with one bolded pronoun. We will\\nalso give you two names from the text that this pronoun could refer to. Your job is to ﬁgure out,\\nbased on the extract, if the pronoun refers to option A, options B, or neither:\\nChoose A if the pronoun refers to option A. For example,\\n\"In 2010 Ella Kabambe was not the ofﬁcial Miss Malawi; this was Faith\\nChibale, but Kabambe represented the country in the Miss World pageant.\\nAt the 2012 Miss World, Susan Mtegha pushed Miss New Zealand, Collette\\nLochore, during the opening headshot of the pageant, claiming that Miss New\\nZealand was in her space.\"\\nDoes her refer to option A or B below?\\nA Susan Mtegha\\nB Collette Lochore\\nC Neither\\nChoose B if the pronoun refers to option B. For example,\\n\"In 1650 he started his career as advisor in the ministerium of ﬁnances in Den\\nHaag. After he became a minister he went back to Amsterdam, and took place\\nas a sort of chairing mayor of this city. After the death of his brother Cornelis,\\nDe Graeff became the strong leader of the republicans. He held this position\\nuntil the rampjaar.\"\\nDoes He refer to option A or B below?\\nA Cornelis\\nB De Graeff\\nC Neither\\nChoose C if the pronoun refers to neither option. For example,\\n\"Reb Chaim Yaakov’s wife is the sister of Rabbi Moishe Sternbuch, as is\\nthe wife of Rabbi Meshulam Dovid Soloveitchik, making the two Rabbis his\\nuncles. Reb Asher’s brother Rabbi Shlomo Arieli is the author of a critical\\nedition of the novallae of Rabbi Akiva Eiger. Before his marriage, Rabbi Arieli\\nstudied in the Ponevezh Yeshiva headed by Rabbi Shmuel Rozovsky, and he\\nlater studied under his father-in-law in the Mirrer Yeshiva.\"\\nDoes his refer to option A or B below?\\nA Reb Asher\\nB Akiva Eiger\\nC Neither\\nIf you have any more questions, please refer to our FAQpage.\\n26'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 27,\n",
       "  'text': 'Table 13: Task-speciﬁc instructions for the Paraphrase Adversaries from Word Scrambling (PAWS)\\ntask. These instructions were provided during both training and annotation phases.\\nParaphrase Detection Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will present you with two similar sentences taken from Wikipedia articles. Your job is to\\nﬁgure out if these two sentences are paraphrases of each other, and convey exactly the same\\nmeaning:\\nChoose Yes if the sentences are paraphrases and have the exact same meaning. For example,\\n\"Hastings Ndlovu was buried with Hector Pieterson at Avalon Cemetery in\\nJohannesburg.\"\\n\"Hastings Ndlovu , together with Hector Pieterson , was buried at the Avalon\\ncemetery in Johannesburg .\"\\n\"The complex of the Trabzon World Trade Center is close to Trabzon Airport\\n.\"\\n\"The complex of World Trade Center Trabzon is situated close to Trabzon\\nAirport .\"\\nChoose No if the two sentences are not exact paraphrases and mean different things. For\\nexample,\\n\"She was only a few months in French service when she met some British\\nfrigates in 1809 .\"\\n\"She was only in British service for a few months , when in 1809 , she\\nencountered some French frigates .\"\\n\"This work caused him to trigger important reﬂections on the practices of\\nmolecular genetics and genomics at a time when this was not considered\\nethical .\"\\n\"This work led him to trigger ethical reﬂections on the practices of molecular\\ngenetics and genomics at a time when this was not considered important .\"\\nIf you have any more questions, please refer to our FAQpage.\\n27'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 28,\n",
       "  'text': 'Table 14: Task-speciﬁc instructions for the Quora Insincere Questions task. These instructions were\\nprovided during both training and annotation phases.\\nInsincere Questions Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will present you with a question that someone posted on Quora. Your job is to ﬁgure out\\nwhether or not this is a sincere question. An insincere question is deﬁned as a question intended\\nto make a statement rather than look for helpful answers. Some characteristics that can signify\\nthat a question is insincere:\\n• Has a non-neutral tone\\n– Has an exaggerated tone to underscore a point about a group of people\\n– Is rhetorical and meant to imply a statement about a group of people\\n• Is disparaging or inﬂammatory\\n– Suggests a discriminatory idea against a protected class of people, or seeks\\nconﬁrmation of a stereotype\\n– Makes disparaging attacks/insults against a speciﬁc person or group of people\\n– Based on an outlandish premise about a group of people\\n– Disparages against a characteristic that is not ﬁxable and not measurable\\n• Isn’t grounded in reality\\n– Based on false information, or contains absurd assumptions\\n– Uses sexual content (incest, bestiality, pedophilia) for shock value, and not to seek\\ngenuine answers\\nPlease note that there are far fewer insincere questions than there are sincere questions! So you\\nshould expect to label most questions as sincere.\\nExamples,\\nChoose Sincere if you believe the person asking the question was genuinely seeking an answer\\nfrom the forum. For example,\\n\"How do DNA and RNA compare and contrast?\"\\n\"Are there any sports that you don’t like?\"\\n\"What is the main purpose of penance?\"\\nChoose Insincere if you believe the person asking the question was not really seeking an answer\\nbut was being inﬂammatory, extremely rhetorical, or absurd. For example,\\n\"How do I sell Pakistan? I need lots of money so I decided to sell Pakistan\\nany one wanna buy?\"\\n\"If Hispanics are so proud of their countries, why do they move out?\"\\n\"Why Chinese people are always not welcome in all countries?\"\\nIf you have any more questions, please refer to our FAQpage.\\n28'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 29,\n",
       "  'text': 'Table 15: Task-speciﬁc instructions for the Ultraﬁne Entity Typing task. These instructions were\\nprovided during both training and annotation phases.\\nEntity Typing Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will provide you with a sentence with on bolded word or phrase. We will also give you a\\npossible tag for this bolded word or phrase. Your job is to decide, in the context of the sentence,\\nif this tag is correct and applicable to the bolded word or phrase:\\nChoose Yes if the tag is applicable and accurately describes the selected word or phrase. For\\nexample,\\n“Spain was the gold line.\" It started out with zero gold in 1937, and by 1945\\nit had 65.5 tons.\\nTag: nation\\nChoose No if the tag is not applicable and does not describes the selected word or phrase. For\\nexample,\\nIraqi museum workersare starting to assess the damage to Iraq’s history.\\nTag: organism\\nIf you have any more questions, please refer to our FAQpage.\\n29'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 30,\n",
       "  'text': 'Table 16: Task-speciﬁc instructions for the Empathetic Reaction task. These instructions were\\nprovided during both training and annotation phases.\\nEmpathy and Distress Analysis Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will present you with a message someone wrote after reading an article. Your job is to ﬁgure\\nout, based on this message, how disressed and empathetic the author was feeling. Empathy is\\ndeﬁned as feeling warm, tender, sympathetic, moved, or compassionate. Distressed is deﬁned as\\nfeeling worried, upset, troubled, perturbed, grieved, distrubed, or alarmed.\\nExamples,\\nThe author of the following message was not feeling empathetic at all with anempathy score of 1,\\nand was very distressed with a distress score of 7,\\n\"I really hate ISIS. They continue to be the stain on society by committing\\natrocities condemned by every nation in the world. They must be stopped at\\nall costs and they must be destroyed so that they wont hurt another soul. These\\npoor people who are trying to survive get killed, imprisoned, or brainwashed\\ninto joining and there seems to be no way to stop them.\"\\nThe author of the following message is feeling very empathetic with an empathy score of 7 and\\nalso very distressed with a distress score of 7,\\n\"All of you know that I love birds. This article was hard for me to read because\\nof that. Wind turbines are killing a lot of birds, including eagles. It’s really\\nvery sad. It makes me feel awful. I am all for wind turbines and renewable\\nsources of energy because of global warming and coal, but this is awful. I\\ndon’t want these poor birds to die like this. Read this article and you’ll see\\nwhy.\"\\nThe author of the following message is feeling moderately empathetic with an\\nempathy score of 4 and moderately distressed with a distress score of 4,\\n\"I just read an article about wild ﬁres sending a smokey haze across the state\\nnear the Appalachian mountains. Can you imagine how big the ﬁre must be\\nto spread so far and wide? And the people in the area obviously suffer the\\nmost. What if you have asthma or some other condition that restricts your\\nbreathing?\"\\nThe author of the following message is feeling very empathetic with an empathy score of 7 and\\nmildly distressed with a distress score of 2,\\n\"This is a very sad article. Being of of the ﬁrst female ﬁghter pilots must\\nhave given her and her family great honor. I think that there should be more\\ntraining for all pilots who deal in these acrobatic ﬂying routines. I also think\\nthat women have just as much of a right to become a ﬁghter pilot as men.\"\\nIf you have any more questions, please refer to our FAQpage.\\n30'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lec_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa07b8a4-1348-42f8-be78-bb5a6cfa8076",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SuperGLUE: A Stickier Benchmark for\n",
      "General-Purpose Language Understanding Systems\n",
      "Alex Wang∗\n",
      "New York University\n",
      "Yada Pruksachatkun∗\n",
      "New York University\n",
      "Nikita Nangia∗\n",
      "New York University\n",
      "Amanpreet Singh∗\n",
      "Facebook AI Research\n",
      "Julian Michael\n",
      "University of Washington\n",
      "Felix Hill\n",
      "DeepMind\n",
      "Omer Levy\n",
      "Facebook AI Research\n",
      "Samuel R. Bowman\n",
      "New York University\n",
      "Abstract\n",
      "In the last year, new models and methods for pretraining and transfer learning have\n",
      "driven striking performance improvements across a range of language understand-\n",
      "ing tasks. The GLUE benchmark, introduced a little over one year ago, offers\n",
      "a single-number metric that summarizes progress on a diverse set of such tasks,\n",
      "but performance on the benchmark has recently surpassed the level of non-expert\n",
      "humans, suggesting limited headroom for further research. In this paper we present\n",
      "SuperGLUE, a new benchmark styled after GLUE with a new set of more difﬁ-\n",
      "cult language understanding tasks, a software toolkit, and a public leaderboard.\n",
      "SuperGLUE is available at super.gluebenchmark.com.\n",
      "1 Introduction\n",
      "In the past year, there has been notable progress across many natural language processing (NLP)\n",
      "tasks, led by methods such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018),\n",
      "and BERT (Devlin et al., 2019). The common thread connecting these methods is that they couple\n",
      "self-supervised learning from massive unlabelled text corpora with a recipe for effectively adapting\n",
      "the resulting model to target tasks. The tasks that have proven amenable to this general approach\n",
      "include question answering, sentiment analysis, textual entailment, and parsing, among many others\n",
      "(Devlin et al., 2019; Kitaev and Klein, 2018, i.a.).\n",
      "In this context, the GLUE benchmark (Wang et al., 2019a) has become a prominent evaluation\n",
      "framework for research towards general-purpose language understanding technologies. GLUE is\n",
      "a collection of nine language understanding tasks built on existing public datasets, together with\n",
      "private test data, an evaluation server, a single-number target metric, and an accompanying expert-\n",
      "constructed diagnostic set. GLUE was designed to provide a general-purpose evaluation of language\n",
      "understanding that covers a range of training data volumes, task genres, and task formulations. We\n",
      "believe it was these aspects that made GLUE particularly appropriate for exhibiting the transfer-\n",
      "learning potential of approaches like OpenAI GPT and BERT.\n",
      "The progress of the last twelve months has eroded headroom on the GLUE benchmark dramatically.\n",
      "While some tasks (Figure 1) and some linguistic phenomena (Figure 2 in Appendix B) measured\n",
      "in GLUE remain difﬁcult, the current state of the art GLUE Score as of early July 2019 (88.4 from\n",
      "Yang et al., 2019) surpasses human performance (87.1 from Nangia and Bowman, 2019) by 1.3\n",
      "∗Equal contribution. Correspondence: glue-benchmark-admin@googlegroups.com\n",
      "33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\n",
      "BiLSTM+ELMo+Attn\n",
      "OpenAI GPT\n",
      "BERT + Single-task Adapters\n",
      "BERT (Large)\n",
      "BERT on STILTs\n",
      "BERT + BAM\n",
      "SemBERT\n",
      "Snorkel MeTaL\n",
      "ALICE (Large)\n",
      "MT-DNN (ensemble)\n",
      "XLNet-Large (ensemble)0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "1.1\n",
      "1.2\n",
      "GLUE Score\n",
      "Human Performance\n",
      "CoLA\n",
      "SST-2\n",
      "MRPC\n",
      "STS-B\n",
      "QQP\n",
      "MNLI\n",
      "QNLI\n",
      "RTE\n",
      "WNLI\n",
      "Figure 1: GLUE benchmark performance for submitted systems, rescaled to set human performance\n",
      "to 1.0, shown as a single number score, and broken down into the nine constituent task performances.\n",
      "For tasks with multiple metrics, we use an average of the metrics. More information on the tasks\n",
      "included in GLUE can be found in Wang et al. (2019a) and in Warstadt et al. (2018, CoLA), Socher\n",
      "et al. (2013, SST-2), Dolan and Brockett (2005, MRPC), Cer et al. (2017, STS-B), and Williams et al.\n",
      "(2018, MNLI), and Rajpurkar et al. (2016, the original data source for QNLI).\n",
      "points, and in fact exceeds this human performance estimate on four tasks. Consequently, while there\n",
      "remains substantial scope for improvement towards GLUE’s high-level goals, the original version of\n",
      "the benchmark is no longer a suitable metric for quantifying such progress.\n",
      "In response, we introduce SuperGLUE, a new benchmark designed to pose a more rigorous test of\n",
      "language understanding. SuperGLUE has the same high-level motivation as GLUE: to provide a\n",
      "simple, hard-to-game measure of progress toward general-purpose language understanding technolo-\n",
      "gies for English. We anticipate that signiﬁcant progress on SuperGLUE should require substantive\n",
      "innovations in a number of core areas of machine learning, including sample-efﬁcient, transfer,\n",
      "multitask, and unsupervised or self-supervised learning.\n",
      "SuperGLUE follows the basic design of GLUE: It consists of a public leaderboard built around\n",
      "eight language understanding tasks, drawing on existing data, accompanied by a single-number\n",
      "performance metric, and an analysis toolkit. However, it improves upon GLUE in several ways:\n",
      "• More challenging tasks: SuperGLUE retains the two hardest tasks in GLUE. The remain-\n",
      "ing tasks were identiﬁed from those submitted to an open call for task proposals and were\n",
      "selected based on difﬁculty for current NLP approaches.\n",
      "• More diverse task formats: The task formats in GLUE are limited to sentence- and\n",
      "sentence-pair classiﬁcation. We expand the set of task formats in SuperGLUE to include\n",
      "coreference resolution and question answering (QA).\n",
      "• Comprehensive human baselines: We include human performance estimates for all bench-\n",
      "mark tasks, which verify that substantial headroom exists between a strong BERT-based\n",
      "baseline and human performance.\n",
      "• Improved code support: SuperGLUE is distributed with a new, modular toolkit for work\n",
      "on pretraining, multi-task learning, and transfer learning in NLP, built around standard tools\n",
      "including PyTorch (Paszke et al., 2017) and AllenNLP (Gardner et al., 2017).\n",
      "• Reﬁned usage rules: The conditions for inclusion on the SuperGLUE leaderboard have\n",
      "been revamped to ensure fair competition, an informative leaderboard, and full credit\n",
      "assignment to data and task creators.\n",
      "The SuperGLUE leaderboard, data, and software tools are available at super.gluebenchmark.com.\n",
      "2\n",
      "2 Related Work\n",
      "Much work prior to GLUE demonstrated that training neural models with large amounts of available\n",
      "supervision can produce representations that effectively transfer to a broad range of NLP tasks\n",
      "(Collobert and Weston, 2008; Dai and Le, 2015; Kiros et al., 2015; Hill et al., 2016; Conneau and\n",
      "Kiela, 2018; McCann et al., 2017; Peters et al., 2018). GLUE was presented as a formal challenge\n",
      "affording straightforward comparison between such task-agnostic transfer learning techniques. Other\n",
      "similarly-motivated benchmarks include SentEval (Conneau and Kiela, 2018), which speciﬁcally\n",
      "evaluates ﬁxed-size sentence embeddings, and DecaNLP (McCann et al., 2018), which recasts a set\n",
      "of target tasks into a general question-answering format and prohibits task-speciﬁc parameters. In\n",
      "contrast, GLUE provides a lightweight classiﬁcation API and no restrictions on model architecture or\n",
      "parameter sharing, which seems to have been well-suited to recent work in this area.\n",
      "Since its release, GLUE has been used as a testbed and showcase by the developers of several\n",
      "inﬂuential models, including GPT (Radford et al., 2018) and BERT (Devlin et al., 2019). As shown\n",
      "in Figure 1, progress on GLUE since its release has been striking. On GLUE, GPT and BERT\n",
      "achieved scores of 72.8 and 80.2 respectively, relative to 66.5 for an ELMo-based model (Peters\n",
      "et al., 2018) and 63.7 for the strongest baseline with no multitask learning or pretraining above the\n",
      "word level. Recent models (Liu et al., 2019d; Yang et al., 2019) have clearly surpassed estimates of\n",
      "non-expert human performance on GLUE (Nangia and Bowman, 2019). The success of these models\n",
      "on GLUE has been driven by ever-increasing model capacity, compute power, and data quantity, as\n",
      "well as innovations in model expressivity (from recurrent to bidirectional recurrent to multi-headed\n",
      "transformer encoders) and degree of contextualization (from learning representation of words in\n",
      "isolation to using uni-directional contexts and ultimately to leveraging bidirectional contexts).\n",
      "In parallel to work scaling up pretrained models, several studies have focused on complementary\n",
      "methods for augmenting performance of pretrained models. Phang et al. (2018) show that BERT can\n",
      "be improved using two-stage pretraining, i.e., ﬁne-tuning the pretrained model on an intermediate\n",
      "data-rich supervised task before ﬁne-tuning it again on a data-poor target task. Liu et al. (2019d,c) and\n",
      "Bach et al. (2018) get further improvements respectively via multi-task ﬁnetuning and using massive\n",
      "amounts of weak supervision. Anonymous (2018) demonstrate that knowledge distillation (Hinton\n",
      "et al., 2015; Furlanello et al., 2018) can lead to student networks that outperform their teachers.\n",
      "Overall, the quantity and quality of research contributions aimed at the challenges posed by GLUE\n",
      "underline the utility of this style of benchmark for machine learning researchers looking to evaluate\n",
      "new application-agnostic methods on language understanding.\n",
      "Limits to current approaches are also apparent via the GLUE suite. Performance on the GLUE\n",
      "diagnostic entailment dataset, at 0.42 R3, falls far below the average human performance of 0.80\n",
      "R3 reported in the original GLUE publication, with models performing near, or even below, chance\n",
      "on some linguistic phenomena (Figure 2, Appendix B). While some initially difﬁcult categories\n",
      "saw gains from advances on GLUE (e.g., double negation), others remain hard (restrictivity) or\n",
      "even adversarial (disjunction, downward monotonicity). This suggests that even as unsupervised\n",
      "pretraining produces ever-better statistical summaries of text, it remains difﬁcult to extract many\n",
      "details crucial to semantics without the right kind of supervision. Much recent work has made similar\n",
      "observations about the limitations of existing pretrained models (Jia and Liang, 2017; Naik et al.,\n",
      "2018; McCoy and Linzen, 2019; McCoy et al., 2019; Liu et al., 2019a,b).\n",
      "3 SuperGLUE Overview\n",
      "3.1 Design Process\n",
      "The goal of SuperGLUE is to provide a simple, robust evaluation metric of any method capable of\n",
      "being applied to a broad range of language understanding tasks. To that end, in designing SuperGLUE,\n",
      "we identify the following desiderata of tasks in the benchmark:\n",
      "• Task substance: Tasks should test a system’s ability to understand and reason about texts\n",
      "written in English.\n",
      "• Task difﬁculty: Tasks should be beyond the scope of current state-of-the-art systems,\n",
      "but solvable by most college-educated English speakers. We exclude tasks that require\n",
      "domain-speciﬁc knowledge, e.g. medical notes or scientiﬁc papers.\n",
      "3\n",
      "Table 1: The tasks included in SuperGLUE. WSD stands for word sense disambiguation, NLI is\n",
      "natural language inference, coref. is coreference resolution, and QA is question answering. For\n",
      "MultiRC, we list the number of total answers for 456/83/166 train/dev/test questions. The metrics for\n",
      "MultiRC are binary F1 on all answer-options and exact match.\n",
      "Corpus |Train| | Dev| | Test| Task Metrics Text Sources\n",
      "BoolQ 9427 3270 3245 QA acc. Google queries, Wikipedia\n",
      "CB 250 57 250 NLI acc./F1 various\n",
      "COPA 400 100 500 QA acc. blogs, photography encyclopedia\n",
      "MultiRC 5100 953 1800 QA F1 a/EM various\n",
      "ReCoRD 101k 10k 10k QA F1/EM news (CNN, Daily Mail)\n",
      "RTE 2500 278 300 NLI acc. news, Wikipedia\n",
      "WiC 6000 638 1400 WSD acc. WordNet, VerbNet, Wiktionary\n",
      "WSC 554 104 146 coref. acc. ﬁction books\n",
      "• Evaluability: Tasks must have an automatic performance metric that corresponds well to\n",
      "human judgments of output quality. Certain text generation tasks fail to meet this criteria\n",
      "due to issues surrounding automatic metrics like ROUGE and BLEU (Callison-Burch et al.,\n",
      "2006; Liu et al., 2016, i.a.).\n",
      "• Public data: We require that tasks haveexisting public training data in order to minimize\n",
      "the risks involved in newly-created datasets. We also prefer tasks for which we have access\n",
      "to (or could create) a test set with private labels.\n",
      "• Task format: We prefer tasks that had relatively simple input and output formats, to avoid\n",
      "incentivizing the users of the benchmark to create complex task-speciﬁc model architectures.\n",
      "Nevertheless, while GLUE is restricted to tasks involving single sentence or sentence pair\n",
      "inputs, for SuperGLUE we expand the scope to consider tasks with longer inputs. This\n",
      "yields a set of tasks that requires understanding individual tokens in context, complete\n",
      "sentences, inter-sentence relations, and entire paragraphs.\n",
      "• License: We require that task data be available under licences that allow use and redistribu-\n",
      "tion for research purposes.\n",
      "To identify possible tasks for SuperGLUE, we disseminated a public call for task proposals to the\n",
      "NLP community, and received approximately 30 proposals. We ﬁltered these proposals according\n",
      "to our criteria. Many proposals were not suitable due to licensing issues, complex formats, and\n",
      "insufﬁcient headroom; we provide examples of such tasks in Appendix D. For each of the remaining\n",
      "tasks, we ran a BERT-based baseline and a human baseline, and ﬁltered out tasks which were either\n",
      "too challenging for humans without extensive training or too easy for our machine baselines.\n",
      "3.2 Selected Tasks\n",
      "Following this process, we arrived at eight tasks to use in SuperGLUE. See Tables 1 and 2 for details\n",
      "and speciﬁc examples of each task.\n",
      "BoolQ (Boolean Questions, Clark et al., 2019) is a QA task where each example consists of a short\n",
      "passage and a yes/no question about the passage. The questions are provided anonymously and\n",
      "unsolicited by users of the Google search engine, and afterwards paired with a paragraph from a\n",
      "Wikipedia article containing the answer. Following the original work, we evaluate with accuracy.\n",
      "CB (CommitmentBank, De Marneffe et al., 2019) is a corpus of short texts in which at least one\n",
      "sentence contains an embedded clause. Each of these embedded clauses is annotated with the degree\n",
      "to which it appears the person who wrote the text iscommitted to the truth of the clause. The resulting\n",
      "task framed as three-class textual entailment on examples that are drawn from the Wall Street Journal,\n",
      "ﬁction from the British National Corpus, and Switchboard. Each example consists of a premise\n",
      "containing an embedded clause and the corresponding hypothesis is the extraction of that clause.\n",
      "We use a subset of the data that had inter-annotator agreement above80%. The data is imbalanced\n",
      "(relatively fewer neutral examples), so we evaluate using accuracy and F1, where for multi-class F1\n",
      "we compute the unweighted average of the F1 per class.\n",
      "4\n",
      "Table 2: Development set examples from the tasks in SuperGLUE. Bold text represents part of the\n",
      "example format for each task. Text in italics is part of the model input. Underlined text is specially\n",
      "marked in the input. Text in a monospaced font represents the expected model output.\n",
      "BoolQ\n",
      "Passage: Barq’s – Barq’s is an American soft drink. Its brand of root beer is notable for having caffeine.\n",
      "Barq’s, created by Edward Barq and bottled since the turn of the 20th century, is owned by the Barq\n",
      "family but bottled by the Coca-Cola Company. It was known as Barq’s Famous Olde Tyme Root Beer\n",
      "until 2012.\n",
      "Question: is barq’s root beer a pepsi product Answer: No\n",
      "CB\n",
      "Text: B: And yet, uh, I we-, I hope to see employer based, you know, helping out. You know, child, uh,\n",
      "care centers at the place of employment and things like that, that will help out. A: Uh-huh. B: What do\n",
      "you think, do you think we are, setting a trend?\n",
      "Hypothesis: they are setting a trend Entailment: Unknown\n",
      "COPA\n",
      "Premise: My body cast a shadow over the grass. Question: What’s the CAUSE for this?\n",
      "Alternative 1: The sun was rising. Alternative 2: The grass was cut.\n",
      "Correct Alternative: 1\n",
      "MultiRC\n",
      "Paragraph: Susan wanted to have a birthday party. She called all of her friends. She has ﬁve friends.\n",
      "Her mom said that Susan can invite them all to the party. Her ﬁrst friend could not go to the party\n",
      "because she was sick. Her second friend was going out of town. Her third friend was not so sure if her\n",
      "parents would let her. The fourth friend said maybe. The ﬁfth friend could go to the party for sure. Susan\n",
      "was a little sad. On the day of the party, all ﬁve friends showed up. Each friend had a present for Susan.\n",
      "Susan was happy and sent each friend a thank you card the next week\n",
      "Question: Did Susan’s sick friend recover? Candidate answers: Yes, she recovered (T), No (F), Yes\n",
      "(T), No, she didn’t recover(F), Yes, she was at Susan’s party(T)\n",
      "ReCoRD\n",
      "Paragraph: (CNN) Puerto Rico on Sunday overwhelmingly voted for statehood. But Congress, the only\n",
      "body that can approve new states, will ultimately decide whether the status of the US commonwealth\n",
      "changes. Ninety-seven percent of the votes in the nonbinding referendum favored statehood, an increase\n",
      "over the results of a 2012 referendum, ofﬁcial results from the State Electorcal Commission show. It\n",
      "was the ﬁfth such vote on statehood. \"Today, we the people of Puerto Rico are sending a strong and\n",
      "clear message to the US Congress ... and to the world ... claiming our equal rights as American citizens,\n",
      "Puerto Rico Gov. Ricardo Rossello said in a news release. @highlight Puerto Rico voted Sunday in\n",
      "favor of US statehood\n",
      "Query For one, they can truthfully say, “Don’t blame me, I didn’t vote for them, ” when discussing the\n",
      "<placeholder> presidency Correct Entities: US\n",
      "RTE\n",
      "Text: Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44,\n",
      "according to the Christopher Reeve Foundation.\n",
      "Hypothesis: Christopher Reeve had an accident. Entailment: False\n",
      "WiC\n",
      "Context 1: Room and board. Context 2: He nailed boards across the windows.\n",
      "Sense match: False\n",
      "WSC\n",
      "Text: Mark told Pete many lies about himself, which Pete included in his book. He should have been\n",
      "more truthful. Coreference: False\n",
      "COPA (Choice of Plausible Alternatives, Roemmele et al., 2011) is a causal reasoning task in which\n",
      "a system is given a premise sentence and must determine either the cause or effect of the premise\n",
      "from two possible choices. All examples are handcrafted and focus on topics from blogs and a\n",
      "photography-related encyclopedia. Following the original work, we evaluate using accuracy.\n",
      "MultiRC (Multi-Sentence Reading Comprehension, Khashabi et al., 2018) is a QA task where each\n",
      "example consists of a context paragraph, a question about that paragraph, and a list of possible\n",
      "answers. The system must predict which answers are true and which are false. While many QA\n",
      "tasks exist, we use MultiRC because of a number of desirable properties: (i) each question can have\n",
      "multiple possible correct answers, so each question-answer pair must be evaluated independent of\n",
      "other pairs, (ii) the questions are designed such that answering each question requires drawing facts\n",
      "from multiple context sentences, and (iii) the question-answer pair format more closely matches\n",
      "the API of other tasks in SuperGLUE than the more popular span-extractive QA format does. The\n",
      "paragraphs are drawn from seven domains including news, ﬁction, and historical text. The evaluation\n",
      "metrics are F1 over all answer-options (F1a) and exact match of each question’s set of answers (EM).\n",
      "5\n",
      "ReCoRD (Reading Comprehension with Commonsense Reasoning Dataset, Zhang et al., 2018) is a\n",
      "multiple-choice QA task. Each example consists of a news article and a Cloze-style question about\n",
      "the article in which one entity is masked out. The system must predict the masked out entity from a\n",
      "given list of possible entities in the provided passage, where the same entity may be expressed using\n",
      "multiple different surface forms, all of which are considered correct. Articles are drawn from CNN\n",
      "and Daily Mail. Following the original work, we evaluate with max (over all mentions) token-level\n",
      "F1 and exact match (EM).\n",
      "RTE (Recognizing Textual Entailment) datasets come from a series of annual competitions on textual\n",
      "entailment.2 RTE is included in GLUE, and we use the same data and format as GLUE: We merge data\n",
      "from RTE1 (Dagan et al., 2006), RTE2 (Bar Haim et al., 2006), RTE3 (Giampiccolo et al., 2007), and\n",
      "RTE5 (Bentivogli et al., 2009).3 All datasets are combined and converted to two-class classiﬁcation:\n",
      "entailment and not_entailment. Of all the GLUE tasks, RTE is among those that beneﬁts from\n",
      "transfer learning the most, with performance jumping from near random-chance (∼56%) at the time\n",
      "of GLUE’s launch to 86.3% accuracy (Liu et al., 2019d; Yang et al., 2019) at the time of writing.\n",
      "Given the nearly eight point gap with respect to human performance, however, the task is not yet\n",
      "solved by machines, and we expect the remaining gap to be difﬁcult to close.\n",
      "WiC (Word-in-Context, Pilehvar and Camacho-Collados, 2019) is a word sense disambiguation task\n",
      "cast as binary classiﬁcation of sentence pairs. Given two text snippets and a polysemous word that\n",
      "appears in both sentences, the task is to determine whether the word is used with the same sense in\n",
      "both sentences. Sentences are drawn from WordNet (Miller, 1995), VerbNet (Schuler, 2005), and\n",
      "Wiktionary. We follow the original work and evaluate using accuracy.\n",
      "WSC (Winograd Schema Challenge, Levesque et al., 2012) is a coreference resolution task in\n",
      "which examples consist of a sentence with a pronoun and a list of noun phrases from the sentence.\n",
      "The system must determine the correct referrent of the pronoun from among the provided choices.\n",
      "Winograd schemas are designed to require everyday knowledge and commonsense reasoning to solve.\n",
      "GLUE includes a version of WSC recast as NLI, known as WNLI. Until very recently, no substantial\n",
      "progress had been made on WNLI, with many submissions opting to submit majority class predic-\n",
      "tions.4 In the past few months, several works (Kocijan et al., 2019; Liu et al., 2019d) have made rapid\n",
      "progress via a hueristic data augmentation scheme, raising machine performance to 90.4% accuracy.\n",
      "Given estimated human performance of ∼96%, there is still a gap between machine and human\n",
      "performance, which we expect will be relatively difﬁcult to close. We therefore include a version of\n",
      "WSC cast as binary classiﬁcation, where each example consists of a sentence with a marked pronoun\n",
      "and noun, and the task is to determine if the pronoun refers to that noun. The training and validation\n",
      "examples are drawn from the original WSC data (Levesque et al., 2012), as well as those distributed\n",
      "by the afﬁliated organization Commonsense Reasoning.5 The test examples are derived from ﬁction\n",
      "books and have been shared with us by the authors of the original dataset. We evaluate using accuracy.\n",
      "3.3 Scoring\n",
      "As with GLUE, we seek to give a sense of aggregate system performance over all tasks by averaging\n",
      "scores of all tasks. Lacking a fair criterion with which to weight the contributions of each task to\n",
      "the overall score, we opt for the simple approach of weighing each task equally, and for tasks with\n",
      "multiple metrics, ﬁrst averaging those metrics to get a task score.\n",
      "3.4 Tools for Model Analysis\n",
      "Analyzing Linguistic and World Knowledge in Models GLUE includes an expert-constructed,\n",
      "diagnostic dataset that automatically tests models for a broad range of linguistic, commonsense, and\n",
      "world knowledge. Each example in this broad-coverage diagnostic is a sentence pair labeled with\n",
      "2Textual entailment is also known as natural language inference, or NLI\n",
      "3RTE4 is not publicly available, while RTE6 and RTE7 do not conform to the standard NLI task.\n",
      "4WNLI is especially difﬁcult due to an adversarial train/dev split: Premise sentences that appear in the\n",
      "training set often appear in the development set with a different hypothesis and a ﬂipped label. If a system\n",
      "memorizes the training set, which was easy due to the small size of the training set, it could perform far below\n",
      "chance on the development set. We remove this adversarial design in our version of WSC by ensuring that no\n",
      "sentences are shared between the training, validation, and test sets.\n",
      "5http://commonsensereasoning.org/disambiguation.html\n",
      "6\n",
      "a three-way entailment relation (entailment, neutral, or contradiction) and tagged with labels that\n",
      "indicate the phenomena that characterize the relationship between the two sentences. Submissions\n",
      "to the GLUE leaderboard are required to include predictions from the submission’s MultiNLI\n",
      "classiﬁer on the diagnostic dataset, and analyses of the results were shown alongside the main\n",
      "leaderboard. Since this broad-coverage diagnostic task has proved difﬁcult for top models, we retain\n",
      "it in SuperGLUE. However, since MultiNLI is not part of SuperGLUE, we collapse contradiction\n",
      "and neutral into a single not_entailment label, and request that submissions include predictions\n",
      "on the resulting set from the model used for the RTE task. We collect non-expert annotations to\n",
      "estimate human performance, following the same procedure we use for the main benchmark tasks\n",
      "(Section 5.2). We estimate an accuracy of 88% and a Matthew’s correlation coefﬁcient (MCC, the\n",
      "two-class variant of the R3 metric used in GLUE) of 0.77.\n",
      "Analyzing Gender Bias in Models Recent work has identiﬁed the presence and ampliﬁcation\n",
      "of many social biases in data-driven machine learning models. (Lu et al., 2018; Zhao et al., 2018;\n",
      "Kiritchenko and Mohammad, 2018). To promote the detection of such biases, we include Winogender\n",
      "(Rudinger et al., 2018) as an additional diagnostic dataset. Winogender is designed to measure gender\n",
      "bias in coreference resolution systems. We use the Diverse Natural Language Inference Collection\n",
      "(DNC; Poliak et al., 2018) version that casts Winogender as a textual entailment task.6 Each example\n",
      "consists of a premise sentence with a male or female pronoun and a hypothesis giving a possible\n",
      "antecedent of the pronoun. Examples occur in minimal pairs, where the only difference between\n",
      "an example and its pair is the gender of the pronoun in the premise. Performance on Winogender\n",
      "is measured with both accuracy and the gender parity score: the percentage of minimal pairs for\n",
      "which the predictions are the same. We note that a system can trivially obtain a perfect gender parity\n",
      "score by guessing the same class for all examples, so a high gender parity score is meaningless unless\n",
      "accompanied by high accuracy. We collect non-expert annotations to estimate human performance,\n",
      "and observe an accuracy of 99.7% and a gender parity score of 0.99.\n",
      "Like any diagnostic, Winogender has limitations. It offers only positive predictive value: A poor\n",
      "bias score is clear evidence that a model exhibits gender bias, but a good score does not mean that\n",
      "the model is unbiased. More speciﬁcally, in the DNC version of the task, a low gender parity score\n",
      "means that a model’s prediction of textual entailment can be changed with a change in pronouns, all\n",
      "else equal. It is plausible that there are forms of bias that are relevant to target tasks of interest, but\n",
      "that do not surface in this setting (Gonen and Goldberg, 2019). In addition, Winogender does not\n",
      "cover all forms of social bias, or even all forms of gender. For instance, the version of the data used\n",
      "here offers no coverage of gender-neutral they or non-binary pronouns. Despite these limitations, we\n",
      "believe that Winogender’s inclusion is worthwhile in providing a coarse sense of how social biases\n",
      "evolve with model performance and for keeping attention on the social ramiﬁcations of NLP models.\n",
      "4 Using SuperGLUE\n",
      "Software Tools To facilitate using SuperGLUE, we releasejiant (Wang et al., 2019b),7 a modular\n",
      "software toolkit, built with PyTorch (Paszke et al., 2017), components from AllenNLP (Gardner\n",
      "et al., 2017), and the pytorch-pretrained-bert package.8 jiant implements our baselines and\n",
      "supports the evaluation of custom models and training methods on the benchmark tasks. The toolkit\n",
      "includes support for existing popular pretrained models such as OpenAI GPT and BERT, as well as\n",
      "support for multistage and multitask learning of the kind seen in the strongest models on GLUE.\n",
      "Eligibility Any system or method that can produce predictions for the SuperGLUE tasks is eligible\n",
      "for submission to the leaderboard, subject to the data-use and submission frequency policies stated\n",
      "immediately below. There are no restrictions on the type of methods that may be used, and there is\n",
      "no requirement that any form of parameter sharing or shared initialization be used across the tasks in\n",
      "the benchmark. To limit overﬁtting to the private test data, users are limited to a maximum of two\n",
      "submissions per day and six submissions per month.\n",
      "6We ﬁlter out 23 examples where the labels are ambiguous\n",
      "7https://github.com/nyu-mll/jiant\n",
      "8https://github.com/huggingface/pytorch-pretrained-BERT\n",
      "7\n",
      "Table 3: Baseline performance on the SuperGLUE test sets and diagnostics. For CB we report\n",
      "accuracy and macro-average F1. For MultiRC we report F1 on all answer-options and exact match\n",
      "of each question’s set of correct answers. AXb is the broad-coverage diagnostic task, scored using\n",
      "Matthews’ correlation (MCC). AXg is the Winogender diagnostic, scored using accuracy and the\n",
      "gender parity score (GPS). All values are scaled by 100. The Avg column is the overall benchmark\n",
      "score on non-AX∗ tasks. The bolded numbers reﬂect the best machine performance on task. *MultiRC\n",
      "has multiple test sets released on a staggered schedule, and these results evaluate on an installation of\n",
      "the test set that is a subset of ours.\n",
      "Model Avg BoolQ CB COPA MultiRC ReCoRD RTE WiC WSC AX b AXg\n",
      "Metrics Acc. F1/Acc. Acc. F1 a/EM F1/EM Acc. Acc. Acc. MCC GPS Acc.\n",
      "Most Frequent 47.1 62.3 21.7/48.4 50.0 61.1 / 0.3 33.4/32.5 50.3 50.0 65.1 0.0 100.0/ 50.0\n",
      "CBoW 44.3 62.1 49.0/71.2 51.6 0.0 / 0.4 14.0/13.6 49.7 53.0 65.1 -0.4 100.0/ 50.0\n",
      "BERT 69.0 77.4 75.7/83.6 70.6 70.0 / 24.0 72.0/71.3 71.6 69.5 64.3 23.0 97.8 / 51.7\n",
      "BERT++ 71.5 79.0 84.7/90.4 73.8 70.0 / 24.1 72.0/71.3 79.0 69.5 64.3 38.0 99.4 / 51.4\n",
      "Outside Best - 80.4 - / - 84.4 70.4 */24.5* 74.8/73.0 82.7 - - - - / -\n",
      "Human (est.) 89.8 89.0 95.8/98.9 100.0 81.8*/51.9* 91.7/91.3 93.6 80.0 100.0 77.0 99.3 / 99.7\n",
      "Data Data for the tasks are available for download through the SuperGLUE site and through a\n",
      "download script included with the software toolkit. Each task comes with a standardized training set,\n",
      "development set, and unlabeled test set. Submitted systems may use any public or private data when\n",
      "developing their systems, with a few exceptions: Systems may only use the SuperGLUE-distributed\n",
      "versions of the task datasets, as these use different train/validation/test splits from other public\n",
      "versions in some cases. Systems also may not use the unlabeled test data for the tasks in system\n",
      "development in any way, may not use the structured source data that was used to collect the WiC\n",
      "labels (sense-annotated example sentences from WordNet, VerbNet, and Wiktionary) in any way, and\n",
      "may not build systems that share information across separate test examples in any way.\n",
      "We do not endorse the use of the benchmark data for non-research applications, due to concerns\n",
      "about socially relevant biases (such as ethnicity–occupation associations) that may be undesirable\n",
      "or legally problematic in deployed systems. Because these biases are evident in texts from a wide\n",
      "variety of sources and collection methods (e.g., Rudinger et al., 2017), and because none of our task\n",
      "datasets directly mitigate them, one can reasonably presume that our training sets teach models these\n",
      "biases to some extent and that our evaluation sets similarly reward models that learn these biases.\n",
      "To ensure reasonable credit assignment, because we build very directly on prior work, we ask the\n",
      "authors of submitted systems to directly name and cite the speciﬁc datasets that they use,including the\n",
      "benchmark datasets. We will enforce this as a requirement for papers to be listed on the leaderboard.\n",
      "5 Experiments\n",
      "5.1 Baselines\n",
      "BERT Our main baselines are built around BERT, variants of which are among the most successful\n",
      "approach on GLUE at the time of writing. Speciﬁcally, we use the bert-large-cased variant.\n",
      "Following the practice recommended in Devlin et al. (2019), for each task, we use the simplest\n",
      "possible architecture on top of BERT. We ﬁne-tune a copy of the pretrained BERT model separately\n",
      "for each task, and leave the development of multi-task learning models to future work. For training,\n",
      "we use the procedure speciﬁed in Devlin et al. (2019): We use Adam (Kingma and Ba, 2014) with an\n",
      "initial learning rate of 10−5 and ﬁne-tune for a maximum of 10 epochs.\n",
      "For classiﬁcation tasks with sentence-pair inputs (BoolQ, CB, RTE, WiC), we concatenate the\n",
      "sentences with a [SEP ] token, feed the fused input to BERT, and use a logistic regression classiﬁer that\n",
      "sees the representation corresponding to [CLS ]. For WiC only, we also concatenate the representation\n",
      "of the marked word to the [CLS ] representation. For COPA, MultiRC, and ReCoRD, for each answer\n",
      "choice, we similarly concatenate the context with that answer choice and feed the resulting sequence\n",
      "into BERT to produce an answer representation. For COPA, we project these representations into a\n",
      "scalar, and take as the answer the choice with the highest associated scalar. For MultiRC, because\n",
      "each question can have more than one correct answer, we feed each answer representation into\n",
      "8\n",
      "a logistic regression classiﬁer. For ReCoRD, we also evaluate the probability of each candidate\n",
      "independent of other candidates, and take the most likely candidate as the model’s prediction. For\n",
      "WSC, which is a span-based task, we use a model inspired by Tenney et al. (2019). Given the BERT\n",
      "representation for each word in the original sentence, we get span representations of the pronoun\n",
      "and noun phrase via a self-attention span-pooling operator (Lee et al., 2017), before feeding it into a\n",
      "logistic regression classiﬁer.\n",
      "BERT++ We also report results using BERT with additional training on related datasets before\n",
      "ﬁne-tuning on the benchmark tasks, following the STILTs two-stage style of transfer learning (Phang\n",
      "et al., 2018). Given the productive use of MultiNLI in pretraining and intermediate ﬁne-tuning of\n",
      "pretrained language models (Conneau et al., 2017; Phang et al., 2018, i.a.), for CB, RTE, and BoolQ,\n",
      "we use MultiNLI as a transfer task by ﬁrst using the above procedure on MultiNLI. Similarly, given\n",
      "the similarity of COPA to SWAG (Zellers et al., 2018), we ﬁrst ﬁne-tune BERT on SWAG. These\n",
      "results are reported as BERT++. For all other tasks, we reuse the results of BERT ﬁne-tuned on just\n",
      "that task.\n",
      "Simple Baselines We include a baseline where for each task we simply predict the majority class,9\n",
      "as well as a bag-of-words baseline where each input is represented as an average of its tokens’ GloVe\n",
      "word vectors (the 300D/840B release from Pennington et al., 2014).\n",
      "Outside Best We list the best known result on each task to date, except on tasks which we recast\n",
      "(WSC), resplit (CB), or achieve the best known result (WiC). The outside results for COPA, MultiRC,\n",
      "and RTE are from Sap et al. (2019), Trivedi et al. (2019), and Liu et al. (2019d) respectively.\n",
      "5.2 Human Performance\n",
      "Pilehvar and Camacho-Collados (2019), Khashabi et al. (2018), Nangia and Bowman (2019), and\n",
      "Zhang et al. (2018) respectively provide estimates for human performance on WiC, MultiRC, RTE,\n",
      "and ReCoRD. For the remaining tasks, including the diagnostic set, we estimate human performance\n",
      "by hiring crowdworker annotators through Amazon’s Mechanical Turk platform to reannotate a\n",
      "sample of each test set. We follow a two step procedure where a crowd worker completes a short\n",
      "training phase before proceeding to the annotation phase, modeled after the method used by Nangia\n",
      "and Bowman (2019) for GLUE. For both phases and all tasks, the average pay rate is $23.75/hr.10\n",
      "In the training phase, workers are provided with instructions on the task, linked to an FAQ page, and\n",
      "are asked to annotate up to 30 examples from the development set. After answering each example,\n",
      "workers are also asked to check their work against the provided ground truth label. After the training\n",
      "phase is complete, we provide the qualiﬁcation to work on the annotation phase to all workers\n",
      "who annotated a minimum of ﬁve examples, i.e. completed ﬁve HITs during training and achieved\n",
      "performance at, or above the median performance across all workers during training.\n",
      "In the annotation phase, workers are provided with the same instructions as the training phase, and\n",
      "are linked to the same FAQ page. The instructions for all tasks are provided in Appendix C. For the\n",
      "annotation phase we randomly sample 100 examples from the task’s test set, with the exception of\n",
      "WSC where we annotate the full test set. For each example, we collect annotations from ﬁve workers\n",
      "and take a majority vote to estimate human performance. For additional details, see Appendix C.3.\n",
      "5.3 Results\n",
      "Table 3 shows results for all baselines. The simple baselines of predicting the most frequent class\n",
      "and CBOW do not perform well overall, achieving near chance performance for several of the tasks.\n",
      "Using BERT increases the average SuperGLUE score by 25 points, attaining signiﬁcant gains on\n",
      "all of the benchmark tasks, particularly MultiRC, ReCoRD, and RTE. On WSC, BERT actually\n",
      "performs worse than the simple baselines, likely due to the small size of the dataset and the lack of\n",
      "data augmentation. Using MultiNLI as an additional source of supervision for BoolQ, CB, and RTE\n",
      "leads to a 2-5 point improvement on all tasks. Using SWAG as a transfer task for COPA sees an 8\n",
      "point improvement.\n",
      "9For ReCoRD, we predict the entity that has the highest F1 with the other entity options.\n",
      "10This estimate is taken from https://turkerview.com.\n",
      "9\n",
      "Our best baselines still lag substantially behind human performance. On average, there is a nearly 20\n",
      "point gap between BERT++ and human performance. The largest gap is on WSC, with a 35 point\n",
      "difference between the best model and human performance. The smallest margins are on BoolQ,\n",
      "CB, RTE, and WiC, with gaps of around 10 points on each of these. We believe these gaps will be\n",
      "challenging to close: On WSC and COPA, human performance is perfect. On three other tasks, it is\n",
      "in the mid-to-high 90s. On the diagnostics, all models continue to lag signiﬁcantly behind humans.\n",
      "Though all models obtain near perfect gender parity scores on Winogender, this is due to the fact that\n",
      "they are obtaining accuracy near that of random guessing.\n",
      "6 Conclusion\n",
      "We present SuperGLUE, a new benchmark for evaluating general-purpose language understanding\n",
      "systems. SuperGLUE updates the GLUE benchmark by identifying a new set of challenging NLU\n",
      "tasks, as measured by the difference between human and machine baselines. The set of eight tasks in\n",
      "our benchmark emphasizes diverse task formats and low-data training data tasks, with nearly half the\n",
      "tasks having fewer than 1k examples and all but one of the tasks having fewer than 10k examples.\n",
      "We evaluate BERT-based baselines and ﬁnd that they still lag behind humans by nearly 20 points.\n",
      "Given the difﬁculty of SuperGLUE for BERT, we expect that further progress in multi-task, transfer,\n",
      "and unsupervised/self-supervised learning techniques will be necessary to approach human-level per-\n",
      "formance on the benchmark. Overall, we argue that SuperGLUE offers a rich and challenging testbed\n",
      "for work developing new general-purpose machine learning methods for language understanding.\n",
      "7 Acknowledgments\n",
      "We thank the original authors of the included datasets in SuperGLUE for their cooperation in the\n",
      "creation of the benchmark, as well as those who proposed tasks and datasets that we ultimately could\n",
      "not include.\n",
      "This work was made possible in part by a donation to NYU from Eric and Wendy Schmidt made by\n",
      "recommendation of the Schmidt Futures program. We gratefully acknowledge the support of the\n",
      "NVIDIA Corporation with the donation of a Titan V GPU used at NYU for this research. AW is\n",
      "supported by the National Science Foundation Graduate Research Fellowship Program under Grant\n",
      "No. DGE 1342536. Any opinions, ﬁndings, and conclusions or recommendations expressed in this\n",
      "material are those of the author(s) and do not necessarily reﬂect the views of the National Science\n",
      "Foundation.\n",
      "References\n",
      "Anonymous. Bam! Born-again multi-task networks for natural language understanding. Anonymous\n",
      "preprint under review, 2018. URL https://openreview.net/forum?id=SylnYlqKw4.\n",
      "Stephen H. Bach, Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao, Cassandra Xia, Souvik\n",
      "Sen, Alexander Ratner, Braden Hancock, Houman Alborzi, Rahul Kuchhal, Christopher Ré, and\n",
      "Rob Malkin. Snorkel drybell: A case study in deploying weak supervision at industrial scale. In\n",
      "SIGMOD. ACM, 2018.\n",
      "Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and\n",
      "Idan Szpektor. The second PASCAL recognising textual entailment challenge. In Proceedings\n",
      "of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, 2006. URL\n",
      "http://u.cs.biu.ac.il/~nlp/RTE2/Proceedings/01.pdf.\n",
      "Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. The\n",
      "ﬁfth PASCAL recognizing textual entailment challenge. In Textual Analysis Conference (TAC),\n",
      "2009. URL http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.232.1231.\n",
      "Sven Buechel, Anneke Buffone, Barry Slaff, Lyle Ungar, and João Sedoc. Modeling empathy and\n",
      "distress in reaction to news stories. In Proceedings of the 2018 Conference on Empirical Methods\n",
      "in Natural Language Processing (EMNLP), 2018.\n",
      "10\n",
      "Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluation the role of bleu in machine\n",
      "translation research. In Proceedings of the Conference of the European Chapter of the Association\n",
      "for Computational Linguistics (EACL). Association for Computational Linguistics, 2006. URL\n",
      "https://www.aclweb.org/anthology/E06-1032.\n",
      "Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task\n",
      "1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings\n",
      "of the 11th International Workshop on Semantic Evaluation (SemEval-2017) . Association for\n",
      "Computational Linguistics, 2017. doi: 10.18653/v1/S17-2001. URL https://www.aclweb.\n",
      "org/anthology/S17-2001.\n",
      "Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke\n",
      "Zettlemoyer. QuAC: Question answering in context. In Proceedings of the 2018 Conference on\n",
      "Empirical Methods in Natural Language Processing (EMNLP). Association for Computational\n",
      "Linguistics, 2018a.\n",
      "Eunsol Choi, Omer Levy, Yejin Choi, and Luke Zettlemoyer. Ultra-ﬁne entity typing. In Proceedings\n",
      "of the Association for Computational Linguistics (ACL). Association for Computational Linguistics,\n",
      "2018b. URL https://www.aclweb.org/anthology/P18-1009.\n",
      "Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\n",
      "Toutanova. Boolq: Exploring the surprising difﬁculty of natural yes/no questions. In Proceedings\n",
      "of the 2019 Conference of the North American Chapter of the Association for Computational\n",
      "Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924–2936,\n",
      "2019.\n",
      "Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: Deep\n",
      "neural networks with multitask learning. In Proceedings of the 25th International Conference on\n",
      "Machine Learning (ICML). Association for Computing Machinery, 2008. URL https://dl.acm.\n",
      "org/citation.cfm?id=1390177.\n",
      "Alexis Conneau and Douwe Kiela. SentEval: An evaluation toolkit for universal sentence representa-\n",
      "tions. In Proceedings of the 11th Language Resources and Evaluation Conference. European Lan-\n",
      "guage Resource Association, 2018. URL https://www.aclweb.org/anthology/L18-1269.\n",
      "Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes. Super-\n",
      "vised learning of universal sentence representations from natural language inference data. In\n",
      "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing\n",
      "(EMNLP). Association for Computational Linguistics, 2017. doi: 10.18653/v1/D17-1070. URL\n",
      "https://www.aclweb.org/anthology/D17-1070.\n",
      "Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entail-\n",
      "ment challenge. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Vi-\n",
      "sual Object Classiﬁcation, and Recognising Textual Entailment . Springer, 2006. URL https:\n",
      "//link.springer.com/chapter/10.1007/11736790_9.\n",
      "Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in Neural\n",
      "Information Processing Systems (NeurIPS). Curran Associates, Inc., 2015. URL http://papers.\n",
      "nips.cc/paper/5949-semi-supervised-sequence-learning.pdf .\n",
      "Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The CommitmentBank:\n",
      "Investigating projection in naturally occurring discourse. 2019. To appear in Proceedings of Sinn\n",
      "und Bedeutung 23. Data can be found at https://github.com/mcdm/CommitmentBank/.\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\n",
      "bidirectional transformers for language understanding. In Proceedings of the Conference of the\n",
      "North American Chapter of the Association for Computational Linguistics: Human Language\n",
      "Technologies (NAACL-HLT). Association for Computational Linguistics, 2019. URL https:\n",
      "//arxiv.org/abs/1810.04805.\n",
      "William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.\n",
      "In Proceedings of IWP, 2005.\n",
      "11\n",
      "Manaal Faruqui and Dipanjan Das. Identifying well-formed natural language questions. In Pro-\n",
      "ceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP) .\n",
      "Association for Computational Linguistics, 2018. URLhttps://www.aclweb.org/anthology/\n",
      "D18-1091.\n",
      "Tommaso Furlanello, Zachary C Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar.\n",
      "Born again neural networks. International Conference on Machine Learning (ICML), 2018. URL\n",
      "http://proceedings.mlr.press/v80/furlanello18a.html.\n",
      "Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew\n",
      "Peters, Michael Schmitz, and Luke S. Zettlemoyer. AllenNLP: A deep semantic natural language\n",
      "processing platform. In Proceedings of Workshop for NLP Open Source Software, 2017. URL\n",
      "https://www.aclweb.org/anthology/W18-2501.\n",
      "Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing\n",
      "textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment\n",
      "and Paraphrasing. Association for Computational Linguistics, 2007.\n",
      "Hila Gonen and Yoav Goldberg. Lipstick on a pig: Debiasing methods cover up systematic\n",
      "gender biases in word embeddings but do not remove them. In Proceedings of the 2019\n",
      "Conference of the North American Chapter of the Association for Computational Linguistics:\n",
      "Human Language Technologies, Volume 1 (Long and Short Papers) , pages 609–614, Min-\n",
      "neapolis, Minnesota, June 2019. Association for Computational Linguistics. URL https:\n",
      "//www.aclweb.org/anthology/N19-1061.\n",
      "Felix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences\n",
      "from unlabelled data. In Proceedings of the Conference of the North American Chapter of\n",
      "the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).\n",
      "Association for Computational Linguistics, 2016. doi: 10.18653/v1/N16-1162. URL https:\n",
      "//www.aclweb.org/anthology/N16-1162.\n",
      "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv\n",
      "preprint 1503.02531, 2015. URL https://arxiv.org/abs/1503.02531.\n",
      "Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems. In\n",
      "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).\n",
      "Association for Computational Linguistics, 2017. doi: 10.18653/v1/D17-1215. URL https:\n",
      "//www.aclweb.org/anthology/D17-1215.\n",
      "Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking\n",
      "beyond the surface: A challenge set for reading comprehension over multiple sentences. In\n",
      "Proceedings of the Conference of the North American Chapter of the Association for Computa-\n",
      "tional Linguistics: Human Language Technologies (NAACL-HLT). Association for Computational\n",
      "Linguistics, 2018. URL https://www.aclweb.org/anthology/papers/N/N18/N18-1023/.\n",
      "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n",
      "1412.6980, 2014. URL https://arxiv.org/abs/1412.6980.\n",
      "Svetlana Kiritchenko and Saif Mohammad. Examining gender and race bias in two hundred sentiment\n",
      "analysis systems. In Proceedings of the Seventh Joint Conference on Lexical and Computational\n",
      "Semantics. Association for Computational Linguistics, 2018. doi: 10.18653/v1/S18-2005. URL\n",
      "https://www.aclweb.org/anthology/S18-2005.\n",
      "Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba,\n",
      "and Sanja Fidler. Skip-thought vectors. In Advances in neural information processing systems,\n",
      "2015.\n",
      "Nikita Kitaev and Dan Klein. Multilingual constituency parsing with self-attention and pre-training.\n",
      "arXiv preprint 1812.11760, 2018. URL https://arxiv.org/abs/1812.11760.\n",
      "Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas Lukasiewicz.\n",
      "A surprisingly robust trick for winograd schema challenge. arXiv preprint 1905.06290, 2019.\n",
      "12\n",
      "Kenton Lee, Luheng He, Mike Lewis, and Luke Zettlemoyer. End-to-end neural coreference\n",
      "resolution. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language\n",
      "Processing. Association for Computational Linguistics, September 2017. doi: 10.18653/v1/\n",
      "D17-1018. URL https://www.aclweb.org/anthology/D17-1018.\n",
      "Hector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge. In\n",
      "Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning,\n",
      "2012. URL http://dl.acm.org/citation.cfm?id=3031843.3031909.\n",
      "Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, and Joelle Pineau.\n",
      "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics\n",
      "for dialogue response generation. In Proceedings of the 2016 Conference on Empirical Methods in\n",
      "Natural Language Processing. Association for Computational Linguistics, 2016. doi: 10.18653/\n",
      "v1/D16-1230. URL https://www.aclweb.org/anthology/D16-1230.\n",
      "Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. Linguistic\n",
      "knowledge and transferability of contextual representations. In Proceedings of the Conference of\n",
      "the North American Chapter of the Association for Computational Linguistics: Human Language\n",
      "Technologies (NAACL-HLT). Association for Computational Linguistics, 2019a. URL https:\n",
      "//arxiv.org/abs/1903.08855.\n",
      "Nelson F. Liu, Roy Schwartz, and Noah A. Smith. Inoculation by ﬁne-tuning: A method for\n",
      "analyzing challenge datasets. In Proceedings of the Conference of the North American Chapter\n",
      "of the Association for Computational Linguistics: Human Language Technologies (NAACL-\n",
      "HLT). Association for Computational Linguistics, 2019b. URL https://arxiv.org/abs/1904.\n",
      "02668.\n",
      "Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Improving multi-task deep neural\n",
      "networks via knowledge distillation for natural language understanding.arXiv preprint 1904.09482,\n",
      "2019c. URL http://arxiv.org/abs/1904.09482.\n",
      "Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for\n",
      "natural language understanding. arXiv preprint 1901.11504, 2019d.\n",
      "Kaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Amancharla, and Anupam Datta. Gender bias in\n",
      "neural natural language processing. arXiv preprint 1807.11714, 2018. URL http://arxiv.org/\n",
      "abs/1807.11714.\n",
      "Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in transla-\n",
      "tion: Contextualized word vectors. In Advances in Neural Information Processing Sys-\n",
      "tems (NeurIPS) . Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/\n",
      "7209-learned-in-translation-contextualized-word-vectors.pdf .\n",
      "Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language\n",
      "decathlon: Multitask learning as question answering. arXiv preprint 1806.08730, 2018. URL\n",
      "https://arxiv.org/abs/1806.08730.\n",
      "R. Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic\n",
      "heuristics in natural language inference. In Proceedings of the Association for Computational\n",
      "Linguistics (ACL). Association for Computational Linguistics, 2019. URL https://arxiv.org/\n",
      "abs/1902.01007.\n",
      "Richard T. McCoy and Tal Linzen. Non-entailed subsequences as a challenge for natural language\n",
      "inference. In Proceedings of the Society for Computational in Linguistics (SCiL) 2019, 2019. URL\n",
      "https://scholarworks.umass.edu/scil/vol2/iss1/46/.\n",
      "George A Miller. WordNet: a lexical database for english. Communications of the ACM, 1995. URL\n",
      "https://www.aclweb.org/anthology/H94-1111.\n",
      "Aakanksha Naik, Abhilasha Ravichander, Norman M. Sadeh, Carolyn Penstein Rosé, and Graham\n",
      "Neubig. Stress test evaluation for natural language inference. In International Conference on\n",
      "Computational Linguistics (COLING), 2018.\n",
      "13\n",
      "Nikita Nangia and Samuel R. Bowman. Human vs. Muppet: A conservative estimate of hu-\n",
      "man performance on the GLUE benchmark. In Proceedings of the Association of Compu-\n",
      "tational Linguistics (ACL) . Association for Computational Linguistics, 2019. URL https:\n",
      "//woollysocks.github.io/assets/GLUE_Human_Baseline.pdf.\n",
      "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\n",
      "Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in\n",
      "PyTorch. In Advances in Neural Information Processing Systems (NeurIPS). Curran Associates,\n",
      "Inc., 2017. URL https://openreview.net/pdf?id=BJJsrmfCZ.\n",
      "Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word\n",
      "representation. In Proceedings of the Conference on Empirical Methods in Natural Language Pro-\n",
      "cessing (EMNLP). Association for Computational Linguistics, 2014. doi: 10.3115/v1/D14-1162.\n",
      "URL https://www.aclweb.org/anthology/D14-1162.\n",
      "Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\n",
      "Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the Conference of\n",
      "the North American Chapter of the Association for Computational Linguistics: Human Language\n",
      "Technologies (NAACL-HLT). Association for Computational Linguistics, 2018. doi: 10.18653/v1/\n",
      "N18-1202. URL https://www.aclweb.org/anthology/N18-1202.\n",
      "Jason Phang, Thibault Févry, and Samuel R Bowman. Sentence encoders on STILTs: Supplementary\n",
      "training on intermediate labeled-data tasks. arXiv preprint 1811.01088 , 2018. URL https:\n",
      "//arxiv.org/abs/1811.01088.\n",
      "Mohammad Taher Pilehvar and Jose Camacho-Collados. WiC: The word-in-context dataset for\n",
      "evaluating context-sensitive meaning representations. In Proceedings of the Conference of the\n",
      "North American Chapter of the Association for Computational Linguistics: Human Language\n",
      "Technologies (NAACL-HLT). Association for Computational Linguistics, 2019. URL https:\n",
      "//arxiv.org/abs/1808.09121.\n",
      "Adam Poliak, Aparajita Haldar, Rachel Rudinger, J. Edward Hu, Ellie Pavlick, Aaron Steven White,\n",
      "and Benjamin Van Durme. Collecting diverse natural language inference problems for sentence\n",
      "representation evaluation. In Proceedings of the 2018 Conference on Empirical Methods in\n",
      "Natural Language Processing. Association for Computational Linguistics, 2018. URL https:\n",
      "//www.aclweb.org/anthology/D18-1007.\n",
      "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language un-\n",
      "derstanding by generative pre-training, 2018. Unpublished ms. available through a link at\n",
      "https://blog.openai.com/language-unsupervised/.\n",
      "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions\n",
      "for machine comprehension of text. In Proceedings of the Conference on Empirical Methods in\n",
      "Natural Language Processing (EMNLP). Association for Computational Linguistics, 2016. doi:\n",
      "10.18653/v1/D16-1264. URL http://aclweb.org/anthology/D16-1264.\n",
      "Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S. Gordon. Choice of plausible alternatives:\n",
      "An evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series, 2011.\n",
      "Rachel Rudinger, Chandler May, and Benjamin Van Durme. Social bias in elicited natural language\n",
      "inferences. In Proceedings of the First ACL Workshop on Ethics in Natural Language Processing.\n",
      "Association for Computational Linguistics, 2017. doi: 10.18653/v1/W17-1609. URL https:\n",
      "//www.aclweb.org/anthology/W17-1609.\n",
      "Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias in\n",
      "coreference resolution. In Proceedings of the 2018 Conference of the North American Chapter\n",
      "of the Association for Computational Linguistics: Human Language Technologies. Association\n",
      "for Computational Linguistics, 2018. doi: 10.18653/v1/N18-2002. URL https://www.aclweb.\n",
      "org/anthology/N18-2002.\n",
      "Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense\n",
      "reasoning about social interactions. arXiv preprint 1904.09728, 2019. URL https://arxiv.\n",
      "org/abs/1904.09728.\n",
      "14\n",
      "Nathan Schneider and Noah A Smith. A corpus and model integrating multiword expressions and\n",
      "supersenses. In Proceedings of the Conference of the North American Chapter of the Association\n",
      "for Computational Linguistics: Human Language Technologies (NAACL-HLT). Association for\n",
      "Computational Linguistics, 2015. URL https://www.aclweb.org/anthology/N15-1177.\n",
      "Karin Kipper Schuler. Verbnet: A Broad-coverage, Comprehensive Verb Lexicon. PhD thesis, 2005.\n",
      "URL http://verbs.colorado.edu/~kipper/Papers/dissertation.pdf.\n",
      "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng,\n",
      "and Christopher. Potts. Recursive deep models for semantic compositionality over a sentiment\n",
      "treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing\n",
      "(EMNLP). Association for Computational Linguistics, 2013. URL https://www.aclweb.org/\n",
      "anthology/D13-1170.\n",
      "Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim,\n",
      "Benjamin Van Durme, Sam Bowman, Dipanjan Das, and Ellie Pavlick. What do you learn from\n",
      "context? probing for sentence structure in contextualized word representations. 2019. URL\n",
      "https://openreview.net/forum?id=SJzSgnRcKX.\n",
      "Harsh Trivedi, Heeyoung Kwon, Tushar Khot, Ashish Sabharwal, and Niranjan Balasubramanian.\n",
      "Repurposing entailment for multi-hop question answering tasks, 2019. URL https://arxiv.\n",
      "org/abs/1904.09380.\n",
      "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\n",
      "GLUE: A multi-task benchmark and analysis platform for natural language understanding. In\n",
      "International Conference on Learning Representations , 2019a. URL https://openreview.\n",
      "net/forum?id=rJ4km2R5t7.\n",
      "Alex Wang, Ian F. Tenney, Yada Pruksachatkun, Katherin Yu, Jan Hula, Patrick Xia, Raghu Pappagari,\n",
      "Shuning Jin, R. Thomas McCoy, Roma Patel, Yinghui Huang, Jason Phang, Edouard Grave,\n",
      "Najoung Kim, Phu Mon Htut, Thibault F’evry, Berlin Chen, Nikita Nangia, Haokun Liu, , Anhad\n",
      "Mohananey, Shikha Bordia, Ellie Pavlick, and Samuel R. Bowman. jiant 1.0: A software toolkit\n",
      "for research on general-purpose text understanding models. http://jiant.info/, 2019b.\n",
      "Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments.\n",
      "arXiv preprint 1805.12471, 2018. URL https://arxiv.org/abs/1805.12471.\n",
      "Kellie Webster, Marta Recasens, Vera Axelrod, and Jason Baldridge. Mind the GAP: A balanced\n",
      "corpus of gendered ambiguous pronouns. Transactions of the Association for Computational\n",
      "Linguistics (TACL), 2018. URL https://www.aclweb.org/anthology/Q18-1042.\n",
      "Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for\n",
      "sentence understanding through inference. In Proceedings of the Conference of the North American\n",
      "Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-\n",
      "HLT). Association for Computational Linguistics, 2018. URLhttp://aclweb.org/anthology/\n",
      "N18-1101.\n",
      "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V .\n",
      "Le. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint\n",
      "1906.0823, 2019.\n",
      "Fabio Massimo Zanzotto and Lorenzo Ferrone. Have you lost the thread? discovering ongoing\n",
      "conversations in scattered dialog blocks. ACM Transactions on Interactive Intelligent Systems\n",
      "(TiiS), 2017.\n",
      "Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. SW AG: A large-scale adversarial dataset\n",
      "for grounded commonsense inference. 2018. URL https://www.aclweb.org/anthology/\n",
      "D18-1009.\n",
      "Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme.\n",
      "Record: Bridging the gap between human and machine commonsense reading comprehension.\n",
      "arXiv preprint 1810.12885, 2018.\n",
      "15\n",
      "Yuan Zhang, Jason Baldridge, and Luheng He. PAWS: Paraphrase adversaries from word scrambling.\n",
      "arXiv preprint 1904.01130, 2019. URL https://arxiv.org/abs/1904.01130.\n",
      "Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Gender bias in\n",
      "coreference resolution: Evaluation and debiasing methods. In Proceedings of the 2018 Conference\n",
      "of the North American Chapter of the Association for Computational Linguistics: Human Language\n",
      "Technologies. Association for Computational Linguistics, 2018. doi: 10.18653/v1/N18-2003. URL\n",
      "https://www.aclweb.org/anthology/N18-2003.\n",
      "16\n",
      "Table 4: Baseline performance on the SuperGLUE development.\n",
      "Model Avg BoolQ CB COPA MultiRC ReCoRD RTE WiC WSC\n",
      "Metrics Acc. Acc./F1 Acc. F1 a/EM F1/EM Acc. Acc. Acc.\n",
      "Most Frequent Class 47.7 62.2 50 /22.2 55 59.9/ 0.8 32.4/31.5 52.7 50.0 63.5\n",
      "CBOW 47.7 62.4 71.4/49.6 63.0 20.3/ 0.3 14.4/13.8 54.2 55.3 61.5\n",
      "BERT 72.2 77.7 94.6/93.7 69.0 70.5/24.7 70.6/69.8 75.8 74.9 68.3\n",
      "BERT++ 74.6 80.1 96.4/95.0 78.0 70.5/24.7 70.6/69.8 82.3 74.9 68.3\n",
      "A Development Set Results\n",
      "In Table 4, we present results of the baselines on the SuperGLUE tasks development sets.\n",
      "B Performance on GLUE Diagnostics\n",
      "Figure 2 shows the performance on the GLUE diagnostics dataset for systems submitted to the public\n",
      "leaderboard.\n",
      "Disjunction Downward Monotone Restrictivity Double Negation Prepositional Phrases\n",
      "60\n",
      "40\n",
      "20\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "Chance\n",
      "BiLSTM+ELMo+Attn\n",
      "OpenAI GPT\n",
      "BERT + Single-task Adapters\n",
      "BERT (Large)\n",
      "BERT on STILTs\n",
      "BERT + BAM\n",
      "SemBERT\n",
      "Snorkel MeTaL\n",
      "ALICE (Large)\n",
      "MT-DNN (ensemble)\n",
      "XLNet-Large (ensemble)\n",
      "Figure 2: Performance of GLUE submissions on selected diagnostic categories, reported using the\n",
      "R3 metric scaled up by 100, as in Wang et al. (2019a, see paper for a description of the categories).\n",
      "Some initially difﬁcult categories, like double negation, saw gains from advances on GLUE, but\n",
      "others remain hard (restrictivity) or even adversarial (disjunction, downward monotone).\n",
      "C Instructions to Crowd Workers\n",
      "C.1 Training Phase Instructions\n",
      "For collecting data to establish human performance on the SuperGLUE tasks, we follow a two\n",
      "step procedure where we ﬁrst provide some training to the crowd workers before they proceed to\n",
      "annotation. In the training step, we provide workers with brief instructions about the training phase.\n",
      "An example of these instructions is given Table 5. These training instructions are the same across\n",
      "tasks, only the task name in the instructions is changed.\n",
      "17\n",
      "C.2 Task Instructions\n",
      "During training and annotation for each task, we provide workers with brief instructions tailored to\n",
      "the task. We also link workers to an FAQ page for the task. Tables 6, 7, 8, and 9, show the instructions\n",
      "we used for all four tasks: COPA, CommitmentBank, WSC, and BoolQ respectively. The instructions\n",
      "given to crowd workers for annotations on the diagnostic and bias diagnostic datasets are shown in\n",
      "Table 11.\n",
      "We collected data to produce conservative estimates for human performance on several tasks that\n",
      "we did not ultimately include in our benchmark, including GAP (Webster et al., 2018), PAWS\n",
      "(Zhang et al., 2019), Quora Insincere Questions,11 Ultraﬁne Entity Typing (Choi et al., 2018b), and\n",
      "Empathetic Reactions datasets (Buechel et al., 2018). The instructions we used for these tasks are\n",
      "shown in Tables 12, 13, 14, 15, and 16.\n",
      "C.3 Task Speciﬁc Details\n",
      "For WSC and COPA we provide annotators with a two way classiﬁcation problem. We then use\n",
      "majority vote across annotations to calculate human performance.\n",
      "CommitmentBank We follow the authors in providing annotators with a 7-way classiﬁcation\n",
      "problem. We then collapse the annotations into 3 classes by using the same ranges for bucketing used\n",
      "by De Marneffe et al. (2019). We then use majority vote to get human performance numbers on the\n",
      "task.\n",
      "Furthermore, for training on CommitmentBank we randomly sample examples from the low inter-\n",
      "annotator agreement portion of the CommitmentBank data that is not included in the benchmark\n",
      "version of the task. These low agreement examples are generally harder to classify since they are\n",
      "more ambiguous.\n",
      "Diagnostic Dataset Since the diagnostic dataset does not come with accompanying training data,\n",
      "we train our workers on examples from RTE’s development set. RTE is also a textual entailment\n",
      "task and is the most closely related task in the main benchmark. Providing the crowd workers with\n",
      "training on RTE enables them to learn label deﬁnitions which should generalize to the diagnostic\n",
      "dataset.\n",
      "Ultraﬁne Entity Typing We cast the task into a binary classiﬁcation problem to make it an easier\n",
      "task for non-expert crowd workers. We work in cooperation with the authors of the dataset (Choi\n",
      "et al., 2018b) to do this reformulation: We give workers one possible tag for a word or phrase and\n",
      "asked them to classify the tag as being applicable or not.\n",
      "The authors used WordNet (Miller, 1995) to expand the set of labels to include synonyms and\n",
      "hypernyms from WordNet. They then asked ﬁve annotators to validate these tags. The tags from this\n",
      "validation had high agreement, and were included in the publicly available Ultraﬁne Entity Typing\n",
      "dataset,12 This constitutes our set of positive examples. The rest of the tags from the validation\n",
      "procedure that are not in the public dataset constitute our negative examples.\n",
      "GAP For the Gendered Ambiguous Pronoun Coreference task (GAP, Webster et al., 2018), we\n",
      "simpliﬁed the task by providing noun phrase spans as part of the input, thus reducing the original\n",
      "structure prediction task to a classiﬁcation task. This task was presented to crowd workers as a three\n",
      "way classiﬁcation problem: Choose span A, B, or neither.\n",
      "D Excluded Tasks\n",
      "In this section we provide some examples of tasks that we evaluated for inclusion but ultimately could\n",
      "not include. We report on these excluded tasks only with the permission of their authors. We turned\n",
      "down many medical text datasets because they are usually only accessible with explicit permission\n",
      "and credentials from the data owners.\n",
      "11https://www.kaggle.com/c/quora-insincere-questions-classification/data\n",
      "12https://homes.cs.washington.edu/~eunsol/open_entity.html\n",
      "18\n",
      "Tasks like QuAC (Choi et al., 2018a) and STREUSLE (Schneider and Smith, 2015) differed substan-\n",
      "tially from the format of other tasks in our benchmark, which we worried would incentivize users\n",
      "to spend signiﬁcant effort on task-speciﬁc model designs, rather than focusing on general-purpose\n",
      "techniques. It was challenging to train annotators to do well on Quora Insincere Questions 13, Empa-\n",
      "thetic Reactions (Buechel et al., 2018), and a recast version of Ultra-Fine Entity Typing (Choi et al.,\n",
      "2018b, see Appendix C.3 for details), leading to low human performance. BERT achieved very high\n",
      "or superhuman performance on Query Well-Formedness (Faruqui and Das, 2018), PAWS (Zhang\n",
      "et al., 2019), Discovering Ongoing Conversations (Zanzotto and Ferrone, 2017), and GAP (Webster\n",
      "et al., 2018).\n",
      "During the process of selecting tasks for our benchmark, we collected human performance baselines\n",
      "and run BERT-based machine baselines for some tasks that we ultimately excluded from our task\n",
      "list. We chose to exclude these tasks because our BERT baseline performs better than our human\n",
      "performance baseline or if the gap between human and machine performance is small.\n",
      "On Quora Insincere Questions our BERT baseline outperforms our human baseline by a small margin:\n",
      "an F1 score of 67.2 versus 66.7 for BERT and human baselines respectively. Similarly, on the\n",
      "Empathetic Reactions dataset, BERT outperforms our human baseline, where BERT’s predictions\n",
      "have a Pearson correlation of 0.45 on empathy and 0.55 on distress, compared to 0.45 and 0.35 for\n",
      "our human baseline. For PAWS-Wiki, we report that BERT achieves an accuracy of 91.9%, while our\n",
      "human baseline achieved 84% accuracy. These three tasks are excluded from the benchmark since\n",
      "our, admittedly conservative, human baselines are worse than machine performance. Our human\n",
      "performance baselines are subject to the clarity of our instructions (all instructions can be found in\n",
      "Appendix C), and crowd workers engagement and ability.\n",
      "For the Query Well-Formedness task, the authors set an estimate human performance at 88.4%\n",
      "accuracy. Our BERT baseline model reaches an accuracy of 82.3%. While there is a positive gap on\n",
      "this task, the gap was smaller than we were were willing to tolerate. Similarly, on our recast version\n",
      "of the Ultraﬁne Entity Typing, we observe too small a gap between human (60.2 F1) and machine\n",
      "performance (55.0 F1). Our recasting for this task is described in Appendix C.2. On GAP, when\n",
      "taken as a classiﬁcation problem without the related task of span selection (details in C.2), BERT\n",
      "performs (91.0 F1) comparably to our human baseline (94.9 F1). Given this small margin, we also\n",
      "exclude GAP.\n",
      "On Discovering Ongoing Conversations, our BERT baseline achieves an F1 of 51.9 on a version of\n",
      "the task cast as sentence pair classiﬁcation (given two snippets of texts from plays, determine if the\n",
      "second snippet is a continuation of the ﬁrst). This dataset is very class imbalanced (90% negative), so\n",
      "we also experimented with a class-balanced version on which our BERT baselines achieves 88.4\n",
      "F1. Qualitatively, we also found the task challenging for humans as there was little context for the\n",
      "text snippets and the examples were drawn from plays using early English. Given this fairly high\n",
      "machine performance and challenging nature for humans, we exclude this task from our benchmark.\n",
      "Instructions tables begin on the following page.\n",
      "13https://www.kaggle.com/c/quora-insincere-questions-classification/data\n",
      "19\n",
      "Table 5: The instructions given to crowd-sourced worker describing the training phase for the Choice\n",
      "of Plausible Answers (COPA) task.\n",
      "The New York University Center for Data Science is collecting your answers for use in research\n",
      "on computer understanding of English. Thank you for your help!\n",
      "This project is a training task that needs to be completed before working on the main project\n",
      "on AMT named Human Performance: Plausible Answer. Once you are done with the training,\n",
      "please proceed to the main task! The qualiﬁcation approval is not immediate but we will add\n",
      "you to our qualiﬁed workers list within a day.\n",
      "In this training, you must answer the question on the page and then, to see how you did, click\n",
      "the Check Work button at the bottom of the page before hitting Submit. The Check Work\n",
      "button will reveal the true label. Please use this training and the provided answers to build\n",
      "an understanding of what the answers to these questions look like (the main project, Human\n",
      "Performance: Plausible Answer, does not have the answers on the page).\n",
      "Table 6: Task-speciﬁc instructions for Choice of Plausible Alternatives (COPA). These instructions\n",
      "were provided during both training and annotation phases.\n",
      "Plausible Answer Instructions\n",
      "The New York University Center for Data Science is collecting your answers for use in research\n",
      "on computer understanding of English. Thank you for your help!\n",
      "We will present you with a prompt sentence and a question. The question will either be about\n",
      "what caused the situation described in the prompt, or what a possible effect of that situation is.\n",
      "We will also give you two possible answers to this question. Your job is to decide, given the\n",
      "situation described in the prompt, which of the two options is a more plausible answer to the\n",
      "question:\n",
      "In the following example, option 1. is a more plausible answer to the question about what caused\n",
      "the situation described in the prompt,\n",
      "The girl received a trophy.\n",
      "What’s the CAUSE for this?\n",
      "1. She won a spelling bee.\n",
      "2. She made a new friend.\n",
      "In the following example, option2. is a more plausible answer the question about what happened\n",
      "because of the situation described in the prompt,\n",
      "The police aimed their weapons at the fugitive.\n",
      "What happened as a RESULT?\n",
      "1. The fugitive fell to the ground.\n",
      "2. The fugitive dropped his gun.\n",
      "If you have any more questions, please refer to our FAQpage.\n",
      "20\n",
      "Table 7: Task-speciﬁc instructions for Commitment Bank. These instructions were provided during\n",
      "both training and annotation phases.\n",
      "Speaker Commitment Instructions\n",
      "The New York University Center for Data Science is collecting your answers for use in research\n",
      "on computer understanding of English. Thank you for your help!\n",
      "We will present you with a prompt taken from a piece of dialogue, this could be a single sentence,\n",
      "a few sentences, or a short exchange between people. Your job is to ﬁgure out, based on this\n",
      "ﬁrst prompt (on top), how certain the speaker is about the truthfulness of the second prompt\n",
      "(on the bottom). You can choose from a 7 point scale ranging from (1) completely certain that\n",
      "the second prompt is true to (7) completely certain that the second prompt is false. Here are\n",
      "examples for a few of the labels:\n",
      "Choose 1 (certain that it is true) if the speaker from the ﬁrst prompt deﬁnitely believes or knows\n",
      "that the second prompt is true. For example,\n",
      "\"What fun to hear Artemis laugh. She’s such a serious child. I didn’t know\n",
      "she had a sense of humor.\"\n",
      "\"Artemis had a sense of humor\"\n",
      "Choose 4 (not certain if it is true or false) if the speaker from the ﬁrst prompt is uncertain if the\n",
      "second prompt is true or false. For example,\n",
      "\"Tess is committed to track. She’s always trained with all her heart and soul.\n",
      "One can only hope that she has recovered from the ﬂu and will cross the ﬁnish\n",
      "line.\"\n",
      "\"Tess crossed the ﬁnish line.\"\n",
      "Choose 7 (certain that it is false) if the speaker from the ﬁrst prompt deﬁnitely believes or knows\n",
      "that the second prompt is false. For example,\n",
      "\"Did you hear about Olivia’s chemistry test? She studied really hard. But\n",
      "even after putting in all that time and energy, she didn’t manage to pass the\n",
      "test\".\n",
      "\"Olivia passed the test.\"\n",
      "If you have any more questions, please refer to our FAQpage.\n",
      "21\n",
      "Table 8: Task-speciﬁc instructions for Winograd Schema Challenge (WSC). These instructions were\n",
      "provided during both training and annotation phases.\n",
      "Winograd Schema Instructions\n",
      "The New York University Center for Data Science is collecting your answers for use in research\n",
      "on computer understanding of English. Thank you for your help!\n",
      "We will present you with a sentence that someone wrote, with one bolded pronoun. We will then\n",
      "ask if you if the pronoun refers to a speciﬁc word or phrase in the sentence. Your job is to ﬁgure\n",
      "out, based on the sentence, if the bolded pronoun refers to this selected word or phrase:\n",
      "Choose Yes if the pronoun refers to the selected word or phrase. For example,\n",
      "\"I put the cake away in the refrigerator. It has a lot of butter in it.\"\n",
      "Does It in \"It has a lot\" refer to cake?\n",
      "Choose No if the pronoun does not refer to the selected word or phrase. For example,\n",
      "\"The large ball crashed right through the table because it was made of\n",
      "styrofoam.\"\n",
      "Does it in \"it was made\" refer to ball?\n",
      "If you have any more questions, please refer to our FAQpage.\n",
      "22\n",
      "Table 9: Task-speciﬁc instructions for BoolQ (continued in Table 10). These instructions were\n",
      "provided during both training and annotation phases.\n",
      "Question-Answering Instructions\n",
      "The New York University Center for Data Science is collecting your answers for use in research\n",
      "on computer understanding of English. Thank you for your help!\n",
      "We will present you with a passage taken from a Wikipedia article and a relevant question. Your\n",
      "job is to decide, given the information provided in the passage, if the answer to the question is\n",
      "Yes or No. For example,\n",
      "In the following examples the correct answer is Yes,\n",
      "The thirteenth season of Criminal Minds was ordered on April 7, 2017, by\n",
      "CBS with an order of 22 episodes. The season premiered on September 27,\n",
      "2017 in a new time slot at 10:00PM on Wednesday when it had previously\n",
      "been at 9:00PM on Wednesday since its inception. The season concluded on\n",
      "April 18, 2018 with a two-part season ﬁnale.\n",
      "will there be a 13th season of criminal minds?\n",
      "(In the above example, the ﬁrst line of the passage says that the 13th season of\n",
      "the show was ordered.)\n",
      "As of 8 August 2016, the FDA extended its regulatory power to include e-\n",
      "cigarettes. Under this ruling the FDA will evaluate certain issues, including\n",
      "ingredients, product features and health risks, as well their appeal to minors\n",
      "and non-users. The FDA rule also bans access to minors. A photo ID is\n",
      "required to buy e-cigarettes, and their sale in all-ages vending machines is not\n",
      "permitted. The FDA in September 2016 has sent warning letters for unlawful\n",
      "underage sales to online retailers and retailers of e-cigarettes.\n",
      "is vaping illegal if you are under 18?\n",
      "(In the above example, the passage states that the \"FDA rule also bans access\n",
      "to minors.\" The question uses the word \"vaping,\" which is a synonym for\n",
      "e-cigrattes.)\n",
      "In the following examples the correct answer is No,\n",
      "Badgers are short-legged omnivores in the family Mustelidae, which also\n",
      "includes the otters, polecats, weasels, and wolverines. They belong to the\n",
      "caniform suborder of carnivoran mammals. The 11 species of badgers are\n",
      "grouped in three subfamilies: Melinae (Eurasian badgers), Mellivorinae (the\n",
      "honey badger or ratel), and Taxideinae (the American badger). The Asiatic\n",
      "stink badgers of the genus Mydaus were formerly included within Melinae\n",
      "(and thus Mustelidae), but recent genetic evidence indicates these are actually\n",
      "members of the skunk family, placing them in the taxonomic family Mephitidae.\n",
      "is a wolverine the same as a badger?\n",
      "(In the above example, the passage says that badgers and wolverines are in\n",
      "the same family, Mustelidae, which does not mean they are the same animal.)\n",
      "23\n",
      "Table 10: Continuation from Table 9 of task-speciﬁc instructions for BoolQ. These instructions were\n",
      "provided during both training and annotation phases.\n",
      "More famously, Harley-Davidson attempted to register as a trademark the\n",
      "distinctive “chug” of a Harley-Davidson motorcycle engine. On February\n",
      "1, 1994, the company ﬁled its application with the following description:\n",
      "“The mark consists of the exhaust sound of applicant’s motorcycles, produced\n",
      "by V-twin, common crankpin motorcycle engines when the goods are in use. ”\n",
      "Nine of Harley-Davidson’s competitors ﬁled oppositions against the applica-\n",
      "tion, arguing that cruiser-style motorcycles of various brands use the same\n",
      "crankpin V-twin engine which produces the same sound. After six years of\n",
      "litigation, with no end in sight, in early 2000, Harley-Davidson withdrew their\n",
      "application.\n",
      "does harley davidson have a patent on their sound?\n",
      "(In the above example, the passage states that Harley-Davidson applied for a\n",
      "patent but then withdrew, so they do not have a patent on the sound.)\n",
      "If you have any more questions, please refer to our FAQpage.\n",
      "24\n",
      "Table 11: Task-speciﬁc instructions for the diagnostic and the bias diagnostic datasets. These\n",
      "instructions were provided during both training and annotation phases.\n",
      "Textual Entailment Instructions\n",
      "The New York University Center for Data Science is collecting your answers for use in research\n",
      "on computer understanding of English. Thank you for your help!\n",
      "We will present you with a prompt taken from an article someone wrote. Your job is to ﬁgure out,\n",
      "based on this correct prompt (the ﬁrst prompt, on top), if another prompt (the second prompt, on\n",
      "bottom) is also necessarily true:\n",
      "Choose True if the event or situation described by the ﬁrst prompt deﬁnitely implies that the\n",
      "second prompt, on bottom, must also be true. For example,\n",
      "• \"Murphy recently decided to move to London.\"\n",
      "\"Murphy recently decided to move to England.\"\n",
      "(The above example is True because London is in England and therefore prompt 2 is\n",
      "clearly implied by prompt 1.)\n",
      "• \"Russian cosmonaut Valery Polyakov set the record for the longest continuous amount\n",
      "of time spent in space, a staggering 438 days, between 1994 and 1995.\"\n",
      "\"Russians hold record for longest stay in space.\"\n",
      "(The above example is True because the information in the second prompt is contained\n",
      "in the ﬁrst prompt: Valery is Russian and she set the record for longest stay in space.)\n",
      "• \"She does not disgree with her brother’s opinion, but she believes he’s too aggresive in\n",
      "his defense\"\n",
      "\"She agrees with her brother’s opinion, but she believes he’s too aggresive in his\n",
      "defense\"\n",
      "(The above example is True because the second prompt is an exact paraphrase of the\n",
      "ﬁrst prompt, with exactly the same meaning.)\n",
      "Choose False if the event or situation described with the ﬁrst prompt on top does not necessarily\n",
      "imply that this second prompt must also be true. For example,\n",
      "• \"This method was developed at Columbia and applied to data processing at CERN.\"\n",
      "\"This method was developed at Columbia and applied to data processing at CERN\n",
      "with limited success.\"\n",
      "(The above example is False because the second prompt is introducing new information\n",
      "not implied in the ﬁrst prompt: The ﬁrst prompt does not give us any knowledge of\n",
      "how succesful the application of the method at CERN was.)\n",
      "• \"This building is very tall.\"\n",
      "\"This is the tallest building in New York.\"\n",
      "(The above example is False because a building being tall does not mean it must be the\n",
      "tallest building, nor that it is in New York.)\n",
      "• \"Hours earlier, Yasser Arafat called for an end to attacks against Israeli civilians in\n",
      "the two weeks before Israeli elections.\"\n",
      "\"Arafat condemned suicide bomb attacks inside Israel.\"\n",
      "(The above example is False because from the ﬁrst prompt we only know that Arafat\n",
      "called for an end to attacks against Israeli citizens, we do not know what kind of attacks\n",
      "he may have been condemning.)\n",
      "You do not have to worry about whether the writing style is maintained between the two prompts.\n",
      "If you have any more questions, please refer to our FAQpage.\n",
      "25\n",
      "Table 12: Task-speciﬁc instructions for the Gendered Ambiguous Pronoun Coreference (GAP) task.\n",
      "These instructions were provided during both training and annotation phases.\n",
      "GAP Instructions\n",
      "The New York University Center for Data Science is collecting your answers for use in research\n",
      "on computer understanding of English. Thank you for your help!\n",
      "We will present you with an extract from a Wikipedia article, with one bolded pronoun. We will\n",
      "also give you two names from the text that this pronoun could refer to. Your job is to ﬁgure out,\n",
      "based on the extract, if the pronoun refers to option A, options B, or neither:\n",
      "Choose A if the pronoun refers to option A. For example,\n",
      "\"In 2010 Ella Kabambe was not the ofﬁcial Miss Malawi; this was Faith\n",
      "Chibale, but Kabambe represented the country in the Miss World pageant.\n",
      "At the 2012 Miss World, Susan Mtegha pushed Miss New Zealand, Collette\n",
      "Lochore, during the opening headshot of the pageant, claiming that Miss New\n",
      "Zealand was in her space.\"\n",
      "Does her refer to option A or B below?\n",
      "A Susan Mtegha\n",
      "B Collette Lochore\n",
      "C Neither\n",
      "Choose B if the pronoun refers to option B. For example,\n",
      "\"In 1650 he started his career as advisor in the ministerium of ﬁnances in Den\n",
      "Haag. After he became a minister he went back to Amsterdam, and took place\n",
      "as a sort of chairing mayor of this city. After the death of his brother Cornelis,\n",
      "De Graeff became the strong leader of the republicans. He held this position\n",
      "until the rampjaar.\"\n",
      "Does He refer to option A or B below?\n",
      "A Cornelis\n",
      "B De Graeff\n",
      "C Neither\n",
      "Choose C if the pronoun refers to neither option. For example,\n",
      "\"Reb Chaim Yaakov’s wife is the sister of Rabbi Moishe Sternbuch, as is\n",
      "the wife of Rabbi Meshulam Dovid Soloveitchik, making the two Rabbis his\n",
      "uncles. Reb Asher’s brother Rabbi Shlomo Arieli is the author of a critical\n",
      "edition of the novallae of Rabbi Akiva Eiger. Before his marriage, Rabbi Arieli\n",
      "studied in the Ponevezh Yeshiva headed by Rabbi Shmuel Rozovsky, and he\n",
      "later studied under his father-in-law in the Mirrer Yeshiva.\"\n",
      "Does his refer to option A or B below?\n",
      "A Reb Asher\n",
      "B Akiva Eiger\n",
      "C Neither\n",
      "If you have any more questions, please refer to our FAQpage.\n",
      "26\n",
      "Table 13: Task-speciﬁc instructions for the Paraphrase Adversaries from Word Scrambling (PAWS)\n",
      "task. These instructions were provided during both training and annotation phases.\n",
      "Paraphrase Detection Instructions\n",
      "The New York University Center for Data Science is collecting your answers for use in research\n",
      "on computer understanding of English. Thank you for your help!\n",
      "We will present you with two similar sentences taken from Wikipedia articles. Your job is to\n",
      "ﬁgure out if these two sentences are paraphrases of each other, and convey exactly the same\n",
      "meaning:\n",
      "Choose Yes if the sentences are paraphrases and have the exact same meaning. For example,\n",
      "\"Hastings Ndlovu was buried with Hector Pieterson at Avalon Cemetery in\n",
      "Johannesburg.\"\n",
      "\"Hastings Ndlovu , together with Hector Pieterson , was buried at the Avalon\n",
      "cemetery in Johannesburg .\"\n",
      "\"The complex of the Trabzon World Trade Center is close to Trabzon Airport\n",
      ".\"\n",
      "\"The complex of World Trade Center Trabzon is situated close to Trabzon\n",
      "Airport .\"\n",
      "Choose No if the two sentences are not exact paraphrases and mean different things. For\n",
      "example,\n",
      "\"She was only a few months in French service when she met some British\n",
      "frigates in 1809 .\"\n",
      "\"She was only in British service for a few months , when in 1809 , she\n",
      "encountered some French frigates .\"\n",
      "\"This work caused him to trigger important reﬂections on the practices of\n",
      "molecular genetics and genomics at a time when this was not considered\n",
      "ethical .\"\n",
      "\"This work led him to trigger ethical reﬂections on the practices of molecular\n",
      "genetics and genomics at a time when this was not considered important .\"\n",
      "If you have any more questions, please refer to our FAQpage.\n",
      "27\n",
      "Table 14: Task-speciﬁc instructions for the Quora Insincere Questions task. These instructions were\n",
      "provided during both training and annotation phases.\n",
      "Insincere Questions Instructions\n",
      "The New York University Center for Data Science is collecting your answers for use in research\n",
      "on computer understanding of English. Thank you for your help!\n",
      "We will present you with a question that someone posted on Quora. Your job is to ﬁgure out\n",
      "whether or not this is a sincere question. An insincere question is deﬁned as a question intended\n",
      "to make a statement rather than look for helpful answers. Some characteristics that can signify\n",
      "that a question is insincere:\n",
      "• Has a non-neutral tone\n",
      "– Has an exaggerated tone to underscore a point about a group of people\n",
      "– Is rhetorical and meant to imply a statement about a group of people\n",
      "• Is disparaging or inﬂammatory\n",
      "– Suggests a discriminatory idea against a protected class of people, or seeks\n",
      "conﬁrmation of a stereotype\n",
      "– Makes disparaging attacks/insults against a speciﬁc person or group of people\n",
      "– Based on an outlandish premise about a group of people\n",
      "– Disparages against a characteristic that is not ﬁxable and not measurable\n",
      "• Isn’t grounded in reality\n",
      "– Based on false information, or contains absurd assumptions\n",
      "– Uses sexual content (incest, bestiality, pedophilia) for shock value, and not to seek\n",
      "genuine answers\n",
      "Please note that there are far fewer insincere questions than there are sincere questions! So you\n",
      "should expect to label most questions as sincere.\n",
      "Examples,\n",
      "Choose Sincere if you believe the person asking the question was genuinely seeking an answer\n",
      "from the forum. For example,\n",
      "\"How do DNA and RNA compare and contrast?\"\n",
      "\"Are there any sports that you don’t like?\"\n",
      "\"What is the main purpose of penance?\"\n",
      "Choose Insincere if you believe the person asking the question was not really seeking an answer\n",
      "but was being inﬂammatory, extremely rhetorical, or absurd. For example,\n",
      "\"How do I sell Pakistan? I need lots of money so I decided to sell Pakistan\n",
      "any one wanna buy?\"\n",
      "\"If Hispanics are so proud of their countries, why do they move out?\"\n",
      "\"Why Chinese people are always not welcome in all countries?\"\n",
      "If you have any more questions, please refer to our FAQpage.\n",
      "28\n",
      "Table 15: Task-speciﬁc instructions for the Ultraﬁne Entity Typing task. These instructions were\n",
      "provided during both training and annotation phases.\n",
      "Entity Typing Instructions\n",
      "The New York University Center for Data Science is collecting your answers for use in research\n",
      "on computer understanding of English. Thank you for your help!\n",
      "We will provide you with a sentence with on bolded word or phrase. We will also give you a\n",
      "possible tag for this bolded word or phrase. Your job is to decide, in the context of the sentence,\n",
      "if this tag is correct and applicable to the bolded word or phrase:\n",
      "Choose Yes if the tag is applicable and accurately describes the selected word or phrase. For\n",
      "example,\n",
      "“Spain was the gold line.\" It started out with zero gold in 1937, and by 1945\n",
      "it had 65.5 tons.\n",
      "Tag: nation\n",
      "Choose No if the tag is not applicable and does not describes the selected word or phrase. For\n",
      "example,\n",
      "Iraqi museum workersare starting to assess the damage to Iraq’s history.\n",
      "Tag: organism\n",
      "If you have any more questions, please refer to our FAQpage.\n",
      "29\n",
      "Table 16: Task-speciﬁc instructions for the Empathetic Reaction task. These instructions were\n",
      "provided during both training and annotation phases.\n",
      "Empathy and Distress Analysis Instructions\n",
      "The New York University Center for Data Science is collecting your answers for use in research\n",
      "on computer understanding of English. Thank you for your help!\n",
      "We will present you with a message someone wrote after reading an article. Your job is to ﬁgure\n",
      "out, based on this message, how disressed and empathetic the author was feeling. Empathy is\n",
      "deﬁned as feeling warm, tender, sympathetic, moved, or compassionate. Distressed is deﬁned as\n",
      "feeling worried, upset, troubled, perturbed, grieved, distrubed, or alarmed.\n",
      "Examples,\n",
      "The author of the following message was not feeling empathetic at all with anempathy score of 1,\n",
      "and was very distressed with a distress score of 7,\n",
      "\"I really hate ISIS. They continue to be the stain on society by committing\n",
      "atrocities condemned by every nation in the world. They must be stopped at\n",
      "all costs and they must be destroyed so that they wont hurt another soul. These\n",
      "poor people who are trying to survive get killed, imprisoned, or brainwashed\n",
      "into joining and there seems to be no way to stop them.\"\n",
      "The author of the following message is feeling very empathetic with an empathy score of 7 and\n",
      "also very distressed with a distress score of 7,\n",
      "\"All of you know that I love birds. This article was hard for me to read because\n",
      "of that. Wind turbines are killing a lot of birds, including eagles. It’s really\n",
      "very sad. It makes me feel awful. I am all for wind turbines and renewable\n",
      "sources of energy because of global warming and coal, but this is awful. I\n",
      "don’t want these poor birds to die like this. Read this article and you’ll see\n",
      "why.\"\n",
      "The author of the following message is feeling moderately empathetic with an\n",
      "empathy score of 4 and moderately distressed with a distress score of 4,\n",
      "\"I just read an article about wild ﬁres sending a smokey haze across the state\n",
      "near the Appalachian mountains. Can you imagine how big the ﬁre must be\n",
      "to spread so far and wide? And the people in the area obviously suffer the\n",
      "most. What if you have asthma or some other condition that restricts your\n",
      "breathing?\"\n",
      "The author of the following message is feeling very empathetic with an empathy score of 7 and\n",
      "mildly distressed with a distress score of 2,\n",
      "\"This is a very sad article. Being of of the ﬁrst female ﬁghter pilots must\n",
      "have given her and her family great honor. I think that there should be more\n",
      "training for all pilots who deal in these acrobatic ﬂying routines. I also think\n",
      "that women have just as much of a right to become a ﬁghter pilot as men.\"\n",
      "If you have any more questions, please refer to our FAQpage.\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "for data in lec_data:\n",
    "    print(data[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48706d22-6719-44b0-b814-7623dd00bb72",
   "metadata": {},
   "source": [
    "### Opening Anoop's lecture PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84f0bc2-1f44-49ac-813c-f0b48f1ed4b8",
   "metadata": {},
   "source": [
    "PyMuPDF's get_text() function does decently well\n",
    "<ul>\n",
    "    <li>Extracts meaningful text</li> \n",
    "    <li>Sigma for summation often replaced with X</li>\n",
    "    <li>Equations and formula often split across multiple lines</li>\n",
    "</ul>\n",
    "\n",
    "But we found from our experiments that PyPDF does remarkably better\n",
    "<ul>\n",
    "    <li>Effectively recognizes most mathematical symbols</li>\n",
    "    <li>Better at retaining new line information</li>\n",
    "</ul>\n",
    "\n",
    "We choose to go with PyPDF's extract_text() function.\n",
    "\n",
    "For metadata, we provide the file name and page number to refer back to quickly to the source of the knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9c23ff8-d6a8-4bea-a753-2632d022045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lec_doc_path = \"../data/input/full/lectures/ff.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6fd4a78-713b-4142-a294-92cc72331c10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lec_data = read_text_with_metadata_from_pdfs(lec_doc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "332f3d24-3b56-4dd1-a90d-fd228ae2a6ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 1,\n",
       "  'text': '0\\nSFU NatLangLab\\nNatural Language Processing\\nAnoop Sarkar\\nanoopsarkar.github.io/nlp-class\\nSimon Fraser University\\nSeptember 27, 2024'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 2,\n",
       "  'text': '1\\nNatural Language Processing\\nAnoop Sarkar\\nanoopsarkar.github.io/nlp-class\\nSimon Fraser University\\nPart 1: Feedforward neural networks'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 3,\n",
       "  'text': '2\\nLog-linear models versus Neural networks\\nFeedforward neural networks\\nStochastic Gradient Descent\\nMotivating example: XOR\\nComputation Graphs'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 4,\n",
       "  'text': '3\\nLog linear model\\n▶ Let there be m features, fk (x, y) for k = 1, . . . ,m\\n▶ Define a parameter vector v ∈ Rm\\n▶ A log-linear model for classification into labels y ∈ Y:\\nPr(y | x; v) = exp (v · f(x, y)))P\\ny′∈Y exp (v · f(x, y′)))\\nAdvantages\\nThe feature representation f(x, y) can represent any aspect of the\\ninput that is useful for classification.\\nDisadvantages\\nThe feature representation f(x, y) has to be designed by hand\\nwhich is time-consuming and error-prone.'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 5,\n",
       "  'text': '4\\nLog linear model\\nFigure from [1]\\nDisadvantages: number of combined features can explode\\n'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 6,\n",
       "  'text': '5\\nNeural Networks\\nAdvantages\\n▶ Neural networks replace hand-engineered features with\\nrepresentation learning\\n▶ Empirical results across many different domains show that\\nlearned representations give significant improvements in\\naccuracy\\n▶ Neural networks allow end to end training for complex NLP\\ntasks and do not have the limitations of multiple chained\\npipeline models\\nDisadvantages\\nFor many tasks linear models are much faster to train compared to\\nneural network models'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 7,\n",
       "  'text': '6\\nAlternative Form of Log linear model\\nLog-linear model:\\nPr(y | x; v) = exp (v · f(x, y)))P\\ny′∈Y exp (v · f(x, y′)))\\nAlternative form using functions:\\nPr(y | x; v) = exp (v(y) · f (x) + γy )P\\ny′∈Y exp\\n\\x00\\nv(y′) · f (x) + γy′)\\n\\x01\\n▶ Feature vector f (x) maps input x to Rd\\n▶ Parameters v(y) ∈ Rd and γy ∈ R for each y ∈ Y\\n▶ We assume v(y) · f (x) is a dot product. Using matrix\\nmultiplication it would be v(y) · f (x)T\\n▶ Let v = {(v(y), γy ) : y ∈ Y}'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 8,\n",
       "  'text': '7\\nLog-linear models versus Neural networks\\nFeedforward neural networks\\nStochastic Gradient Descent\\nMotivating example: XOR\\nComputation Graphs'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 9,\n",
       "  'text': '8\\nRepresentation Learning: Feedforward Neural Network\\nReplace hand-engineered features f with learned features ϕ:\\nPr(y | x; θ, v) = exp (v(y) · ϕ(x; θ) + γy )P\\ny′∈Y exp\\n\\x00\\nv(y′) · ϕ(x; θ) + γy′)\\n\\x01\\n▶ Replace f (x) with ϕ(x; θ) ∈ Rd where θ are new parameters\\n▶ Parameters θ are learned from training data\\n▶ Using θ the model ϕ maps input x to Rd : a learned\\nrepresentation from x\\n▶ x ∈ Rd is a pre-trained vector of size d\\n▶ We will use feedforward neural networks to define ϕ(x; θ)\\n▶ ϕ(x; θ) will be a non-linear mapping to Rd\\n▶ ϕ replaces f which was a linear model'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 10,\n",
       "  'text': '9\\nA Single Neuron aka Perceptron\\nA single neuron maps input x ∈ Rd to output h:\\nh = g(w · x + b)\\n▶ Weight vector w ∈ Rd , a bias b ∈ R are the parameters of the\\nmodel learned from training data\\n▶ Transfer function(also called activation function)\\ng : R → R\\n▶ It is important that g is a non-linear transfer function\\n▶ Linear g(z) = α · z + β for constants α, β(linear perceptron)'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 11,\n",
       "  'text': '10\\nActivation Functions and their Gradients\\nfrom [2], Fig. 4.3\\n'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 12,\n",
       "  'text': '11\\nThe sigmoid Transfer Function: σ\\nsigmoid transfer function:\\ng(z) = 1\\n1 − exp(z)\\nDerivative of sigmoid:\\ndg(z)\\ndz = g(z)(1 − g(z))'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 13,\n",
       "  'text': '12\\nThe tanh Transfer Function\\ntanh transfer function:\\ng(z) = exp(2z) − 1\\nexp(2z) + 1\\nDerivative of tanh:\\ndg(z)\\ndz = 1 − g(z)2'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 14,\n",
       "  'text': '13\\nAlternatives to tanh\\nhardtanh:\\ng(z) =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n1 if z > 1\\n−1 if z < −1\\nz otherwise\\ndg(z)\\ndz =\\n\\x1a 1 if −1 ≤ z ≤ 1\\n0 otherwise\\nsoftsign:\\ng(z) = z\\n1 + |z|\\ndg(z)\\ndz =\\n( 1\\n(1+z)2 if z ≥ 0\\n−1\\n(1+z)2 if z < 0'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 15,\n",
       "  'text': '14\\nThe ReLU Transfer Function\\nRectified Linear Unit (ReLU):\\ng(z) = {z if z ≥ 0 or 0 if z < 0}\\nor equivalently g(z) = max{0, z}\\nDerivative of ReLU:\\ndg(z)\\ndz = {1 if z > 0 or 0 if z < 0}\\nnon-differentiable or undefined if z = 0\\n(in practice: choose a value for z = 0)'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 16,\n",
       "  'text': '15\\nThe GeLU Transfer Function\\nGaussian Error Linear Unit (GELU):\\ng(z) = {z\\n2(1 + (\\nr\\n2\\nπ × (z + 0.044715 × z3)))}\\nor\\ng(z) = {z\\n2(1 + tanh(\\nr\\n2\\nπ × (z + 0.044715 × z3)))}\\nTransfer function of choice for Transformer language models.\\n'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 17,\n",
       "  'text': '16\\nDesperately Seeking Transfer Functions\\nfrom [3]\\nEnumeration of non-linear functions\\n'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 18,\n",
       "  'text': '17\\nDesperately Seeking Transfer Functions\\nfrom [3]\\nEnumeration of non-linear functions\\n'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 19,\n",
       "  'text': '18\\nThe Swish Transfer Function [3]\\nEnumeration of activation functions:\\nSwish was the end result of comparing all the auto-generated\\nactivation functions for accuracy on standard datasets.\\nSwish uses the sigmoid σ:\\ng(z) = z · σ(βz)\\n▶ If β = 0 then g(z) = z\\n2 (a linear function; so avoid this)\\n▶ If β → ∞then g(z) = ReLu\\nDerivative of Swish:\\ndg(z)\\ndz = βg(z) + σ(βz)(1 − βg(z))'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 20,\n",
       "  'text': '19\\nThe Swish Transfer Function [3]\\nSwish transfer function with\\ndifferent values of β\\nFirst derivative of the Swish\\ntransfer function'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 21,\n",
       "  'text': '20\\nDerivatives w.r.t. parameters\\nDerivatives w.r.t. w:\\nGiven\\nh = g(w · x + b)\\nderivatives w.r.t. w1, . . . ,wj , . . .wd :\\ndh\\ndwj\\nDerivatives w.r.t. b:\\nderivatives w.r.t. b: dh\\ndb'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 22,\n",
       "  'text': '21\\nChain Rule of Differentiation\\nIntroduce an intermediate variable z ∈ R\\nz = w · x + b\\nh = g(z)\\nThen by the chain rule to differentiate w.r.t. w:\\ndh\\ndwj\\n= dh\\ndz\\ndz\\ndwj\\n= dg(z)\\ndz × xj\\nAnd similarly for b:\\ndh\\ndb = dh\\ndz\\ndz\\ndb = dg(z)\\ndz × 1'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 23,\n",
       "  'text': '22\\nSingle Layer Feedforward model\\nA single layer feedforward model consists of:\\n▶ An integer d specifying the input dimension. Each input to\\nthe network is x ∈ Rd\\n▶ Think of it as a d dimensional word embedding\\n▶ An integer m specifying the number of hidden units\\n▶ A parameter matrix W ∈ Rm×d . The vector Wk ∈ Rd for\\n1 ≤ k ≤ m is the kth row of W\\n▶ A vector b ∈ Rd of bias parameters\\n▶ A transfer function g : R → R\\ng(z) = ReLU(z) or g(z) = tanh(z)'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 24,\n",
       "  'text': '23\\nSingle Layer Feedforward model (continued)\\nFor k = 1, . . . ,m:\\n▶ The input to the kth neuron is: zk = Wk · x + bk\\n▶ The output from the kth neuron is: hk = g(zk )\\n▶ Define vector ϕ(x; θ) ∈ Rm as: ϕ(x; θ) = hk\\n▶ θ = (W , b) where W ∈ Rm×d and b ∈ Rd\\n▶ Size of θ is m × (d + 1) parameters\\nSome intuition\\nThe neural network employs m hidden units, each with their own\\nparameters Wk and bk , and these neurons are used to construct a\\nhidden representation h ∈ Rm'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 25,\n",
       "  'text': '24\\nMatrix Form\\nWe can replace the operation:\\nzk = Wk · x + b for k = 1, . . . ,m\\nwith\\nz = Wx + b\\nwhere the dimensions are as follows (vector of size m equals a\\nmatrix of size m × 1):\\nz|{z}\\nm×1\\n= W|{z}\\nm×d\\nx|{z}\\nd×1| {z }\\nm×1\\n+ b|{z}\\nm×1'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 26,\n",
       "  'text': '25\\nSingle Layer Feedforward model (matrix form)\\nA single layer feedforward model consists of:\\n▶ An integer d specifying the input dimension. Each input to\\nthe network is x ∈ Rd\\n▶ An integer m specifying the number of hidden units\\n▶ A parameter matrix W ∈ Rm×d\\n▶ A vector b ∈ Rd of bias parameters\\n▶ A transfer function g : Rm → Rm\\ng(z) = [. . . ,ReLU(zi ), . . .] or\\ng(z) = [. . . ,tanh(zi ), . . .] or\\ng(z) = [. . . , σ(zi ), . . .] or\\nfor i = 1, . . . ,m'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 27,\n",
       "  'text': '26\\nSingle Layer Feedforward model (matrix form, continued)\\n▶ Vector of inputs to the hidden layer z ∈ Rm: z = Wx + b\\n▶ Vector of outputs from hidden layer h ∈ Rm: h = g(z)\\n▶ Define ϕ(x; θ) = h where θ = (W , b)\\n▶ Define softmaxy = exp(ry )P\\ny′ exp(ry′) for ry = v(y) · h + γy\\n▶ Let V = [. . . ,vy , . . .] for y ∈ Y. vy ∈ Rm so V ∈ R|Y|×m.\\n▶ Let Γ = [ . . . , γy , . . .] for y ∈ Y. Γ ∈ R|Y|.\\nPutting it all together:\\nr|{z}\\nvector of size|Y|\\n= softmax( V · ϕ(x; θ) + Γ| {z }\\nfor eachy ∈ Yan R value\\n)\\n| {z }\\nA vector of sizeRY that sums to 1'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 28,\n",
       "  'text': '27\\nFeedforward neural network\\n'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 29,\n",
       "  'text': '28\\nn-gram Feedforward neural network\\nfrom [5]\\n'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 30,\n",
       "  'text': '29\\nLog-linear models versus Neural networks\\nFeedforward neural networks\\nStochastic Gradient Descent\\nMotivating example: XOR\\nComputation Graphs'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 31,\n",
       "  'text': '30\\nSimple stochastic gradient descent\\nInputs:\\n▶ Training examples (xi , yi ) for i = 1, . . . ,n\\n▶ A feedforward representation ϕ(x; θ)\\n▶ Integer T specifying the number of updates\\n▶ A sequence of learning rates: η1, . . . , ηT where ηt ∈ [0, 1]\\n▶ One should experiment with learning rates: 0.001, 0.01, 0.1, 1\\n▶ Bottou (2012) suggests a learning rate ηt = η1\\n1+η1×λ×t where λ\\nis a hyperparameter that can be tuned experimentally\\nInitialization:\\nSet v = (v(y), γy ) for all y, and θ to random values'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 32,\n",
       "  'text': '31\\nGradient descent\\nAlgorithm:\\n▶ For t = 1, . . . ,T\\n▶ Select an integer i uniformly at random from {1, . . . ,n}\\n▶ Define L(θ, v) = −log P(yi | xi ; θ, v)\\n▶ For each parameter θj and vk (y) and γy (for each label y):\\nθj = θj − ηt × dL(θ, v)\\ndθj\\nvk (y) = vk (y) − ηt × dL(θ, v)\\ndvk (y)\\nγ(y) = γ(y) − ηt × dL(θ, v)\\ndγ(y)\\n▶ Output: parameters θ, v = (v(y), γy ) for all y'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 33,\n",
       "  'text': '32\\nLog-linear models versus Neural networks\\nFeedforward neural networks\\nStochastic Gradient Descent\\nMotivating example: XOR\\nComputation Graphs'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 34,\n",
       "  'text': '33\\nMotivating example: the XOR problem\\nFrom Deep Learning by Goodfellow, Bengio, Courville\\nWe will assume a training set where each label is in the set\\nY = {−1, +1}\\nThere are four training examples:\\nx1 = [0 , 0], y1 = −1\\nx2 = [0 , 1], y2 = +1\\nx3 = [1 , 0], y3 = +1\\nx4 = [1 , 1], y4 = −1'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 35,\n",
       "  'text': '34\\nMotivating example: the XOR problem\\n'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 36,\n",
       "  'text': '35\\nMotivating example: the XOR problem\\nTheorem\\nFor examples (xi , yi ) for i = 1, . . . ,4 as defined previously for the\\nfeedforward neural network:\\nPr(y | x; W , b, v) = exp (v(y) · g(Wx + b) + γy )P\\ny′∈Y exp\\n\\x00\\nv(y′) · g(Wx + b) + γy′)\\n\\x01\\nwhere x ∈ R2 (d = 2) and let m = 2 so W ∈ R2×2 and b ∈ R2\\nand g is a ReLU transfer function.\\nThen there are parameter settings v(−1), v(+1), γ−1, γ+1, W , b\\nsuch that\\np(yi | xi ; v) > 0.5 for i = 1, . . . ,4'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 37,\n",
       "  'text': '36\\nMotivating example: the XOR problem\\nProof Sketch\\nDefine W =\\n\\x141 1\\n1 1\\n\\x15\\nand b =\\n\\x14 0\\n−1\\n\\x15\\nThen for each input x\\ncalculate values of z = Wx + b and h = g(z):\\nx = [0, 0] ⇒ z = [0, −1] ⇒ h = [0, 0]\\nx = [1, 0] ⇒ z = [1, 0] ⇒ h = [1, 0]\\nx = [0, 1] ⇒ z = [1, 0] ⇒ h = [1, 0]\\nx = [1, 1] ⇒ z = [2, 1] ⇒ h = [2, 1]'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 38,\n",
       "  'text': '37\\nMotivating example: the XOR problem\\nProof Sketch (continued)\\np(+1 | x; v) = exp(v(+1) · h + γ+1)\\nexp(v(+1) · h + γ+1) + exp(v(−1) · h + γ−1)\\n= 1\\n1 + exp(−(u · h + γ))\\nTo satisfy P(yi | xi ; v) > 0.5 for i = 1, . . . ,4 we have to find\\nparameters u = v(+1) − v(−1) and γ = γ+1 − γ−1 such that:\\nu · [0, 0] + γ < 0\\nu · [1, 0] + γ > 0\\nu · [1, 0] + γ > 0\\nu · [2, 1] + γ < 0\\nu = [1, −2] and γ = −0.5 satisfies these constraints.'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 39,\n",
       "  'text': '38\\nSolving the XOR problem\\n'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 40,\n",
       "  'text': '39\\nLog-linear models versus Neural networks\\nFeedforward neural networks\\nStochastic Gradient Descent\\nMotivating example: XOR\\nComputation Graphs'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 41,\n",
       "  'text': '40\\nComplex neural networks\\nNeural network with a loss function\\nConsider a neural network trained using a squared-error loss. For\\nthe correct answer y∗ the output value y is compared using the\\nfunction (y∗ − y)2.\\nh′ = Wxhx + bh\\nh = tanh( h′)\\ny = why h + by\\nℓ = ( y∗ − y)2'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 42,\n",
       "  'text': '41\\nDerivative wrt loss\\nh′ = Wxhx +bh\\nh = tanh(h′)\\ny = whyh +by\\nℓ = ( y∗ −y)2\\nWe want to computedℓ\\ndby , dℓ\\ndwhy , dℓ\\ndbh , dℓ\\ndWxh\\ndℓ\\ndby\\n= dℓ\\ndy\\ndy\\ndby\\ndℓ\\ndwhy\\n= dℓ\\ndy\\ndy\\ndwhy\\ndℓ\\ndbh\\n= dℓ\\ndy\\ndy\\ndh\\ndh\\ndh′\\ndh′\\ndbh\\ndℓ\\ndWxh\\n= dℓ\\ndy\\ndy\\ndh\\ndh\\ndh′\\ndh′\\ndWxh'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 43,\n",
       "  'text': '42\\nComputation graphs and automatic differentiation\\nFigure from [1]\\n'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 44,\n",
       "  'text': '43\\nComputation graphs and automatic differentiation\\n▶ Automatic differentiation is a two-step dynamic programming\\nalgorithm that operates over the second graph and performs:\\nForward calculation which traverses the nodes in the graph in\\ntopological order, calculating the actual result of\\nthe computation.\\nBack propagation which traverses the nodes in reverse\\ntopological order, calculating the gradients.\\n▶ Many neural network toolkits can perform auto differentiation\\nfor very large computation graphs.'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 45,\n",
       "  'text': '44\\n[1] Graham Neubig\\nNeural Networks for NLP\\n2018.\\n[2] Yoav Goldberg\\nNeural Network Methods for Natural Language Processing\\n2017.\\n[3] Prajit Ramachandran, Barret Zoph, Quoc V. Le\\nSearching for Activation Functions\\n2017.\\n[4] Xavier Glorot, Yoshua Bengio\\nUnderstanding the difficulty of training deep feedforward\\nneural networks\\n2010.\\n[5] Yoshua Bengio, R´ ejean Ducharme, Pascal Vincent, Christian\\nJauvin\\nA Neural Probabilistic Language Model\\n2003.'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/lectures/ff.pdf',\n",
       "  'marker': 46,\n",
       "  'text': '45\\nAcknowledgements\\nMany slides borrowed or inspired from lecture notes by Michael\\nCollins, Chris Dyer, Kevin Knight, Chris Manning, Philipp Koehn,\\nAdam Lopez, Graham Neubig, Richard Socher and Luke\\nZettlemoyer from their NLP course materials.\\nAll mistakes are my own.\\nA big thank you to all the students who read through these notes\\nand helped me improve them.'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lec_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7031f5f5-710d-46a3-8807-1990aa5c32b4",
   "metadata": {},
   "source": [
    "### Sample QAs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a372445-2700-42b4-a26a-97df2d02d028",
   "metadata": {},
   "source": [
    "For the QA files we found, it was easier to first convert it into a CSV with some preliminary information for each question, the question itself and the answer. We found from our tests that using PyPDF, we were unable to define a heuristic or a method that would copy over the preliminary information for each sub-question.\n",
    "\n",
    "For metadata, we only provide a random index since the source knowledge would not be super useful here, or at least, we were unable to find a use case for it since the question, preliminary information and all of the answers are already available in the parsed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6db23ffd-fea4-459b-9871-587f72735ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_with_metadata_from_qa_csvs(qa_doc_path):\n",
    "    qas = pd.read_csv(qa_doc_path, skiprows=1, names=[\"info\", \"question\", \"answer\"])\n",
    "    qas[\"info\"] = qas[\"info\"].fillna(\"No extra information given\")\n",
    "    data = []\n",
    "    for index, row in qas.iterrows():\n",
    "        text = f\"Info: {row['info']}\\nQuestion: {row['question']}\\nAnswer: {row['answer']}\"\n",
    "        data.append({\n",
    "            \"file_type\": \"csv\",\n",
    "            \"file_name\": \"test\",\n",
    "            \"marker\": index+1,\n",
    "            \"sub_marker\": index+1, # Not necessary since we are not chunking\n",
    "            \"text\": text,\n",
    "            \"first_10_words\": \" \".join(text.split()[:10])\n",
    "        })\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a614c69-5702-426b-849d-3858d575010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_doc_path = \"../data/input/full/qas/midterm-questions-3.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30b797a4-5cc2-48de-8154-41d00b1b8f71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qa_data = read_text_with_metadata_from_qa_csvs(qa_doc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "169ed156-8fe2-4dc9-9253-1fdd5af3c461",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file_type': 'csv',\n",
       "  'file_name': 'test',\n",
       "  'marker': 1,\n",
       "  'sub_marker': 1,\n",
       "  'text': 'Info: (1) You are given the following training data for the prepositional phrase (PP) attachment task. v        n1        p        n2        Attachment\\njoin        board        as        director        V\\nis        chairman        of        N.V.        N\\nusing        crocidolite        in        filters        V\\nWhere the attachment value of V indicates that p attaches to v and the attachment value of N indicates\\nthat p attaches to n1.\\nIn order to resolve PP attachment ambiguity we can train a probability model: P(A= N |v,n1,p,n2)\\nwhich predicts the attachment A as N if P >0.5 and V otherwise.\\nQuestion: (6pts) To define P(A= N |v,n1,p,n2) using n-gram probabilities, since we are unlikely to see the\\nsame four words v,n1,p,n2 in novel unseen data, in order for this probability model to be useful we\\nneed to take care of zero counts.\\nˆ\\nProvide a Jelinek-Mercer style interpolation smoothing model\\nP(A= N |v,n1,p,n2) for this PP\\nattachment probability model. Do not use recursive interpolation in your solution.\\nAssume that our training data is large enough to contain all the prepositions we might observe in\\nunseen data. You cannot assume that all the verbs and nouns in the unseen data were seen in training.\\nYour solution must have a 4-gram and at least two trigrams, at least two bigrams and at least one\\nunigram.\\nmust obey such that\\nIn the interpolation model if you use any new variables then provide the constraints that the variables\\nˆ\\nP continues to be a valid probability.\\nAnswer: P(A= N |v,n1,p,n2)= λ1P(A= N |v,n1,p,n2)\\n+λ2P(A= N |v,p,n2)\\n+λ3P(A= N |n1,p,n2)\\n+λ4P(A= N |v,p)\\n+λ5P(A= N |n1,p)\\n+λ6P(A= N |p)\\nTo be a well-formed interpolation model, i λi = 1. There are many other solutions such as for instance,\\nthe 3-gram model could have diﬀerent choices for the conditioning context, e.g. v,n1,p instead of v,p,n2 and\\nsimilarly for the bigrams. The solution must have at least one four-gram, two trigrams, two bigrams and one\\nunigram (the preposition).',\n",
       "  'first_10_words': 'Info: (1) You are given the following training data for'},\n",
       " {'file_type': 'csv',\n",
       "  'file_name': 'test',\n",
       "  'marker': 2,\n",
       "  'sub_marker': 2,\n",
       "  'text': 'Info: (1) You are given the following training data for the prepositional phrase (PP) attachment task. v        n1        p        n2        Attachment\\njoin        board        as        director        V\\nis        chairman        of        N.V.        N\\nusing        crocidolite        in        filters        V\\nWhere the attachment value of V indicates that p attaches to v and the attachment value of N indicates\\nthat p attaches to n1.\\nIn order to resolve PP attachment ambiguity we can train a probability model: P(A= N |v,n1,p,n2)\\nwhich predicts the attachment A as N if P >0.5 and V otherwise.\\nQuestion: (4pts) Assume you are given pre-trained word embeddings for each of the words in the prepositional\\nphrase dataset. The embeddings we get from the word appearing as the center or target word are:\\ntv,tn1,tp,tn2. The embeddings we get from the word appearing in the context are: cv,cn1,cp,cn2.\\nˆ\\nGiven a definition of\\nP(A= N |v,n1,p,n2) we can define a classifier that predicts noun attachment if\\nˆ\\nˆ\\nP(A= N |v,n1,p,n2) ≥0.5 and verb attachment if 1−\\nP(A= N |v,n1,p,n2) >0.5 otherwise.\\nˆ\\nYour task is to use only the dot products cn1·tp and cv·tp in order to provide\\nP(A= N |v,n1,p,n2).\\nThe goal is to use the pre-trained word vectors to provide the probability for noun/verb attachment.\\nIgnore the word embeddings for n2 (cn2 and tn2) for this question. Do not sum over the entire\\nvocabulary to obtain a probability distribution.\\nAnswer: P(A= N |v,n1,p,n2)= exp(cn1·tp)/\\nexp(cn1·tp) + exp(cv·tp)',\n",
       "  'first_10_words': 'Info: (1) You are given the following training data for'},\n",
       " {'file_type': 'csv',\n",
       "  'file_name': 'test',\n",
       "  'marker': 3,\n",
       "  'sub_marker': 3,\n",
       "  'text': 'Info: No extra information given\\nQuestion: (3pts) You are given a text of words: w1,...,wN and you proceed to estimate bigram probabilities with\\nthe maximum likelihood estimate using the bigram frequencies c(wi,wi−1) and unigram frequencies c(wi):\\nˆ\\nc(wi,wi−1)\\nP(wi |wi−1)=\\nc(wi−1)\\nWrite down the definition of a backoﬀ smoothed distribution Pbo(wi |wi−1) where we backoﬀ from the\\nbigram probability c(wi,wi−1) to the unigram P(wi). using a new function c∗(wi−1,wi) which uses the\\nabsolute discounting method and α(wi−1), where α(wi−1) is chosen to make sure that Pbo(wi | wi−1) is a\\nproper probability:\\nα(wi−1)= 1−\\nc∗(wi−1,wi)\\nc(wi−1)\\nwi\\nProvide the definition of Pbo(wi |wi−1) and c∗(wi−1,wi). Assume that 1=\\nwi P(wi).\\nAnswer: Pbo(wi | wi−1)=\\nc(wi−1,wi )−D\\nc(wi−1) if c(wi−1,wi) >0\\nα(wi−1)Pbo(wi) otherwise where D is set to some value less than one using held out set.\\n',\n",
       "  'first_10_words': 'Info: No extra information given Question: (3pts) You are given'},\n",
       " {'file_type': 'csv',\n",
       "  'file_name': 'test',\n",
       "  'marker': 4,\n",
       "  'sub_marker': 4,\n",
       "  'text': 'Info: No extra information given\\nQuestion: (2pts) Consider embedding a sentence by summing the GloVe embeddings of each word in the sentence.\\nPick which statement is true from the list below about the embeddings of the sentences we are winning\\nand we are winning we are winning we are winning (assume the words are in the vocabulary).\\n1. 2. The embeddings of the two sentences are equal.\\nThe embeddings of the two sentences are close when using Euclidean distance, but not cosine\\ndistance.\\n3. The embeddings of the two sentences are close when using cosine distance, but not Euclidean\\ndistance.\\n4. The embeddings of the two sentences are close when using cosine distance and Euclidean distance.\\n5. None of the above.\\nAnswer: The true statement is: The embeddings of the two sentences are close when using cosine distance, but\\nnot Euclidean distance.',\n",
       "  'first_10_words': 'Info: No extra information given Question: (2pts) Consider embedding a'},\n",
       " {'file_type': 'csv',\n",
       "  'file_name': 'test',\n",
       "  'marker': 5,\n",
       "  'sub_marker': 5,\n",
       "  'text': 'Info: We define a log-linear model that estimates a distribution Pr(s|w) where s is a sentiment label for a word,\\ns ∈{positive,negative}and w is a word from the vocabulary V. The set V contains the words amazing and\\nhorrible as well as additional words (so that |V|>2). We want our log-linear model to specify the\\nfollowing probabilities (some probabilities are left undefined):\\nPr(positive |amazing)= 0.9\\nPr(negative |horrible)= 0.9\\nPr(positive |w)= Pr(negative |w)= 0.6 for any word w other than amazing, horrible\\n0.4 for any word w other than amazing, horrible\\nThe value for probabilities Pr(negative |amazing),Pr(positive |horrible) are left unspecified. It is\\nassumed they are given values such that the following condition is satisfied for every w ∈V:\\nPr(s |w)= 1.0\\ns\\nYou are given exactly four features: f1(w,s),..., f4(w,s):\\nf1(w,s)= 1 if w is amazing and s is positive, 0 otherwise\\nf2(w,s)= 1 if w is horrible and s is negative, 0 otherwise\\nf3(w,s)= 1 if w {amazing,horrible}and s is positive, 0 otherwise\\nf4(w,s)= 1 if w {amazing,horrible}and s is negative, 0 otherwise\\nHint: For some input (w′\\n,s′) if for all k the value of fk(w′\\n,s′)= 0 then Pr(s′|w′)= e0\\nZ where Z is the\\nnormalization term.\\nQuestion: (4pts) For your feature vector {f1, f2, f3, f4}let the parameter vector be {v1,v2,v3,v4}. Write down the\\nexpressions for the following using the log linear model:\\n1. Pr(positive |awesome)\\n2. Pr(negative |amazing)\\nAnswer: 1. Pr(positive |awesome)=\\nev3/\\nev3 +ev4\\n2. Pr(negative |amazing)=\\ne0/\\nev1 +e0',\n",
       "  'first_10_words': 'Info: We define a log-linear model that estimates a distribution'},\n",
       " {'file_type': 'csv',\n",
       "  'file_name': 'test',\n",
       "  'marker': 6,\n",
       "  'sub_marker': 6,\n",
       "  'text': 'Info: No extra information given\\nQuestion: (6pts) Define the values of the parameters v1,v2,v3,v4 for the log-linear model that can model the\\ndistribution provided above perfectly.\\nAnswer: Pr(positive | amazing) = e^v1 / (e^v1 + e^0) = 0.9\\n\\nPr(negative | horrible) = e^v2 / (e^0 + e^v2) = 0.9\\n\\nPr(positive | w) = e^v3 / (e^v3 + e^v4) = 0.6 for any word w other than amazing, horrible\\n\\nPr(negative | w) = e^v4 / (e^v3 + e^v4) = 0.4 for any word w other than amazing, horrible\\nFrom the above we get the parameter values v1 = v2 = log(9) and v3 = log(6) and v4 = log(4). There are many\\nother equivalent values as long as they give the right probability in the table above.',\n",
       "  'first_10_words': 'Info: No extra information given Question: (6pts) Define the values'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2a9df7-e6fd-4e83-9c66-1475292b11a5",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeffad7-fa33-44a5-9dfd-0d200f91d4a3",
   "metadata": {},
   "source": [
    "#### PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6bb5e8-64e5-42d3-a01d-a3e8c0a6985e",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Struggles with multi-column layouts</li>\n",
    "    <li>Text often split across multiple lines</li>\n",
    "    <li>The other usual problems...</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7a4c887-ba6e-44fa-8c36-c83724897361",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_doc_path = \"../data/input/full/references/superglue.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "814db055-f481-4f00-a0f3-4802f6c70cf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ref_data = read_text_with_metadata_from_pdfs(ref_doc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eea8b293-cf14-4672-8bdf-653c985f01cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 1,\n",
       "  'text': 'SuperGLUE: A Stickier Benchmark for\\nGeneral-Purpose Language Understanding Systems\\nAlex Wang∗\\nNew York University\\nYada Pruksachatkun∗\\nNew York University\\nNikita Nangia∗\\nNew York University\\nAmanpreet Singh∗\\nFacebook AI Research\\nJulian Michael\\nUniversity of Washington\\nFelix Hill\\nDeepMind\\nOmer Levy\\nFacebook AI Research\\nSamuel R. Bowman\\nNew York University\\nAbstract\\nIn the last year, new models and methods for pretraining and transfer learning have\\ndriven striking performance improvements across a range of language understand-\\ning tasks. The GLUE benchmark, introduced a little over one year ago, offers\\na single-number metric that summarizes progress on a diverse set of such tasks,\\nbut performance on the benchmark has recently surpassed the level of non-expert\\nhumans, suggesting limited headroom for further research. In this paper we present\\nSuperGLUE, a new benchmark styled after GLUE with a new set of more difﬁ-\\ncult language understanding tasks, a software toolkit, and a public leaderboard.\\nSuperGLUE is available at super.gluebenchmark.com.\\n1 Introduction\\nIn the past year, there has been notable progress across many natural language processing (NLP)\\ntasks, led by methods such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018),\\nand BERT (Devlin et al., 2019). The common thread connecting these methods is that they couple\\nself-supervised learning from massive unlabelled text corpora with a recipe for effectively adapting\\nthe resulting model to target tasks. The tasks that have proven amenable to this general approach\\ninclude question answering, sentiment analysis, textual entailment, and parsing, among many others\\n(Devlin et al., 2019; Kitaev and Klein, 2018, i.a.).\\nIn this context, the GLUE benchmark (Wang et al., 2019a) has become a prominent evaluation\\nframework for research towards general-purpose language understanding technologies. GLUE is\\na collection of nine language understanding tasks built on existing public datasets, together with\\nprivate test data, an evaluation server, a single-number target metric, and an accompanying expert-\\nconstructed diagnostic set. GLUE was designed to provide a general-purpose evaluation of language\\nunderstanding that covers a range of training data volumes, task genres, and task formulations. We\\nbelieve it was these aspects that made GLUE particularly appropriate for exhibiting the transfer-\\nlearning potential of approaches like OpenAI GPT and BERT.\\nThe progress of the last twelve months has eroded headroom on the GLUE benchmark dramatically.\\nWhile some tasks (Figure 1) and some linguistic phenomena (Figure 2 in Appendix B) measured\\nin GLUE remain difﬁcult, the current state of the art GLUE Score as of early July 2019 (88.4 from\\nYang et al., 2019) surpasses human performance (87.1 from Nangia and Bowman, 2019) by 1.3\\n∗Equal contribution. Correspondence: glue-benchmark-admin@googlegroups.com\\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 2,\n",
       "  'text': 'BiLSTM+ELMo+Attn\\nOpenAI GPT\\nBERT + Single-task Adapters\\nBERT (Large)\\nBERT on STILTs\\nBERT + BAM\\nSemBERT\\nSnorkel MeTaL\\nALICE (Large)\\nMT-DNN (ensemble)\\nXLNet-Large (ensemble)0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0\\n1.1\\n1.2\\nGLUE Score\\nHuman Performance\\nCoLA\\nSST-2\\nMRPC\\nSTS-B\\nQQP\\nMNLI\\nQNLI\\nRTE\\nWNLI\\nFigure 1: GLUE benchmark performance for submitted systems, rescaled to set human performance\\nto 1.0, shown as a single number score, and broken down into the nine constituent task performances.\\nFor tasks with multiple metrics, we use an average of the metrics. More information on the tasks\\nincluded in GLUE can be found in Wang et al. (2019a) and in Warstadt et al. (2018, CoLA), Socher\\net al. (2013, SST-2), Dolan and Brockett (2005, MRPC), Cer et al. (2017, STS-B), and Williams et al.\\n(2018, MNLI), and Rajpurkar et al. (2016, the original data source for QNLI).\\npoints, and in fact exceeds this human performance estimate on four tasks. Consequently, while there\\nremains substantial scope for improvement towards GLUE’s high-level goals, the original version of\\nthe benchmark is no longer a suitable metric for quantifying such progress.\\nIn response, we introduce SuperGLUE, a new benchmark designed to pose a more rigorous test of\\nlanguage understanding. SuperGLUE has the same high-level motivation as GLUE: to provide a\\nsimple, hard-to-game measure of progress toward general-purpose language understanding technolo-\\ngies for English. We anticipate that signiﬁcant progress on SuperGLUE should require substantive\\ninnovations in a number of core areas of machine learning, including sample-efﬁcient, transfer,\\nmultitask, and unsupervised or self-supervised learning.\\nSuperGLUE follows the basic design of GLUE: It consists of a public leaderboard built around\\neight language understanding tasks, drawing on existing data, accompanied by a single-number\\nperformance metric, and an analysis toolkit. However, it improves upon GLUE in several ways:\\n• More challenging tasks: SuperGLUE retains the two hardest tasks in GLUE. The remain-\\ning tasks were identiﬁed from those submitted to an open call for task proposals and were\\nselected based on difﬁculty for current NLP approaches.\\n• More diverse task formats: The task formats in GLUE are limited to sentence- and\\nsentence-pair classiﬁcation. We expand the set of task formats in SuperGLUE to include\\ncoreference resolution and question answering (QA).\\n• Comprehensive human baselines: We include human performance estimates for all bench-\\nmark tasks, which verify that substantial headroom exists between a strong BERT-based\\nbaseline and human performance.\\n• Improved code support: SuperGLUE is distributed with a new, modular toolkit for work\\non pretraining, multi-task learning, and transfer learning in NLP, built around standard tools\\nincluding PyTorch (Paszke et al., 2017) and AllenNLP (Gardner et al., 2017).\\n• Reﬁned usage rules: The conditions for inclusion on the SuperGLUE leaderboard have\\nbeen revamped to ensure fair competition, an informative leaderboard, and full credit\\nassignment to data and task creators.\\nThe SuperGLUE leaderboard, data, and software tools are available at super.gluebenchmark.com.\\n2'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 3,\n",
       "  'text': '2 Related Work\\nMuch work prior to GLUE demonstrated that training neural models with large amounts of available\\nsupervision can produce representations that effectively transfer to a broad range of NLP tasks\\n(Collobert and Weston, 2008; Dai and Le, 2015; Kiros et al., 2015; Hill et al., 2016; Conneau and\\nKiela, 2018; McCann et al., 2017; Peters et al., 2018). GLUE was presented as a formal challenge\\naffording straightforward comparison between such task-agnostic transfer learning techniques. Other\\nsimilarly-motivated benchmarks include SentEval (Conneau and Kiela, 2018), which speciﬁcally\\nevaluates ﬁxed-size sentence embeddings, and DecaNLP (McCann et al., 2018), which recasts a set\\nof target tasks into a general question-answering format and prohibits task-speciﬁc parameters. In\\ncontrast, GLUE provides a lightweight classiﬁcation API and no restrictions on model architecture or\\nparameter sharing, which seems to have been well-suited to recent work in this area.\\nSince its release, GLUE has been used as a testbed and showcase by the developers of several\\ninﬂuential models, including GPT (Radford et al., 2018) and BERT (Devlin et al., 2019). As shown\\nin Figure 1, progress on GLUE since its release has been striking. On GLUE, GPT and BERT\\nachieved scores of 72.8 and 80.2 respectively, relative to 66.5 for an ELMo-based model (Peters\\net al., 2018) and 63.7 for the strongest baseline with no multitask learning or pretraining above the\\nword level. Recent models (Liu et al., 2019d; Yang et al., 2019) have clearly surpassed estimates of\\nnon-expert human performance on GLUE (Nangia and Bowman, 2019). The success of these models\\non GLUE has been driven by ever-increasing model capacity, compute power, and data quantity, as\\nwell as innovations in model expressivity (from recurrent to bidirectional recurrent to multi-headed\\ntransformer encoders) and degree of contextualization (from learning representation of words in\\nisolation to using uni-directional contexts and ultimately to leveraging bidirectional contexts).\\nIn parallel to work scaling up pretrained models, several studies have focused on complementary\\nmethods for augmenting performance of pretrained models. Phang et al. (2018) show that BERT can\\nbe improved using two-stage pretraining, i.e., ﬁne-tuning the pretrained model on an intermediate\\ndata-rich supervised task before ﬁne-tuning it again on a data-poor target task. Liu et al. (2019d,c) and\\nBach et al. (2018) get further improvements respectively via multi-task ﬁnetuning and using massive\\namounts of weak supervision. Anonymous (2018) demonstrate that knowledge distillation (Hinton\\net al., 2015; Furlanello et al., 2018) can lead to student networks that outperform their teachers.\\nOverall, the quantity and quality of research contributions aimed at the challenges posed by GLUE\\nunderline the utility of this style of benchmark for machine learning researchers looking to evaluate\\nnew application-agnostic methods on language understanding.\\nLimits to current approaches are also apparent via the GLUE suite. Performance on the GLUE\\ndiagnostic entailment dataset, at 0.42 R3, falls far below the average human performance of 0.80\\nR3 reported in the original GLUE publication, with models performing near, or even below, chance\\non some linguistic phenomena (Figure 2, Appendix B). While some initially difﬁcult categories\\nsaw gains from advances on GLUE (e.g., double negation), others remain hard (restrictivity) or\\neven adversarial (disjunction, downward monotonicity). This suggests that even as unsupervised\\npretraining produces ever-better statistical summaries of text, it remains difﬁcult to extract many\\ndetails crucial to semantics without the right kind of supervision. Much recent work has made similar\\nobservations about the limitations of existing pretrained models (Jia and Liang, 2017; Naik et al.,\\n2018; McCoy and Linzen, 2019; McCoy et al., 2019; Liu et al., 2019a,b).\\n3 SuperGLUE Overview\\n3.1 Design Process\\nThe goal of SuperGLUE is to provide a simple, robust evaluation metric of any method capable of\\nbeing applied to a broad range of language understanding tasks. To that end, in designing SuperGLUE,\\nwe identify the following desiderata of tasks in the benchmark:\\n• Task substance: Tasks should test a system’s ability to understand and reason about texts\\nwritten in English.\\n• Task difﬁculty: Tasks should be beyond the scope of current state-of-the-art systems,\\nbut solvable by most college-educated English speakers. We exclude tasks that require\\ndomain-speciﬁc knowledge, e.g. medical notes or scientiﬁc papers.\\n3'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 4,\n",
       "  'text': 'Table 1: The tasks included in SuperGLUE. WSD stands for word sense disambiguation, NLI is\\nnatural language inference, coref. is coreference resolution, and QA is question answering. For\\nMultiRC, we list the number of total answers for 456/83/166 train/dev/test questions. The metrics for\\nMultiRC are binary F1 on all answer-options and exact match.\\nCorpus |Train| | Dev| | Test| Task Metrics Text Sources\\nBoolQ 9427 3270 3245 QA acc. Google queries, Wikipedia\\nCB 250 57 250 NLI acc./F1 various\\nCOPA 400 100 500 QA acc. blogs, photography encyclopedia\\nMultiRC 5100 953 1800 QA F1 a/EM various\\nReCoRD 101k 10k 10k QA F1/EM news (CNN, Daily Mail)\\nRTE 2500 278 300 NLI acc. news, Wikipedia\\nWiC 6000 638 1400 WSD acc. WordNet, VerbNet, Wiktionary\\nWSC 554 104 146 coref. acc. ﬁction books\\n• Evaluability: Tasks must have an automatic performance metric that corresponds well to\\nhuman judgments of output quality. Certain text generation tasks fail to meet this criteria\\ndue to issues surrounding automatic metrics like ROUGE and BLEU (Callison-Burch et al.,\\n2006; Liu et al., 2016, i.a.).\\n• Public data: We require that tasks haveexisting public training data in order to minimize\\nthe risks involved in newly-created datasets. We also prefer tasks for which we have access\\nto (or could create) a test set with private labels.\\n• Task format: We prefer tasks that had relatively simple input and output formats, to avoid\\nincentivizing the users of the benchmark to create complex task-speciﬁc model architectures.\\nNevertheless, while GLUE is restricted to tasks involving single sentence or sentence pair\\ninputs, for SuperGLUE we expand the scope to consider tasks with longer inputs. This\\nyields a set of tasks that requires understanding individual tokens in context, complete\\nsentences, inter-sentence relations, and entire paragraphs.\\n• License: We require that task data be available under licences that allow use and redistribu-\\ntion for research purposes.\\nTo identify possible tasks for SuperGLUE, we disseminated a public call for task proposals to the\\nNLP community, and received approximately 30 proposals. We ﬁltered these proposals according\\nto our criteria. Many proposals were not suitable due to licensing issues, complex formats, and\\ninsufﬁcient headroom; we provide examples of such tasks in Appendix D. For each of the remaining\\ntasks, we ran a BERT-based baseline and a human baseline, and ﬁltered out tasks which were either\\ntoo challenging for humans without extensive training or too easy for our machine baselines.\\n3.2 Selected Tasks\\nFollowing this process, we arrived at eight tasks to use in SuperGLUE. See Tables 1 and 2 for details\\nand speciﬁc examples of each task.\\nBoolQ (Boolean Questions, Clark et al., 2019) is a QA task where each example consists of a short\\npassage and a yes/no question about the passage. The questions are provided anonymously and\\nunsolicited by users of the Google search engine, and afterwards paired with a paragraph from a\\nWikipedia article containing the answer. Following the original work, we evaluate with accuracy.\\nCB (CommitmentBank, De Marneffe et al., 2019) is a corpus of short texts in which at least one\\nsentence contains an embedded clause. Each of these embedded clauses is annotated with the degree\\nto which it appears the person who wrote the text iscommitted to the truth of the clause. The resulting\\ntask framed as three-class textual entailment on examples that are drawn from the Wall Street Journal,\\nﬁction from the British National Corpus, and Switchboard. Each example consists of a premise\\ncontaining an embedded clause and the corresponding hypothesis is the extraction of that clause.\\nWe use a subset of the data that had inter-annotator agreement above80%. The data is imbalanced\\n(relatively fewer neutral examples), so we evaluate using accuracy and F1, where for multi-class F1\\nwe compute the unweighted average of the F1 per class.\\n4'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 5,\n",
       "  'text': 'Table 2: Development set examples from the tasks in SuperGLUE. Bold text represents part of the\\nexample format for each task. Text in italics is part of the model input. Underlined text is specially\\nmarked in the input. Text in a monospaced font represents the expected model output.\\nBoolQ\\nPassage: Barq’s – Barq’s is an American soft drink. Its brand of root beer is notable for having caffeine.\\nBarq’s, created by Edward Barq and bottled since the turn of the 20th century, is owned by the Barq\\nfamily but bottled by the Coca-Cola Company. It was known as Barq’s Famous Olde Tyme Root Beer\\nuntil 2012.\\nQuestion: is barq’s root beer a pepsi product Answer: No\\nCB\\nText: B: And yet, uh, I we-, I hope to see employer based, you know, helping out. You know, child, uh,\\ncare centers at the place of employment and things like that, that will help out. A: Uh-huh. B: What do\\nyou think, do you think we are, setting a trend?\\nHypothesis: they are setting a trend Entailment: Unknown\\nCOPA\\nPremise: My body cast a shadow over the grass. Question: What’s the CAUSE for this?\\nAlternative 1: The sun was rising. Alternative 2: The grass was cut.\\nCorrect Alternative: 1\\nMultiRC\\nParagraph: Susan wanted to have a birthday party. She called all of her friends. She has ﬁve friends.\\nHer mom said that Susan can invite them all to the party. Her ﬁrst friend could not go to the party\\nbecause she was sick. Her second friend was going out of town. Her third friend was not so sure if her\\nparents would let her. The fourth friend said maybe. The ﬁfth friend could go to the party for sure. Susan\\nwas a little sad. On the day of the party, all ﬁve friends showed up. Each friend had a present for Susan.\\nSusan was happy and sent each friend a thank you card the next week\\nQuestion: Did Susan’s sick friend recover? Candidate answers: Yes, she recovered (T), No (F), Yes\\n(T), No, she didn’t recover(F), Yes, she was at Susan’s party(T)\\nReCoRD\\nParagraph: (CNN) Puerto Rico on Sunday overwhelmingly voted for statehood. But Congress, the only\\nbody that can approve new states, will ultimately decide whether the status of the US commonwealth\\nchanges. Ninety-seven percent of the votes in the nonbinding referendum favored statehood, an increase\\nover the results of a 2012 referendum, ofﬁcial results from the State Electorcal Commission show. It\\nwas the ﬁfth such vote on statehood. \"Today, we the people of Puerto Rico are sending a strong and\\nclear message to the US Congress ... and to the world ... claiming our equal rights as American citizens,\\nPuerto Rico Gov. Ricardo Rossello said in a news release. @highlight Puerto Rico voted Sunday in\\nfavor of US statehood\\nQuery For one, they can truthfully say, “Don’t blame me, I didn’t vote for them, ” when discussing the\\n<placeholder> presidency Correct Entities: US\\nRTE\\nText: Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44,\\naccording to the Christopher Reeve Foundation.\\nHypothesis: Christopher Reeve had an accident. Entailment: False\\nWiC\\nContext 1: Room and board. Context 2: He nailed boards across the windows.\\nSense match: False\\nWSC\\nText: Mark told Pete many lies about himself, which Pete included in his book. He should have been\\nmore truthful. Coreference: False\\nCOPA (Choice of Plausible Alternatives, Roemmele et al., 2011) is a causal reasoning task in which\\na system is given a premise sentence and must determine either the cause or effect of the premise\\nfrom two possible choices. All examples are handcrafted and focus on topics from blogs and a\\nphotography-related encyclopedia. Following the original work, we evaluate using accuracy.\\nMultiRC (Multi-Sentence Reading Comprehension, Khashabi et al., 2018) is a QA task where each\\nexample consists of a context paragraph, a question about that paragraph, and a list of possible\\nanswers. The system must predict which answers are true and which are false. While many QA\\ntasks exist, we use MultiRC because of a number of desirable properties: (i) each question can have\\nmultiple possible correct answers, so each question-answer pair must be evaluated independent of\\nother pairs, (ii) the questions are designed such that answering each question requires drawing facts\\nfrom multiple context sentences, and (iii) the question-answer pair format more closely matches\\nthe API of other tasks in SuperGLUE than the more popular span-extractive QA format does. The\\nparagraphs are drawn from seven domains including news, ﬁction, and historical text. The evaluation\\nmetrics are F1 over all answer-options (F1a) and exact match of each question’s set of answers (EM).\\n5'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 6,\n",
       "  'text': 'ReCoRD (Reading Comprehension with Commonsense Reasoning Dataset, Zhang et al., 2018) is a\\nmultiple-choice QA task. Each example consists of a news article and a Cloze-style question about\\nthe article in which one entity is masked out. The system must predict the masked out entity from a\\ngiven list of possible entities in the provided passage, where the same entity may be expressed using\\nmultiple different surface forms, all of which are considered correct. Articles are drawn from CNN\\nand Daily Mail. Following the original work, we evaluate with max (over all mentions) token-level\\nF1 and exact match (EM).\\nRTE (Recognizing Textual Entailment) datasets come from a series of annual competitions on textual\\nentailment.2 RTE is included in GLUE, and we use the same data and format as GLUE: We merge data\\nfrom RTE1 (Dagan et al., 2006), RTE2 (Bar Haim et al., 2006), RTE3 (Giampiccolo et al., 2007), and\\nRTE5 (Bentivogli et al., 2009).3 All datasets are combined and converted to two-class classiﬁcation:\\nentailment and not_entailment. Of all the GLUE tasks, RTE is among those that beneﬁts from\\ntransfer learning the most, with performance jumping from near random-chance (∼56%) at the time\\nof GLUE’s launch to 86.3% accuracy (Liu et al., 2019d; Yang et al., 2019) at the time of writing.\\nGiven the nearly eight point gap with respect to human performance, however, the task is not yet\\nsolved by machines, and we expect the remaining gap to be difﬁcult to close.\\nWiC (Word-in-Context, Pilehvar and Camacho-Collados, 2019) is a word sense disambiguation task\\ncast as binary classiﬁcation of sentence pairs. Given two text snippets and a polysemous word that\\nappears in both sentences, the task is to determine whether the word is used with the same sense in\\nboth sentences. Sentences are drawn from WordNet (Miller, 1995), VerbNet (Schuler, 2005), and\\nWiktionary. We follow the original work and evaluate using accuracy.\\nWSC (Winograd Schema Challenge, Levesque et al., 2012) is a coreference resolution task in\\nwhich examples consist of a sentence with a pronoun and a list of noun phrases from the sentence.\\nThe system must determine the correct referrent of the pronoun from among the provided choices.\\nWinograd schemas are designed to require everyday knowledge and commonsense reasoning to solve.\\nGLUE includes a version of WSC recast as NLI, known as WNLI. Until very recently, no substantial\\nprogress had been made on WNLI, with many submissions opting to submit majority class predic-\\ntions.4 In the past few months, several works (Kocijan et al., 2019; Liu et al., 2019d) have made rapid\\nprogress via a hueristic data augmentation scheme, raising machine performance to 90.4% accuracy.\\nGiven estimated human performance of ∼96%, there is still a gap between machine and human\\nperformance, which we expect will be relatively difﬁcult to close. We therefore include a version of\\nWSC cast as binary classiﬁcation, where each example consists of a sentence with a marked pronoun\\nand noun, and the task is to determine if the pronoun refers to that noun. The training and validation\\nexamples are drawn from the original WSC data (Levesque et al., 2012), as well as those distributed\\nby the afﬁliated organization Commonsense Reasoning.5 The test examples are derived from ﬁction\\nbooks and have been shared with us by the authors of the original dataset. We evaluate using accuracy.\\n3.3 Scoring\\nAs with GLUE, we seek to give a sense of aggregate system performance over all tasks by averaging\\nscores of all tasks. Lacking a fair criterion with which to weight the contributions of each task to\\nthe overall score, we opt for the simple approach of weighing each task equally, and for tasks with\\nmultiple metrics, ﬁrst averaging those metrics to get a task score.\\n3.4 Tools for Model Analysis\\nAnalyzing Linguistic and World Knowledge in Models GLUE includes an expert-constructed,\\ndiagnostic dataset that automatically tests models for a broad range of linguistic, commonsense, and\\nworld knowledge. Each example in this broad-coverage diagnostic is a sentence pair labeled with\\n2Textual entailment is also known as natural language inference, or NLI\\n3RTE4 is not publicly available, while RTE6 and RTE7 do not conform to the standard NLI task.\\n4WNLI is especially difﬁcult due to an adversarial train/dev split: Premise sentences that appear in the\\ntraining set often appear in the development set with a different hypothesis and a ﬂipped label. If a system\\nmemorizes the training set, which was easy due to the small size of the training set, it could perform far below\\nchance on the development set. We remove this adversarial design in our version of WSC by ensuring that no\\nsentences are shared between the training, validation, and test sets.\\n5http://commonsensereasoning.org/disambiguation.html\\n6'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 7,\n",
       "  'text': 'a three-way entailment relation (entailment, neutral, or contradiction) and tagged with labels that\\nindicate the phenomena that characterize the relationship between the two sentences. Submissions\\nto the GLUE leaderboard are required to include predictions from the submission’s MultiNLI\\nclassiﬁer on the diagnostic dataset, and analyses of the results were shown alongside the main\\nleaderboard. Since this broad-coverage diagnostic task has proved difﬁcult for top models, we retain\\nit in SuperGLUE. However, since MultiNLI is not part of SuperGLUE, we collapse contradiction\\nand neutral into a single not_entailment label, and request that submissions include predictions\\non the resulting set from the model used for the RTE task. We collect non-expert annotations to\\nestimate human performance, following the same procedure we use for the main benchmark tasks\\n(Section 5.2). We estimate an accuracy of 88% and a Matthew’s correlation coefﬁcient (MCC, the\\ntwo-class variant of the R3 metric used in GLUE) of 0.77.\\nAnalyzing Gender Bias in Models Recent work has identiﬁed the presence and ampliﬁcation\\nof many social biases in data-driven machine learning models. (Lu et al., 2018; Zhao et al., 2018;\\nKiritchenko and Mohammad, 2018). To promote the detection of such biases, we include Winogender\\n(Rudinger et al., 2018) as an additional diagnostic dataset. Winogender is designed to measure gender\\nbias in coreference resolution systems. We use the Diverse Natural Language Inference Collection\\n(DNC; Poliak et al., 2018) version that casts Winogender as a textual entailment task.6 Each example\\nconsists of a premise sentence with a male or female pronoun and a hypothesis giving a possible\\nantecedent of the pronoun. Examples occur in minimal pairs, where the only difference between\\nan example and its pair is the gender of the pronoun in the premise. Performance on Winogender\\nis measured with both accuracy and the gender parity score: the percentage of minimal pairs for\\nwhich the predictions are the same. We note that a system can trivially obtain a perfect gender parity\\nscore by guessing the same class for all examples, so a high gender parity score is meaningless unless\\naccompanied by high accuracy. We collect non-expert annotations to estimate human performance,\\nand observe an accuracy of 99.7% and a gender parity score of 0.99.\\nLike any diagnostic, Winogender has limitations. It offers only positive predictive value: A poor\\nbias score is clear evidence that a model exhibits gender bias, but a good score does not mean that\\nthe model is unbiased. More speciﬁcally, in the DNC version of the task, a low gender parity score\\nmeans that a model’s prediction of textual entailment can be changed with a change in pronouns, all\\nelse equal. It is plausible that there are forms of bias that are relevant to target tasks of interest, but\\nthat do not surface in this setting (Gonen and Goldberg, 2019). In addition, Winogender does not\\ncover all forms of social bias, or even all forms of gender. For instance, the version of the data used\\nhere offers no coverage of gender-neutral they or non-binary pronouns. Despite these limitations, we\\nbelieve that Winogender’s inclusion is worthwhile in providing a coarse sense of how social biases\\nevolve with model performance and for keeping attention on the social ramiﬁcations of NLP models.\\n4 Using SuperGLUE\\nSoftware Tools To facilitate using SuperGLUE, we releasejiant (Wang et al., 2019b),7 a modular\\nsoftware toolkit, built with PyTorch (Paszke et al., 2017), components from AllenNLP (Gardner\\net al., 2017), and the pytorch-pretrained-bert package.8 jiant implements our baselines and\\nsupports the evaluation of custom models and training methods on the benchmark tasks. The toolkit\\nincludes support for existing popular pretrained models such as OpenAI GPT and BERT, as well as\\nsupport for multistage and multitask learning of the kind seen in the strongest models on GLUE.\\nEligibility Any system or method that can produce predictions for the SuperGLUE tasks is eligible\\nfor submission to the leaderboard, subject to the data-use and submission frequency policies stated\\nimmediately below. There are no restrictions on the type of methods that may be used, and there is\\nno requirement that any form of parameter sharing or shared initialization be used across the tasks in\\nthe benchmark. To limit overﬁtting to the private test data, users are limited to a maximum of two\\nsubmissions per day and six submissions per month.\\n6We ﬁlter out 23 examples where the labels are ambiguous\\n7https://github.com/nyu-mll/jiant\\n8https://github.com/huggingface/pytorch-pretrained-BERT\\n7'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 8,\n",
       "  'text': 'Table 3: Baseline performance on the SuperGLUE test sets and diagnostics. For CB we report\\naccuracy and macro-average F1. For MultiRC we report F1 on all answer-options and exact match\\nof each question’s set of correct answers. AXb is the broad-coverage diagnostic task, scored using\\nMatthews’ correlation (MCC). AXg is the Winogender diagnostic, scored using accuracy and the\\ngender parity score (GPS). All values are scaled by 100. The Avg column is the overall benchmark\\nscore on non-AX∗ tasks. The bolded numbers reﬂect the best machine performance on task. *MultiRC\\nhas multiple test sets released on a staggered schedule, and these results evaluate on an installation of\\nthe test set that is a subset of ours.\\nModel Avg BoolQ CB COPA MultiRC ReCoRD RTE WiC WSC AX b AXg\\nMetrics Acc. F1/Acc. Acc. F1 a/EM F1/EM Acc. Acc. Acc. MCC GPS Acc.\\nMost Frequent 47.1 62.3 21.7/48.4 50.0 61.1 / 0.3 33.4/32.5 50.3 50.0 65.1 0.0 100.0/ 50.0\\nCBoW 44.3 62.1 49.0/71.2 51.6 0.0 / 0.4 14.0/13.6 49.7 53.0 65.1 -0.4 100.0/ 50.0\\nBERT 69.0 77.4 75.7/83.6 70.6 70.0 / 24.0 72.0/71.3 71.6 69.5 64.3 23.0 97.8 / 51.7\\nBERT++ 71.5 79.0 84.7/90.4 73.8 70.0 / 24.1 72.0/71.3 79.0 69.5 64.3 38.0 99.4 / 51.4\\nOutside Best - 80.4 - / - 84.4 70.4 */24.5* 74.8/73.0 82.7 - - - - / -\\nHuman (est.) 89.8 89.0 95.8/98.9 100.0 81.8*/51.9* 91.7/91.3 93.6 80.0 100.0 77.0 99.3 / 99.7\\nData Data for the tasks are available for download through the SuperGLUE site and through a\\ndownload script included with the software toolkit. Each task comes with a standardized training set,\\ndevelopment set, and unlabeled test set. Submitted systems may use any public or private data when\\ndeveloping their systems, with a few exceptions: Systems may only use the SuperGLUE-distributed\\nversions of the task datasets, as these use different train/validation/test splits from other public\\nversions in some cases. Systems also may not use the unlabeled test data for the tasks in system\\ndevelopment in any way, may not use the structured source data that was used to collect the WiC\\nlabels (sense-annotated example sentences from WordNet, VerbNet, and Wiktionary) in any way, and\\nmay not build systems that share information across separate test examples in any way.\\nWe do not endorse the use of the benchmark data for non-research applications, due to concerns\\nabout socially relevant biases (such as ethnicity–occupation associations) that may be undesirable\\nor legally problematic in deployed systems. Because these biases are evident in texts from a wide\\nvariety of sources and collection methods (e.g., Rudinger et al., 2017), and because none of our task\\ndatasets directly mitigate them, one can reasonably presume that our training sets teach models these\\nbiases to some extent and that our evaluation sets similarly reward models that learn these biases.\\nTo ensure reasonable credit assignment, because we build very directly on prior work, we ask the\\nauthors of submitted systems to directly name and cite the speciﬁc datasets that they use,including the\\nbenchmark datasets. We will enforce this as a requirement for papers to be listed on the leaderboard.\\n5 Experiments\\n5.1 Baselines\\nBERT Our main baselines are built around BERT, variants of which are among the most successful\\napproach on GLUE at the time of writing. Speciﬁcally, we use the bert-large-cased variant.\\nFollowing the practice recommended in Devlin et al. (2019), for each task, we use the simplest\\npossible architecture on top of BERT. We ﬁne-tune a copy of the pretrained BERT model separately\\nfor each task, and leave the development of multi-task learning models to future work. For training,\\nwe use the procedure speciﬁed in Devlin et al. (2019): We use Adam (Kingma and Ba, 2014) with an\\ninitial learning rate of 10−5 and ﬁne-tune for a maximum of 10 epochs.\\nFor classiﬁcation tasks with sentence-pair inputs (BoolQ, CB, RTE, WiC), we concatenate the\\nsentences with a [SEP ] token, feed the fused input to BERT, and use a logistic regression classiﬁer that\\nsees the representation corresponding to [CLS ]. For WiC only, we also concatenate the representation\\nof the marked word to the [CLS ] representation. For COPA, MultiRC, and ReCoRD, for each answer\\nchoice, we similarly concatenate the context with that answer choice and feed the resulting sequence\\ninto BERT to produce an answer representation. For COPA, we project these representations into a\\nscalar, and take as the answer the choice with the highest associated scalar. For MultiRC, because\\neach question can have more than one correct answer, we feed each answer representation into\\n8'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 9,\n",
       "  'text': 'a logistic regression classiﬁer. For ReCoRD, we also evaluate the probability of each candidate\\nindependent of other candidates, and take the most likely candidate as the model’s prediction. For\\nWSC, which is a span-based task, we use a model inspired by Tenney et al. (2019). Given the BERT\\nrepresentation for each word in the original sentence, we get span representations of the pronoun\\nand noun phrase via a self-attention span-pooling operator (Lee et al., 2017), before feeding it into a\\nlogistic regression classiﬁer.\\nBERT++ We also report results using BERT with additional training on related datasets before\\nﬁne-tuning on the benchmark tasks, following the STILTs two-stage style of transfer learning (Phang\\net al., 2018). Given the productive use of MultiNLI in pretraining and intermediate ﬁne-tuning of\\npretrained language models (Conneau et al., 2017; Phang et al., 2018, i.a.), for CB, RTE, and BoolQ,\\nwe use MultiNLI as a transfer task by ﬁrst using the above procedure on MultiNLI. Similarly, given\\nthe similarity of COPA to SWAG (Zellers et al., 2018), we ﬁrst ﬁne-tune BERT on SWAG. These\\nresults are reported as BERT++. For all other tasks, we reuse the results of BERT ﬁne-tuned on just\\nthat task.\\nSimple Baselines We include a baseline where for each task we simply predict the majority class,9\\nas well as a bag-of-words baseline where each input is represented as an average of its tokens’ GloVe\\nword vectors (the 300D/840B release from Pennington et al., 2014).\\nOutside Best We list the best known result on each task to date, except on tasks which we recast\\n(WSC), resplit (CB), or achieve the best known result (WiC). The outside results for COPA, MultiRC,\\nand RTE are from Sap et al. (2019), Trivedi et al. (2019), and Liu et al. (2019d) respectively.\\n5.2 Human Performance\\nPilehvar and Camacho-Collados (2019), Khashabi et al. (2018), Nangia and Bowman (2019), and\\nZhang et al. (2018) respectively provide estimates for human performance on WiC, MultiRC, RTE,\\nand ReCoRD. For the remaining tasks, including the diagnostic set, we estimate human performance\\nby hiring crowdworker annotators through Amazon’s Mechanical Turk platform to reannotate a\\nsample of each test set. We follow a two step procedure where a crowd worker completes a short\\ntraining phase before proceeding to the annotation phase, modeled after the method used by Nangia\\nand Bowman (2019) for GLUE. For both phases and all tasks, the average pay rate is $23.75/hr.10\\nIn the training phase, workers are provided with instructions on the task, linked to an FAQ page, and\\nare asked to annotate up to 30 examples from the development set. After answering each example,\\nworkers are also asked to check their work against the provided ground truth label. After the training\\nphase is complete, we provide the qualiﬁcation to work on the annotation phase to all workers\\nwho annotated a minimum of ﬁve examples, i.e. completed ﬁve HITs during training and achieved\\nperformance at, or above the median performance across all workers during training.\\nIn the annotation phase, workers are provided with the same instructions as the training phase, and\\nare linked to the same FAQ page. The instructions for all tasks are provided in Appendix C. For the\\nannotation phase we randomly sample 100 examples from the task’s test set, with the exception of\\nWSC where we annotate the full test set. For each example, we collect annotations from ﬁve workers\\nand take a majority vote to estimate human performance. For additional details, see Appendix C.3.\\n5.3 Results\\nTable 3 shows results for all baselines. The simple baselines of predicting the most frequent class\\nand CBOW do not perform well overall, achieving near chance performance for several of the tasks.\\nUsing BERT increases the average SuperGLUE score by 25 points, attaining signiﬁcant gains on\\nall of the benchmark tasks, particularly MultiRC, ReCoRD, and RTE. On WSC, BERT actually\\nperforms worse than the simple baselines, likely due to the small size of the dataset and the lack of\\ndata augmentation. Using MultiNLI as an additional source of supervision for BoolQ, CB, and RTE\\nleads to a 2-5 point improvement on all tasks. Using SWAG as a transfer task for COPA sees an 8\\npoint improvement.\\n9For ReCoRD, we predict the entity that has the highest F1 with the other entity options.\\n10This estimate is taken from https://turkerview.com.\\n9'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 10,\n",
       "  'text': 'Our best baselines still lag substantially behind human performance. On average, there is a nearly 20\\npoint gap between BERT++ and human performance. The largest gap is on WSC, with a 35 point\\ndifference between the best model and human performance. The smallest margins are on BoolQ,\\nCB, RTE, and WiC, with gaps of around 10 points on each of these. We believe these gaps will be\\nchallenging to close: On WSC and COPA, human performance is perfect. On three other tasks, it is\\nin the mid-to-high 90s. On the diagnostics, all models continue to lag signiﬁcantly behind humans.\\nThough all models obtain near perfect gender parity scores on Winogender, this is due to the fact that\\nthey are obtaining accuracy near that of random guessing.\\n6 Conclusion\\nWe present SuperGLUE, a new benchmark for evaluating general-purpose language understanding\\nsystems. SuperGLUE updates the GLUE benchmark by identifying a new set of challenging NLU\\ntasks, as measured by the difference between human and machine baselines. The set of eight tasks in\\nour benchmark emphasizes diverse task formats and low-data training data tasks, with nearly half the\\ntasks having fewer than 1k examples and all but one of the tasks having fewer than 10k examples.\\nWe evaluate BERT-based baselines and ﬁnd that they still lag behind humans by nearly 20 points.\\nGiven the difﬁculty of SuperGLUE for BERT, we expect that further progress in multi-task, transfer,\\nand unsupervised/self-supervised learning techniques will be necessary to approach human-level per-\\nformance on the benchmark. Overall, we argue that SuperGLUE offers a rich and challenging testbed\\nfor work developing new general-purpose machine learning methods for language understanding.\\n7 Acknowledgments\\nWe thank the original authors of the included datasets in SuperGLUE for their cooperation in the\\ncreation of the benchmark, as well as those who proposed tasks and datasets that we ultimately could\\nnot include.\\nThis work was made possible in part by a donation to NYU from Eric and Wendy Schmidt made by\\nrecommendation of the Schmidt Futures program. We gratefully acknowledge the support of the\\nNVIDIA Corporation with the donation of a Titan V GPU used at NYU for this research. AW is\\nsupported by the National Science Foundation Graduate Research Fellowship Program under Grant\\nNo. DGE 1342536. Any opinions, ﬁndings, and conclusions or recommendations expressed in this\\nmaterial are those of the author(s) and do not necessarily reﬂect the views of the National Science\\nFoundation.\\nReferences\\nAnonymous. Bam! Born-again multi-task networks for natural language understanding. Anonymous\\npreprint under review, 2018. URL https://openreview.net/forum?id=SylnYlqKw4.\\nStephen H. Bach, Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao, Cassandra Xia, Souvik\\nSen, Alexander Ratner, Braden Hancock, Houman Alborzi, Rahul Kuchhal, Christopher Ré, and\\nRob Malkin. Snorkel drybell: A case study in deploying weak supervision at industrial scale. In\\nSIGMOD. ACM, 2018.\\nRoy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and\\nIdan Szpektor. The second PASCAL recognising textual entailment challenge. In Proceedings\\nof the Second PASCAL Challenges Workshop on Recognising Textual Entailment, 2006. URL\\nhttp://u.cs.biu.ac.il/~nlp/RTE2/Proceedings/01.pdf.\\nLuisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. The\\nﬁfth PASCAL recognizing textual entailment challenge. In Textual Analysis Conference (TAC),\\n2009. URL http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.232.1231.\\nSven Buechel, Anneke Buffone, Barry Slaff, Lyle Ungar, and João Sedoc. Modeling empathy and\\ndistress in reaction to news stories. In Proceedings of the 2018 Conference on Empirical Methods\\nin Natural Language Processing (EMNLP), 2018.\\n10'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 11,\n",
       "  'text': 'Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluation the role of bleu in machine\\ntranslation research. In Proceedings of the Conference of the European Chapter of the Association\\nfor Computational Linguistics (EACL). Association for Computational Linguistics, 2006. URL\\nhttps://www.aclweb.org/anthology/E06-1032.\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task\\n1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings\\nof the 11th International Workshop on Semantic Evaluation (SemEval-2017) . Association for\\nComputational Linguistics, 2017. doi: 10.18653/v1/S17-2001. URL https://www.aclweb.\\norg/anthology/S17-2001.\\nEunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke\\nZettlemoyer. QuAC: Question answering in context. In Proceedings of the 2018 Conference on\\nEmpirical Methods in Natural Language Processing (EMNLP). Association for Computational\\nLinguistics, 2018a.\\nEunsol Choi, Omer Levy, Yejin Choi, and Luke Zettlemoyer. Ultra-ﬁne entity typing. In Proceedings\\nof the Association for Computational Linguistics (ACL). Association for Computational Linguistics,\\n2018b. URL https://www.aclweb.org/anthology/P18-1009.\\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\\nToutanova. Boolq: Exploring the surprising difﬁculty of natural yes/no questions. In Proceedings\\nof the 2019 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924–2936,\\n2019.\\nRonan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: Deep\\nneural networks with multitask learning. In Proceedings of the 25th International Conference on\\nMachine Learning (ICML). Association for Computing Machinery, 2008. URL https://dl.acm.\\norg/citation.cfm?id=1390177.\\nAlexis Conneau and Douwe Kiela. SentEval: An evaluation toolkit for universal sentence representa-\\ntions. In Proceedings of the 11th Language Resources and Evaluation Conference. European Lan-\\nguage Resource Association, 2018. URL https://www.aclweb.org/anthology/L18-1269.\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes. Super-\\nvised learning of universal sentence representations from natural language inference data. In\\nProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing\\n(EMNLP). Association for Computational Linguistics, 2017. doi: 10.18653/v1/D17-1070. URL\\nhttps://www.aclweb.org/anthology/D17-1070.\\nIdo Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entail-\\nment challenge. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Vi-\\nsual Object Classiﬁcation, and Recognising Textual Entailment . Springer, 2006. URL https:\\n//link.springer.com/chapter/10.1007/11736790_9.\\nAndrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in Neural\\nInformation Processing Systems (NeurIPS). Curran Associates, Inc., 2015. URL http://papers.\\nnips.cc/paper/5949-semi-supervised-sequence-learning.pdf .\\nMarie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The CommitmentBank:\\nInvestigating projection in naturally occurring discourse. 2019. To appear in Proceedings of Sinn\\nund Bedeutung 23. Data can be found at https://github.com/mcdm/CommitmentBank/.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\\nbidirectional transformers for language understanding. In Proceedings of the Conference of the\\nNorth American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies (NAACL-HLT). Association for Computational Linguistics, 2019. URL https:\\n//arxiv.org/abs/1810.04805.\\nWilliam B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.\\nIn Proceedings of IWP, 2005.\\n11'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 12,\n",
       "  'text': 'Manaal Faruqui and Dipanjan Das. Identifying well-formed natural language questions. In Pro-\\nceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP) .\\nAssociation for Computational Linguistics, 2018. URLhttps://www.aclweb.org/anthology/\\nD18-1091.\\nTommaso Furlanello, Zachary C Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar.\\nBorn again neural networks. International Conference on Machine Learning (ICML), 2018. URL\\nhttp://proceedings.mlr.press/v80/furlanello18a.html.\\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew\\nPeters, Michael Schmitz, and Luke S. Zettlemoyer. AllenNLP: A deep semantic natural language\\nprocessing platform. In Proceedings of Workshop for NLP Open Source Software, 2017. URL\\nhttps://www.aclweb.org/anthology/W18-2501.\\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing\\ntextual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment\\nand Paraphrasing. Association for Computational Linguistics, 2007.\\nHila Gonen and Yoav Goldberg. Lipstick on a pig: Debiasing methods cover up systematic\\ngender biases in word embeddings but do not remove them. In Proceedings of the 2019\\nConference of the North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies, Volume 1 (Long and Short Papers) , pages 609–614, Min-\\nneapolis, Minnesota, June 2019. Association for Computational Linguistics. URL https:\\n//www.aclweb.org/anthology/N19-1061.\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences\\nfrom unlabelled data. In Proceedings of the Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).\\nAssociation for Computational Linguistics, 2016. doi: 10.18653/v1/N16-1162. URL https:\\n//www.aclweb.org/anthology/N16-1162.\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv\\npreprint 1503.02531, 2015. URL https://arxiv.org/abs/1503.02531.\\nRobin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems. In\\nProceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).\\nAssociation for Computational Linguistics, 2017. doi: 10.18653/v1/D17-1215. URL https:\\n//www.aclweb.org/anthology/D17-1215.\\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking\\nbeyond the surface: A challenge set for reading comprehension over multiple sentences. In\\nProceedings of the Conference of the North American Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies (NAACL-HLT). Association for Computational\\nLinguistics, 2018. URL https://www.aclweb.org/anthology/papers/N/N18/N18-1023/.\\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\\n1412.6980, 2014. URL https://arxiv.org/abs/1412.6980.\\nSvetlana Kiritchenko and Saif Mohammad. Examining gender and race bias in two hundred sentiment\\nanalysis systems. In Proceedings of the Seventh Joint Conference on Lexical and Computational\\nSemantics. Association for Computational Linguistics, 2018. doi: 10.18653/v1/S18-2005. URL\\nhttps://www.aclweb.org/anthology/S18-2005.\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. Skip-thought vectors. In Advances in neural information processing systems,\\n2015.\\nNikita Kitaev and Dan Klein. Multilingual constituency parsing with self-attention and pre-training.\\narXiv preprint 1812.11760, 2018. URL https://arxiv.org/abs/1812.11760.\\nVid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas Lukasiewicz.\\nA surprisingly robust trick for winograd schema challenge. arXiv preprint 1905.06290, 2019.\\n12'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 13,\n",
       "  'text': 'Kenton Lee, Luheng He, Mike Lewis, and Luke Zettlemoyer. End-to-end neural coreference\\nresolution. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language\\nProcessing. Association for Computational Linguistics, September 2017. doi: 10.18653/v1/\\nD17-1018. URL https://www.aclweb.org/anthology/D17-1018.\\nHector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge. In\\nThirteenth International Conference on the Principles of Knowledge Representation and Reasoning,\\n2012. URL http://dl.acm.org/citation.cfm?id=3031843.3031909.\\nChia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, and Joelle Pineau.\\nHow not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics\\nfor dialogue response generation. In Proceedings of the 2016 Conference on Empirical Methods in\\nNatural Language Processing. Association for Computational Linguistics, 2016. doi: 10.18653/\\nv1/D16-1230. URL https://www.aclweb.org/anthology/D16-1230.\\nNelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. Linguistic\\nknowledge and transferability of contextual representations. In Proceedings of the Conference of\\nthe North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies (NAACL-HLT). Association for Computational Linguistics, 2019a. URL https:\\n//arxiv.org/abs/1903.08855.\\nNelson F. Liu, Roy Schwartz, and Noah A. Smith. Inoculation by ﬁne-tuning: A method for\\nanalyzing challenge datasets. In Proceedings of the Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Technologies (NAACL-\\nHLT). Association for Computational Linguistics, 2019b. URL https://arxiv.org/abs/1904.\\n02668.\\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Improving multi-task deep neural\\nnetworks via knowledge distillation for natural language understanding.arXiv preprint 1904.09482,\\n2019c. URL http://arxiv.org/abs/1904.09482.\\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for\\nnatural language understanding. arXiv preprint 1901.11504, 2019d.\\nKaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Amancharla, and Anupam Datta. Gender bias in\\nneural natural language processing. arXiv preprint 1807.11714, 2018. URL http://arxiv.org/\\nabs/1807.11714.\\nBryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in transla-\\ntion: Contextualized word vectors. In Advances in Neural Information Processing Sys-\\ntems (NeurIPS) . Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/\\n7209-learned-in-translation-contextualized-word-vectors.pdf .\\nBryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language\\ndecathlon: Multitask learning as question answering. arXiv preprint 1806.08730, 2018. URL\\nhttps://arxiv.org/abs/1806.08730.\\nR. Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic\\nheuristics in natural language inference. In Proceedings of the Association for Computational\\nLinguistics (ACL). Association for Computational Linguistics, 2019. URL https://arxiv.org/\\nabs/1902.01007.\\nRichard T. McCoy and Tal Linzen. Non-entailed subsequences as a challenge for natural language\\ninference. In Proceedings of the Society for Computational in Linguistics (SCiL) 2019, 2019. URL\\nhttps://scholarworks.umass.edu/scil/vol2/iss1/46/.\\nGeorge A Miller. WordNet: a lexical database for english. Communications of the ACM, 1995. URL\\nhttps://www.aclweb.org/anthology/H94-1111.\\nAakanksha Naik, Abhilasha Ravichander, Norman M. Sadeh, Carolyn Penstein Rosé, and Graham\\nNeubig. Stress test evaluation for natural language inference. In International Conference on\\nComputational Linguistics (COLING), 2018.\\n13'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 14,\n",
       "  'text': 'Nikita Nangia and Samuel R. Bowman. Human vs. Muppet: A conservative estimate of hu-\\nman performance on the GLUE benchmark. In Proceedings of the Association of Compu-\\ntational Linguistics (ACL) . Association for Computational Linguistics, 2019. URL https:\\n//woollysocks.github.io/assets/GLUE_Human_Baseline.pdf.\\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\\nZeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in\\nPyTorch. In Advances in Neural Information Processing Systems (NeurIPS). Curran Associates,\\nInc., 2017. URL https://openreview.net/pdf?id=BJJsrmfCZ.\\nJeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word\\nrepresentation. In Proceedings of the Conference on Empirical Methods in Natural Language Pro-\\ncessing (EMNLP). Association for Computational Linguistics, 2014. doi: 10.3115/v1/D14-1162.\\nURL https://www.aclweb.org/anthology/D14-1162.\\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\\nLuke Zettlemoyer. Deep contextualized word representations. In Proceedings of the Conference of\\nthe North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies (NAACL-HLT). Association for Computational Linguistics, 2018. doi: 10.18653/v1/\\nN18-1202. URL https://www.aclweb.org/anthology/N18-1202.\\nJason Phang, Thibault Févry, and Samuel R Bowman. Sentence encoders on STILTs: Supplementary\\ntraining on intermediate labeled-data tasks. arXiv preprint 1811.01088 , 2018. URL https:\\n//arxiv.org/abs/1811.01088.\\nMohammad Taher Pilehvar and Jose Camacho-Collados. WiC: The word-in-context dataset for\\nevaluating context-sensitive meaning representations. In Proceedings of the Conference of the\\nNorth American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies (NAACL-HLT). Association for Computational Linguistics, 2019. URL https:\\n//arxiv.org/abs/1808.09121.\\nAdam Poliak, Aparajita Haldar, Rachel Rudinger, J. Edward Hu, Ellie Pavlick, Aaron Steven White,\\nand Benjamin Van Durme. Collecting diverse natural language inference problems for sentence\\nrepresentation evaluation. In Proceedings of the 2018 Conference on Empirical Methods in\\nNatural Language Processing. Association for Computational Linguistics, 2018. URL https:\\n//www.aclweb.org/anthology/D18-1007.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language un-\\nderstanding by generative pre-training, 2018. Unpublished ms. available through a link at\\nhttps://blog.openai.com/language-unsupervised/.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions\\nfor machine comprehension of text. In Proceedings of the Conference on Empirical Methods in\\nNatural Language Processing (EMNLP). Association for Computational Linguistics, 2016. doi:\\n10.18653/v1/D16-1264. URL http://aclweb.org/anthology/D16-1264.\\nMelissa Roemmele, Cosmin Adrian Bejan, and Andrew S. Gordon. Choice of plausible alternatives:\\nAn evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series, 2011.\\nRachel Rudinger, Chandler May, and Benjamin Van Durme. Social bias in elicited natural language\\ninferences. In Proceedings of the First ACL Workshop on Ethics in Natural Language Processing.\\nAssociation for Computational Linguistics, 2017. doi: 10.18653/v1/W17-1609. URL https:\\n//www.aclweb.org/anthology/W17-1609.\\nRachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias in\\ncoreference resolution. In Proceedings of the 2018 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Technologies. Association\\nfor Computational Linguistics, 2018. doi: 10.18653/v1/N18-2002. URL https://www.aclweb.\\norg/anthology/N18-2002.\\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense\\nreasoning about social interactions. arXiv preprint 1904.09728, 2019. URL https://arxiv.\\norg/abs/1904.09728.\\n14'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 15,\n",
       "  'text': 'Nathan Schneider and Noah A Smith. A corpus and model integrating multiword expressions and\\nsupersenses. In Proceedings of the Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies (NAACL-HLT). Association for\\nComputational Linguistics, 2015. URL https://www.aclweb.org/anthology/N15-1177.\\nKarin Kipper Schuler. Verbnet: A Broad-coverage, Comprehensive Verb Lexicon. PhD thesis, 2005.\\nURL http://verbs.colorado.edu/~kipper/Papers/dissertation.pdf.\\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng,\\nand Christopher. Potts. Recursive deep models for semantic compositionality over a sentiment\\ntreebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing\\n(EMNLP). Association for Computational Linguistics, 2013. URL https://www.aclweb.org/\\nanthology/D13-1170.\\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim,\\nBenjamin Van Durme, Sam Bowman, Dipanjan Das, and Ellie Pavlick. What do you learn from\\ncontext? probing for sentence structure in contextualized word representations. 2019. URL\\nhttps://openreview.net/forum?id=SJzSgnRcKX.\\nHarsh Trivedi, Heeyoung Kwon, Tushar Khot, Ashish Sabharwal, and Niranjan Balasubramanian.\\nRepurposing entailment for multi-hop question answering tasks, 2019. URL https://arxiv.\\norg/abs/1904.09380.\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. In\\nInternational Conference on Learning Representations , 2019a. URL https://openreview.\\nnet/forum?id=rJ4km2R5t7.\\nAlex Wang, Ian F. Tenney, Yada Pruksachatkun, Katherin Yu, Jan Hula, Patrick Xia, Raghu Pappagari,\\nShuning Jin, R. Thomas McCoy, Roma Patel, Yinghui Huang, Jason Phang, Edouard Grave,\\nNajoung Kim, Phu Mon Htut, Thibault F’evry, Berlin Chen, Nikita Nangia, Haokun Liu, , Anhad\\nMohananey, Shikha Bordia, Ellie Pavlick, and Samuel R. Bowman. jiant 1.0: A software toolkit\\nfor research on general-purpose text understanding models. http://jiant.info/, 2019b.\\nAlex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments.\\narXiv preprint 1805.12471, 2018. URL https://arxiv.org/abs/1805.12471.\\nKellie Webster, Marta Recasens, Vera Axelrod, and Jason Baldridge. Mind the GAP: A balanced\\ncorpus of gendered ambiguous pronouns. Transactions of the Association for Computational\\nLinguistics (TACL), 2018. URL https://www.aclweb.org/anthology/Q18-1042.\\nAdina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for\\nsentence understanding through inference. In Proceedings of the Conference of the North American\\nChapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-\\nHLT). Association for Computational Linguistics, 2018. URLhttp://aclweb.org/anthology/\\nN18-1101.\\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V .\\nLe. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint\\n1906.0823, 2019.\\nFabio Massimo Zanzotto and Lorenzo Ferrone. Have you lost the thread? discovering ongoing\\nconversations in scattered dialog blocks. ACM Transactions on Interactive Intelligent Systems\\n(TiiS), 2017.\\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. SW AG: A large-scale adversarial dataset\\nfor grounded commonsense inference. 2018. URL https://www.aclweb.org/anthology/\\nD18-1009.\\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme.\\nRecord: Bridging the gap between human and machine commonsense reading comprehension.\\narXiv preprint 1810.12885, 2018.\\n15'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 16,\n",
       "  'text': 'Yuan Zhang, Jason Baldridge, and Luheng He. PAWS: Paraphrase adversaries from word scrambling.\\narXiv preprint 1904.01130, 2019. URL https://arxiv.org/abs/1904.01130.\\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Gender bias in\\ncoreference resolution: Evaluation and debiasing methods. In Proceedings of the 2018 Conference\\nof the North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies. Association for Computational Linguistics, 2018. doi: 10.18653/v1/N18-2003. URL\\nhttps://www.aclweb.org/anthology/N18-2003.\\n16'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 17,\n",
       "  'text': 'Table 4: Baseline performance on the SuperGLUE development.\\nModel Avg BoolQ CB COPA MultiRC ReCoRD RTE WiC WSC\\nMetrics Acc. Acc./F1 Acc. F1 a/EM F1/EM Acc. Acc. Acc.\\nMost Frequent Class 47.7 62.2 50 /22.2 55 59.9/ 0.8 32.4/31.5 52.7 50.0 63.5\\nCBOW 47.7 62.4 71.4/49.6 63.0 20.3/ 0.3 14.4/13.8 54.2 55.3 61.5\\nBERT 72.2 77.7 94.6/93.7 69.0 70.5/24.7 70.6/69.8 75.8 74.9 68.3\\nBERT++ 74.6 80.1 96.4/95.0 78.0 70.5/24.7 70.6/69.8 82.3 74.9 68.3\\nA Development Set Results\\nIn Table 4, we present results of the baselines on the SuperGLUE tasks development sets.\\nB Performance on GLUE Diagnostics\\nFigure 2 shows the performance on the GLUE diagnostics dataset for systems submitted to the public\\nleaderboard.\\nDisjunction Downward Monotone Restrictivity Double Negation Prepositional Phrases\\n60\\n40\\n20\\n0\\n20\\n40\\n60\\n80\\nChance\\nBiLSTM+ELMo+Attn\\nOpenAI GPT\\nBERT + Single-task Adapters\\nBERT (Large)\\nBERT on STILTs\\nBERT + BAM\\nSemBERT\\nSnorkel MeTaL\\nALICE (Large)\\nMT-DNN (ensemble)\\nXLNet-Large (ensemble)\\nFigure 2: Performance of GLUE submissions on selected diagnostic categories, reported using the\\nR3 metric scaled up by 100, as in Wang et al. (2019a, see paper for a description of the categories).\\nSome initially difﬁcult categories, like double negation, saw gains from advances on GLUE, but\\nothers remain hard (restrictivity) or even adversarial (disjunction, downward monotone).\\nC Instructions to Crowd Workers\\nC.1 Training Phase Instructions\\nFor collecting data to establish human performance on the SuperGLUE tasks, we follow a two\\nstep procedure where we ﬁrst provide some training to the crowd workers before they proceed to\\nannotation. In the training step, we provide workers with brief instructions about the training phase.\\nAn example of these instructions is given Table 5. These training instructions are the same across\\ntasks, only the task name in the instructions is changed.\\n17'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 18,\n",
       "  'text': 'C.2 Task Instructions\\nDuring training and annotation for each task, we provide workers with brief instructions tailored to\\nthe task. We also link workers to an FAQ page for the task. Tables 6, 7, 8, and 9, show the instructions\\nwe used for all four tasks: COPA, CommitmentBank, WSC, and BoolQ respectively. The instructions\\ngiven to crowd workers for annotations on the diagnostic and bias diagnostic datasets are shown in\\nTable 11.\\nWe collected data to produce conservative estimates for human performance on several tasks that\\nwe did not ultimately include in our benchmark, including GAP (Webster et al., 2018), PAWS\\n(Zhang et al., 2019), Quora Insincere Questions,11 Ultraﬁne Entity Typing (Choi et al., 2018b), and\\nEmpathetic Reactions datasets (Buechel et al., 2018). The instructions we used for these tasks are\\nshown in Tables 12, 13, 14, 15, and 16.\\nC.3 Task Speciﬁc Details\\nFor WSC and COPA we provide annotators with a two way classiﬁcation problem. We then use\\nmajority vote across annotations to calculate human performance.\\nCommitmentBank We follow the authors in providing annotators with a 7-way classiﬁcation\\nproblem. We then collapse the annotations into 3 classes by using the same ranges for bucketing used\\nby De Marneffe et al. (2019). We then use majority vote to get human performance numbers on the\\ntask.\\nFurthermore, for training on CommitmentBank we randomly sample examples from the low inter-\\nannotator agreement portion of the CommitmentBank data that is not included in the benchmark\\nversion of the task. These low agreement examples are generally harder to classify since they are\\nmore ambiguous.\\nDiagnostic Dataset Since the diagnostic dataset does not come with accompanying training data,\\nwe train our workers on examples from RTE’s development set. RTE is also a textual entailment\\ntask and is the most closely related task in the main benchmark. Providing the crowd workers with\\ntraining on RTE enables them to learn label deﬁnitions which should generalize to the diagnostic\\ndataset.\\nUltraﬁne Entity Typing We cast the task into a binary classiﬁcation problem to make it an easier\\ntask for non-expert crowd workers. We work in cooperation with the authors of the dataset (Choi\\net al., 2018b) to do this reformulation: We give workers one possible tag for a word or phrase and\\nasked them to classify the tag as being applicable or not.\\nThe authors used WordNet (Miller, 1995) to expand the set of labels to include synonyms and\\nhypernyms from WordNet. They then asked ﬁve annotators to validate these tags. The tags from this\\nvalidation had high agreement, and were included in the publicly available Ultraﬁne Entity Typing\\ndataset,12 This constitutes our set of positive examples. The rest of the tags from the validation\\nprocedure that are not in the public dataset constitute our negative examples.\\nGAP For the Gendered Ambiguous Pronoun Coreference task (GAP, Webster et al., 2018), we\\nsimpliﬁed the task by providing noun phrase spans as part of the input, thus reducing the original\\nstructure prediction task to a classiﬁcation task. This task was presented to crowd workers as a three\\nway classiﬁcation problem: Choose span A, B, or neither.\\nD Excluded Tasks\\nIn this section we provide some examples of tasks that we evaluated for inclusion but ultimately could\\nnot include. We report on these excluded tasks only with the permission of their authors. We turned\\ndown many medical text datasets because they are usually only accessible with explicit permission\\nand credentials from the data owners.\\n11https://www.kaggle.com/c/quora-insincere-questions-classification/data\\n12https://homes.cs.washington.edu/~eunsol/open_entity.html\\n18'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 19,\n",
       "  'text': 'Tasks like QuAC (Choi et al., 2018a) and STREUSLE (Schneider and Smith, 2015) differed substan-\\ntially from the format of other tasks in our benchmark, which we worried would incentivize users\\nto spend signiﬁcant effort on task-speciﬁc model designs, rather than focusing on general-purpose\\ntechniques. It was challenging to train annotators to do well on Quora Insincere Questions 13, Empa-\\nthetic Reactions (Buechel et al., 2018), and a recast version of Ultra-Fine Entity Typing (Choi et al.,\\n2018b, see Appendix C.3 for details), leading to low human performance. BERT achieved very high\\nor superhuman performance on Query Well-Formedness (Faruqui and Das, 2018), PAWS (Zhang\\net al., 2019), Discovering Ongoing Conversations (Zanzotto and Ferrone, 2017), and GAP (Webster\\net al., 2018).\\nDuring the process of selecting tasks for our benchmark, we collected human performance baselines\\nand run BERT-based machine baselines for some tasks that we ultimately excluded from our task\\nlist. We chose to exclude these tasks because our BERT baseline performs better than our human\\nperformance baseline or if the gap between human and machine performance is small.\\nOn Quora Insincere Questions our BERT baseline outperforms our human baseline by a small margin:\\nan F1 score of 67.2 versus 66.7 for BERT and human baselines respectively. Similarly, on the\\nEmpathetic Reactions dataset, BERT outperforms our human baseline, where BERT’s predictions\\nhave a Pearson correlation of 0.45 on empathy and 0.55 on distress, compared to 0.45 and 0.35 for\\nour human baseline. For PAWS-Wiki, we report that BERT achieves an accuracy of 91.9%, while our\\nhuman baseline achieved 84% accuracy. These three tasks are excluded from the benchmark since\\nour, admittedly conservative, human baselines are worse than machine performance. Our human\\nperformance baselines are subject to the clarity of our instructions (all instructions can be found in\\nAppendix C), and crowd workers engagement and ability.\\nFor the Query Well-Formedness task, the authors set an estimate human performance at 88.4%\\naccuracy. Our BERT baseline model reaches an accuracy of 82.3%. While there is a positive gap on\\nthis task, the gap was smaller than we were were willing to tolerate. Similarly, on our recast version\\nof the Ultraﬁne Entity Typing, we observe too small a gap between human (60.2 F1) and machine\\nperformance (55.0 F1). Our recasting for this task is described in Appendix C.2. On GAP, when\\ntaken as a classiﬁcation problem without the related task of span selection (details in C.2), BERT\\nperforms (91.0 F1) comparably to our human baseline (94.9 F1). Given this small margin, we also\\nexclude GAP.\\nOn Discovering Ongoing Conversations, our BERT baseline achieves an F1 of 51.9 on a version of\\nthe task cast as sentence pair classiﬁcation (given two snippets of texts from plays, determine if the\\nsecond snippet is a continuation of the ﬁrst). This dataset is very class imbalanced (90% negative), so\\nwe also experimented with a class-balanced version on which our BERT baselines achieves 88.4\\nF1. Qualitatively, we also found the task challenging for humans as there was little context for the\\ntext snippets and the examples were drawn from plays using early English. Given this fairly high\\nmachine performance and challenging nature for humans, we exclude this task from our benchmark.\\nInstructions tables begin on the following page.\\n13https://www.kaggle.com/c/quora-insincere-questions-classification/data\\n19'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 20,\n",
       "  'text': 'Table 5: The instructions given to crowd-sourced worker describing the training phase for the Choice\\nof Plausible Answers (COPA) task.\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nThis project is a training task that needs to be completed before working on the main project\\non AMT named Human Performance: Plausible Answer. Once you are done with the training,\\nplease proceed to the main task! The qualiﬁcation approval is not immediate but we will add\\nyou to our qualiﬁed workers list within a day.\\nIn this training, you must answer the question on the page and then, to see how you did, click\\nthe Check Work button at the bottom of the page before hitting Submit. The Check Work\\nbutton will reveal the true label. Please use this training and the provided answers to build\\nan understanding of what the answers to these questions look like (the main project, Human\\nPerformance: Plausible Answer, does not have the answers on the page).\\nTable 6: Task-speciﬁc instructions for Choice of Plausible Alternatives (COPA). These instructions\\nwere provided during both training and annotation phases.\\nPlausible Answer Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will present you with a prompt sentence and a question. The question will either be about\\nwhat caused the situation described in the prompt, or what a possible effect of that situation is.\\nWe will also give you two possible answers to this question. Your job is to decide, given the\\nsituation described in the prompt, which of the two options is a more plausible answer to the\\nquestion:\\nIn the following example, option 1. is a more plausible answer to the question about what caused\\nthe situation described in the prompt,\\nThe girl received a trophy.\\nWhat’s the CAUSE for this?\\n1. She won a spelling bee.\\n2. She made a new friend.\\nIn the following example, option2. is a more plausible answer the question about what happened\\nbecause of the situation described in the prompt,\\nThe police aimed their weapons at the fugitive.\\nWhat happened as a RESULT?\\n1. The fugitive fell to the ground.\\n2. The fugitive dropped his gun.\\nIf you have any more questions, please refer to our FAQpage.\\n20'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 21,\n",
       "  'text': 'Table 7: Task-speciﬁc instructions for Commitment Bank. These instructions were provided during\\nboth training and annotation phases.\\nSpeaker Commitment Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will present you with a prompt taken from a piece of dialogue, this could be a single sentence,\\na few sentences, or a short exchange between people. Your job is to ﬁgure out, based on this\\nﬁrst prompt (on top), how certain the speaker is about the truthfulness of the second prompt\\n(on the bottom). You can choose from a 7 point scale ranging from (1) completely certain that\\nthe second prompt is true to (7) completely certain that the second prompt is false. Here are\\nexamples for a few of the labels:\\nChoose 1 (certain that it is true) if the speaker from the ﬁrst prompt deﬁnitely believes or knows\\nthat the second prompt is true. For example,\\n\"What fun to hear Artemis laugh. She’s such a serious child. I didn’t know\\nshe had a sense of humor.\"\\n\"Artemis had a sense of humor\"\\nChoose 4 (not certain if it is true or false) if the speaker from the ﬁrst prompt is uncertain if the\\nsecond prompt is true or false. For example,\\n\"Tess is committed to track. She’s always trained with all her heart and soul.\\nOne can only hope that she has recovered from the ﬂu and will cross the ﬁnish\\nline.\"\\n\"Tess crossed the ﬁnish line.\"\\nChoose 7 (certain that it is false) if the speaker from the ﬁrst prompt deﬁnitely believes or knows\\nthat the second prompt is false. For example,\\n\"Did you hear about Olivia’s chemistry test? She studied really hard. But\\neven after putting in all that time and energy, she didn’t manage to pass the\\ntest\".\\n\"Olivia passed the test.\"\\nIf you have any more questions, please refer to our FAQpage.\\n21'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 22,\n",
       "  'text': 'Table 8: Task-speciﬁc instructions for Winograd Schema Challenge (WSC). These instructions were\\nprovided during both training and annotation phases.\\nWinograd Schema Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will present you with a sentence that someone wrote, with one bolded pronoun. We will then\\nask if you if the pronoun refers to a speciﬁc word or phrase in the sentence. Your job is to ﬁgure\\nout, based on the sentence, if the bolded pronoun refers to this selected word or phrase:\\nChoose Yes if the pronoun refers to the selected word or phrase. For example,\\n\"I put the cake away in the refrigerator. It has a lot of butter in it.\"\\nDoes It in \"It has a lot\" refer to cake?\\nChoose No if the pronoun does not refer to the selected word or phrase. For example,\\n\"The large ball crashed right through the table because it was made of\\nstyrofoam.\"\\nDoes it in \"it was made\" refer to ball?\\nIf you have any more questions, please refer to our FAQpage.\\n22'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 23,\n",
       "  'text': 'Table 9: Task-speciﬁc instructions for BoolQ (continued in Table 10). These instructions were\\nprovided during both training and annotation phases.\\nQuestion-Answering Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will present you with a passage taken from a Wikipedia article and a relevant question. Your\\njob is to decide, given the information provided in the passage, if the answer to the question is\\nYes or No. For example,\\nIn the following examples the correct answer is Yes,\\nThe thirteenth season of Criminal Minds was ordered on April 7, 2017, by\\nCBS with an order of 22 episodes. The season premiered on September 27,\\n2017 in a new time slot at 10:00PM on Wednesday when it had previously\\nbeen at 9:00PM on Wednesday since its inception. The season concluded on\\nApril 18, 2018 with a two-part season ﬁnale.\\nwill there be a 13th season of criminal minds?\\n(In the above example, the ﬁrst line of the passage says that the 13th season of\\nthe show was ordered.)\\nAs of 8 August 2016, the FDA extended its regulatory power to include e-\\ncigarettes. Under this ruling the FDA will evaluate certain issues, including\\ningredients, product features and health risks, as well their appeal to minors\\nand non-users. The FDA rule also bans access to minors. A photo ID is\\nrequired to buy e-cigarettes, and their sale in all-ages vending machines is not\\npermitted. The FDA in September 2016 has sent warning letters for unlawful\\nunderage sales to online retailers and retailers of e-cigarettes.\\nis vaping illegal if you are under 18?\\n(In the above example, the passage states that the \"FDA rule also bans access\\nto minors.\" The question uses the word \"vaping,\" which is a synonym for\\ne-cigrattes.)\\nIn the following examples the correct answer is No,\\nBadgers are short-legged omnivores in the family Mustelidae, which also\\nincludes the otters, polecats, weasels, and wolverines. They belong to the\\ncaniform suborder of carnivoran mammals. The 11 species of badgers are\\ngrouped in three subfamilies: Melinae (Eurasian badgers), Mellivorinae (the\\nhoney badger or ratel), and Taxideinae (the American badger). The Asiatic\\nstink badgers of the genus Mydaus were formerly included within Melinae\\n(and thus Mustelidae), but recent genetic evidence indicates these are actually\\nmembers of the skunk family, placing them in the taxonomic family Mephitidae.\\nis a wolverine the same as a badger?\\n(In the above example, the passage says that badgers and wolverines are in\\nthe same family, Mustelidae, which does not mean they are the same animal.)\\n23'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 24,\n",
       "  'text': 'Table 10: Continuation from Table 9 of task-speciﬁc instructions for BoolQ. These instructions were\\nprovided during both training and annotation phases.\\nMore famously, Harley-Davidson attempted to register as a trademark the\\ndistinctive “chug” of a Harley-Davidson motorcycle engine. On February\\n1, 1994, the company ﬁled its application with the following description:\\n“The mark consists of the exhaust sound of applicant’s motorcycles, produced\\nby V-twin, common crankpin motorcycle engines when the goods are in use. ”\\nNine of Harley-Davidson’s competitors ﬁled oppositions against the applica-\\ntion, arguing that cruiser-style motorcycles of various brands use the same\\ncrankpin V-twin engine which produces the same sound. After six years of\\nlitigation, with no end in sight, in early 2000, Harley-Davidson withdrew their\\napplication.\\ndoes harley davidson have a patent on their sound?\\n(In the above example, the passage states that Harley-Davidson applied for a\\npatent but then withdrew, so they do not have a patent on the sound.)\\nIf you have any more questions, please refer to our FAQpage.\\n24'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 25,\n",
       "  'text': 'Table 11: Task-speciﬁc instructions for the diagnostic and the bias diagnostic datasets. These\\ninstructions were provided during both training and annotation phases.\\nTextual Entailment Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will present you with a prompt taken from an article someone wrote. Your job is to ﬁgure out,\\nbased on this correct prompt (the ﬁrst prompt, on top), if another prompt (the second prompt, on\\nbottom) is also necessarily true:\\nChoose True if the event or situation described by the ﬁrst prompt deﬁnitely implies that the\\nsecond prompt, on bottom, must also be true. For example,\\n• \"Murphy recently decided to move to London.\"\\n\"Murphy recently decided to move to England.\"\\n(The above example is True because London is in England and therefore prompt 2 is\\nclearly implied by prompt 1.)\\n• \"Russian cosmonaut Valery Polyakov set the record for the longest continuous amount\\nof time spent in space, a staggering 438 days, between 1994 and 1995.\"\\n\"Russians hold record for longest stay in space.\"\\n(The above example is True because the information in the second prompt is contained\\nin the ﬁrst prompt: Valery is Russian and she set the record for longest stay in space.)\\n• \"She does not disgree with her brother’s opinion, but she believes he’s too aggresive in\\nhis defense\"\\n\"She agrees with her brother’s opinion, but she believes he’s too aggresive in his\\ndefense\"\\n(The above example is True because the second prompt is an exact paraphrase of the\\nﬁrst prompt, with exactly the same meaning.)\\nChoose False if the event or situation described with the ﬁrst prompt on top does not necessarily\\nimply that this second prompt must also be true. For example,\\n• \"This method was developed at Columbia and applied to data processing at CERN.\"\\n\"This method was developed at Columbia and applied to data processing at CERN\\nwith limited success.\"\\n(The above example is False because the second prompt is introducing new information\\nnot implied in the ﬁrst prompt: The ﬁrst prompt does not give us any knowledge of\\nhow succesful the application of the method at CERN was.)\\n• \"This building is very tall.\"\\n\"This is the tallest building in New York.\"\\n(The above example is False because a building being tall does not mean it must be the\\ntallest building, nor that it is in New York.)\\n• \"Hours earlier, Yasser Arafat called for an end to attacks against Israeli civilians in\\nthe two weeks before Israeli elections.\"\\n\"Arafat condemned suicide bomb attacks inside Israel.\"\\n(The above example is False because from the ﬁrst prompt we only know that Arafat\\ncalled for an end to attacks against Israeli citizens, we do not know what kind of attacks\\nhe may have been condemning.)\\nYou do not have to worry about whether the writing style is maintained between the two prompts.\\nIf you have any more questions, please refer to our FAQpage.\\n25'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 26,\n",
       "  'text': 'Table 12: Task-speciﬁc instructions for the Gendered Ambiguous Pronoun Coreference (GAP) task.\\nThese instructions were provided during both training and annotation phases.\\nGAP Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will present you with an extract from a Wikipedia article, with one bolded pronoun. We will\\nalso give you two names from the text that this pronoun could refer to. Your job is to ﬁgure out,\\nbased on the extract, if the pronoun refers to option A, options B, or neither:\\nChoose A if the pronoun refers to option A. For example,\\n\"In 2010 Ella Kabambe was not the ofﬁcial Miss Malawi; this was Faith\\nChibale, but Kabambe represented the country in the Miss World pageant.\\nAt the 2012 Miss World, Susan Mtegha pushed Miss New Zealand, Collette\\nLochore, during the opening headshot of the pageant, claiming that Miss New\\nZealand was in her space.\"\\nDoes her refer to option A or B below?\\nA Susan Mtegha\\nB Collette Lochore\\nC Neither\\nChoose B if the pronoun refers to option B. For example,\\n\"In 1650 he started his career as advisor in the ministerium of ﬁnances in Den\\nHaag. After he became a minister he went back to Amsterdam, and took place\\nas a sort of chairing mayor of this city. After the death of his brother Cornelis,\\nDe Graeff became the strong leader of the republicans. He held this position\\nuntil the rampjaar.\"\\nDoes He refer to option A or B below?\\nA Cornelis\\nB De Graeff\\nC Neither\\nChoose C if the pronoun refers to neither option. For example,\\n\"Reb Chaim Yaakov’s wife is the sister of Rabbi Moishe Sternbuch, as is\\nthe wife of Rabbi Meshulam Dovid Soloveitchik, making the two Rabbis his\\nuncles. Reb Asher’s brother Rabbi Shlomo Arieli is the author of a critical\\nedition of the novallae of Rabbi Akiva Eiger. Before his marriage, Rabbi Arieli\\nstudied in the Ponevezh Yeshiva headed by Rabbi Shmuel Rozovsky, and he\\nlater studied under his father-in-law in the Mirrer Yeshiva.\"\\nDoes his refer to option A or B below?\\nA Reb Asher\\nB Akiva Eiger\\nC Neither\\nIf you have any more questions, please refer to our FAQpage.\\n26'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 27,\n",
       "  'text': 'Table 13: Task-speciﬁc instructions for the Paraphrase Adversaries from Word Scrambling (PAWS)\\ntask. These instructions were provided during both training and annotation phases.\\nParaphrase Detection Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will present you with two similar sentences taken from Wikipedia articles. Your job is to\\nﬁgure out if these two sentences are paraphrases of each other, and convey exactly the same\\nmeaning:\\nChoose Yes if the sentences are paraphrases and have the exact same meaning. For example,\\n\"Hastings Ndlovu was buried with Hector Pieterson at Avalon Cemetery in\\nJohannesburg.\"\\n\"Hastings Ndlovu , together with Hector Pieterson , was buried at the Avalon\\ncemetery in Johannesburg .\"\\n\"The complex of the Trabzon World Trade Center is close to Trabzon Airport\\n.\"\\n\"The complex of World Trade Center Trabzon is situated close to Trabzon\\nAirport .\"\\nChoose No if the two sentences are not exact paraphrases and mean different things. For\\nexample,\\n\"She was only a few months in French service when she met some British\\nfrigates in 1809 .\"\\n\"She was only in British service for a few months , when in 1809 , she\\nencountered some French frigates .\"\\n\"This work caused him to trigger important reﬂections on the practices of\\nmolecular genetics and genomics at a time when this was not considered\\nethical .\"\\n\"This work led him to trigger ethical reﬂections on the practices of molecular\\ngenetics and genomics at a time when this was not considered important .\"\\nIf you have any more questions, please refer to our FAQpage.\\n27'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 28,\n",
       "  'text': 'Table 14: Task-speciﬁc instructions for the Quora Insincere Questions task. These instructions were\\nprovided during both training and annotation phases.\\nInsincere Questions Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will present you with a question that someone posted on Quora. Your job is to ﬁgure out\\nwhether or not this is a sincere question. An insincere question is deﬁned as a question intended\\nto make a statement rather than look for helpful answers. Some characteristics that can signify\\nthat a question is insincere:\\n• Has a non-neutral tone\\n– Has an exaggerated tone to underscore a point about a group of people\\n– Is rhetorical and meant to imply a statement about a group of people\\n• Is disparaging or inﬂammatory\\n– Suggests a discriminatory idea against a protected class of people, or seeks\\nconﬁrmation of a stereotype\\n– Makes disparaging attacks/insults against a speciﬁc person or group of people\\n– Based on an outlandish premise about a group of people\\n– Disparages against a characteristic that is not ﬁxable and not measurable\\n• Isn’t grounded in reality\\n– Based on false information, or contains absurd assumptions\\n– Uses sexual content (incest, bestiality, pedophilia) for shock value, and not to seek\\ngenuine answers\\nPlease note that there are far fewer insincere questions than there are sincere questions! So you\\nshould expect to label most questions as sincere.\\nExamples,\\nChoose Sincere if you believe the person asking the question was genuinely seeking an answer\\nfrom the forum. For example,\\n\"How do DNA and RNA compare and contrast?\"\\n\"Are there any sports that you don’t like?\"\\n\"What is the main purpose of penance?\"\\nChoose Insincere if you believe the person asking the question was not really seeking an answer\\nbut was being inﬂammatory, extremely rhetorical, or absurd. For example,\\n\"How do I sell Pakistan? I need lots of money so I decided to sell Pakistan\\nany one wanna buy?\"\\n\"If Hispanics are so proud of their countries, why do they move out?\"\\n\"Why Chinese people are always not welcome in all countries?\"\\nIf you have any more questions, please refer to our FAQpage.\\n28'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 29,\n",
       "  'text': 'Table 15: Task-speciﬁc instructions for the Ultraﬁne Entity Typing task. These instructions were\\nprovided during both training and annotation phases.\\nEntity Typing Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will provide you with a sentence with on bolded word or phrase. We will also give you a\\npossible tag for this bolded word or phrase. Your job is to decide, in the context of the sentence,\\nif this tag is correct and applicable to the bolded word or phrase:\\nChoose Yes if the tag is applicable and accurately describes the selected word or phrase. For\\nexample,\\n“Spain was the gold line.\" It started out with zero gold in 1937, and by 1945\\nit had 65.5 tons.\\nTag: nation\\nChoose No if the tag is not applicable and does not describes the selected word or phrase. For\\nexample,\\nIraqi museum workersare starting to assess the damage to Iraq’s history.\\nTag: organism\\nIf you have any more questions, please refer to our FAQpage.\\n29'},\n",
       " {'file_type': 'pdf',\n",
       "  'file_name': '../data/input/full/references/superglue.pdf',\n",
       "  'marker': 30,\n",
       "  'text': 'Table 16: Task-speciﬁc instructions for the Empathetic Reaction task. These instructions were\\nprovided during both training and annotation phases.\\nEmpathy and Distress Analysis Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will present you with a message someone wrote after reading an article. Your job is to ﬁgure\\nout, based on this message, how disressed and empathetic the author was feeling. Empathy is\\ndeﬁned as feeling warm, tender, sympathetic, moved, or compassionate. Distressed is deﬁned as\\nfeeling worried, upset, troubled, perturbed, grieved, distrubed, or alarmed.\\nExamples,\\nThe author of the following message was not feeling empathetic at all with anempathy score of 1,\\nand was very distressed with a distress score of 7,\\n\"I really hate ISIS. They continue to be the stain on society by committing\\natrocities condemned by every nation in the world. They must be stopped at\\nall costs and they must be destroyed so that they wont hurt another soul. These\\npoor people who are trying to survive get killed, imprisoned, or brainwashed\\ninto joining and there seems to be no way to stop them.\"\\nThe author of the following message is feeling very empathetic with an empathy score of 7 and\\nalso very distressed with a distress score of 7,\\n\"All of you know that I love birds. This article was hard for me to read because\\nof that. Wind turbines are killing a lot of birds, including eagles. It’s really\\nvery sad. It makes me feel awful. I am all for wind turbines and renewable\\nsources of energy because of global warming and coal, but this is awful. I\\ndon’t want these poor birds to die like this. Read this article and you’ll see\\nwhy.\"\\nThe author of the following message is feeling moderately empathetic with an\\nempathy score of 4 and moderately distressed with a distress score of 4,\\n\"I just read an article about wild ﬁres sending a smokey haze across the state\\nnear the Appalachian mountains. Can you imagine how big the ﬁre must be\\nto spread so far and wide? And the people in the area obviously suffer the\\nmost. What if you have asthma or some other condition that restricts your\\nbreathing?\"\\nThe author of the following message is feeling very empathetic with an empathy score of 7 and\\nmildly distressed with a distress score of 2,\\n\"This is a very sad article. Being of of the ﬁrst female ﬁghter pilots must\\nhave given her and her family great honor. I think that there should be more\\ntraining for all pilots who deal in these acrobatic ﬂying routines. I also think\\nthat women have just as much of a right to become a ﬁghter pilot as men.\"\\nIf you have any more questions, please refer to our FAQpage.\\n30'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd846cb2-8318-45da-8e3a-3eac49f1438e",
   "metadata": {},
   "source": [
    "#### HTMLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e928877-f904-48eb-b6cb-d2f885c1a1bf",
   "metadata": {},
   "source": [
    "For webpages listed as references, we used boilerpy to remove some of the boilerplate headers and css and js. We then tokenize it at a sentence level.\n",
    "\n",
    "For metadata, we include the file name and the sentence number to refer back to the source knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5864d066-bfc7-47b0-8ac9-b530b90ddd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from boilerpy3 import extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88444bcb-52da-4b75-9c18-aaa75e59a54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_from_html(file_path):\n",
    "    # Use ArticleExtractor from boilerpy3\n",
    "    extractor = extractors.ArticleExtractor()\n",
    "    data = []\n",
    "    try:\n",
    "        clean_content = extractor.get_content_from_file(html_doc_path)\n",
    "        clean_content = nltk.sent_tokenize(clean_content)\n",
    "        # data.append({\n",
    "        #     \"file_type\": \"html\",\n",
    "        #     \"file_name\": file_path,\n",
    "        #     \"text\": \" \".join(clean_content),\n",
    "        #     \"marker\": None\n",
    "        # })\n",
    "        seq_num = 0\n",
    "        for content in clean_content:\n",
    "            data.append({\n",
    "                \"file_type\": \"html\",\n",
    "                \"file_name\": file_path,\n",
    "                \"text\": content,\n",
    "                \"marker\": seq_num\n",
    "            })\n",
    "            seq_num += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with BoilerPy3 extraction: {e}\")\n",
    "    finally:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "968159a9-efa5-454d-a83c-ba4d5fb70be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc_path = \"../data/input/full/references/Generation with LLMs.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "633d1200-9966-4afc-9ad9-71bb5c4fbb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_data = read_text_from_html(html_doc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d446cb94-033e-468e-84f7-f84c2f85abe4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'Graph models\\nInternal Helpers\\nCustom Layers and Utilities Utilities for pipelines Utilities for Tokenizers Utilities for Trainer Utilities for Generation Utilities for Image Processors Utilities for Audio processing General Utilities Utilities for Time Series\\nGeneration with LLMs\\nLLMs, or Large Language Models, are the key component behind text generation.',\n",
       "  'marker': 0},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'In a nutshell, they consist of large pretrained transformer models trained to predict the next word (or, more precisely, token) given some input text.',\n",
       "  'marker': 1},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'Since they predict one token at a time, you need to do something more elaborate to generate new sentences other than just calling the model — you need to do autoregressive generation.',\n",
       "  'marker': 2},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'Autoregressive generation is the inference-time procedure of iteratively calling a model with its own generated outputs, given a few initial inputs.',\n",
       "  'marker': 3},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'In 🤗 Transformers, this is handled by the generate() method, which is available to all models with generative capabilities.',\n",
       "  'marker': 4},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'This tutorial will show you how to:\\nGenerate text with an LLM\\nAvoid common pitfalls\\nNext steps to help you get the most out of your LLM\\nBefore you begin, make sure you have all the necessary libraries installed:\\nCopied\\npip install transformers bitsandbytes>=0.39.0 -q\\nGenerate text\\nA language model trained for causal language modeling takes a sequence of text tokens as input and returns the probability distribution for the next token.',\n",
       "  'marker': 5},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': '\"Forward pass of an LLM\"\\nA critical aspect of autoregressive generation with LLMs is how to select the next token from this probability distribution.',\n",
       "  'marker': 6},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'Anything goes in this step as long as you end up with a token for the next iteration.',\n",
       "  'marker': 7},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'This means it can be as simple as selecting the most likely token from the probability distribution or as complex as applying a dozen transformations before sampling from the resulting distribution.',\n",
       "  'marker': 8},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': '\"Autoregressive generation iteratively selects the next token from a probability distribution to generate text\"\\nThe process depicted above is repeated iteratively until some stopping condition is reached.',\n",
       "  'marker': 9},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'Ideally, the stopping condition is dictated by the model, which should learn when to output an end-of-sequence (EOS) token.',\n",
       "  'marker': 10},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'If this is not the case, generation stops when some predefined maximum length is reached.',\n",
       "  'marker': 11},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'Properly setting up the token selection step and the stopping condition is essential to make your model behave as you’d expect on your task.',\n",
       "  'marker': 12},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'That is why we have a GenerationConfig file associated with each model, which contains a good default generative parameterization and is loaded alongside your model.',\n",
       "  'marker': 13},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'Let’s talk code!',\n",
       "  'marker': 14},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'If you’re interested in basic LLM usage, our high-level Pipeline interface is a great starting point.',\n",
       "  'marker': 15},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'However, LLMs often require advanced features like quantization and fine control of the token selection step, which is best done through generate() .',\n",
       "  'marker': 16},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'Autoregressive generation with LLMs is also resource-intensive and should be executed on a GPU for adequate throughput.',\n",
       "  'marker': 17},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'First, you need to load the model.',\n",
       "  'marker': 18},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'Copied\\n>>> from transformers import AutoModelForCausalLM >>> model = AutoModelForCausalLM.from_pretrained( ... \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", load_in_4bit=True ... )\\nYou’ll notice two flags in the from_pretrained call:\\ndevice_map ensures the model is moved to your GPU(s)\\nload_in_4bit applies 4-bit dynamic quantization to massively reduce the resource requirements\\nThere are other ways to initialize a model, but this is a good baseline to begin with an LLM.',\n",
       "  'marker': 19},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'Next, you need to preprocess your text input with a tokenizer .',\n",
       "  'marker': 20},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'Copied\\n>>> from transformers import AutoTokenizer >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\") >>> model_inputs = tokenizer([\"A list of colors: red, blue\"], return_tensors=\"pt\").to(\"cuda\")\\nThe model_inputs variable holds the tokenized text input, as well as the attention mask.',\n",
       "  'marker': 21},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'While generate() does its best effort to infer the attention mask when it is not passed, we recommend passing it whenever possible for optimal results.',\n",
       "  'marker': 22},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'After tokenizing the inputs, you can call the generate() method to returns the generated tokens.',\n",
       "  'marker': 23},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'The generated tokens then should be converted to text before printing.',\n",
       "  'marker': 24},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': \"Copied\\n>>> generated_ids = model.generate(**model_inputs) >>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] 'A list of colors: red, blue, green, yellow, orange, purple, pink,'\\nFinally, you don’t need to do it one sequence at a time!\",\n",
       "  'marker': 25},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'You can batch your inputs, which will greatly improve the throughput at a small latency and memory cost.',\n",
       "  'marker': 26},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'All you need to do is to make sure you pad your inputs properly (more on that below).',\n",
       "  'marker': 27},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'Copied\\n>>> tokenizer.pad_token = tokenizer.eos_token # Most LLMs don\\'t have a pad token by default >>> model_inputs = tokenizer( ... [\"A list of colors: red, blue\", \"Portugal is\"], return_tensors=\"pt\", padding=True ... ).to(\"cuda\") >>> generated_ids = model.generate(**model_inputs) >>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\\n[\\'A list of colors: red, blue, green, yellow, orange, purple, pink,\\', \\'Portugal is a country in southwestern Europe, on the Iber\\']\\nAnd that’s it!',\n",
       "  'marker': 28},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'In a few lines of code, you can harness the power of an LLM.',\n",
       "  'marker': 29},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'Common pitfalls\\nThere are many generation strategies , and sometimes the default values may not be appropriate for your use case.',\n",
       "  'marker': 30},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'If your outputs aren’t aligned with what you’re expecting, we’ve created a list of the most common pitfalls and how to avoid them.',\n",
       "  'marker': 31},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'Copied\\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\") >>> tokenizer.pad_token = tokenizer.eos_token # Most LLMs don\\'t have a pad token by default >>> model = AutoModelForCausalLM.from_pretrained( ... \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", load_in_4bit=True ... )\\nGenerated output is too short/long\\nIf not specified in the GenerationConfig file, generate returns up to 20 tokens by default.',\n",
       "  'marker': 32},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'We highly recommend manually setting max_new_tokens in your generate call to control the maximum number of new tokens it can return.',\n",
       "  'marker': 33},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'Keep in mind LLMs (more precisely, decoder-only models ) also return the input prompt as part of the output.',\n",
       "  'marker': 34},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'Copied\\n>>> model_inputs = tokenizer([\"A sequence of numbers: 1, 2\"], return_tensors=\"pt\").to(\"cuda\") >>> # By default, the output will contain up to 20 tokens >>> generated_ids = model.generate(**model_inputs) >>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] \\'A sequence of numbers: 1, 2, 3, 4, 5\\' >>> # Setting `max_new_tokens` allows you to control the maximum length >>> generated_ids = model.generate(**model_inputs, max_new_tokens=50) >>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] \\'A sequence of numbers: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,\\'\\nIncorrect generation mode\\nBy default, and unless specified in the GenerationConfig file, generate selects the most likely token at each iteration (greedy decoding).',\n",
       "  'marker': 35},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'Depending on your task, this may be undesirable; creative tasks like chatbots or writing an essay benefit from sampling.',\n",
       "  'marker': 36},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'On the other hand, input-grounded tasks like audio transcription or translation benefit from greedy decoding.',\n",
       "  'marker': 37},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'Enable sampling with do_sample=True, and you can learn more about this topic in this blog post .',\n",
       "  'marker': 38},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'Copied\\n>>> # Set seed for reproducibility -- you don\\'t need this unless you want full reproducibility >>> from transformers import set_seed >>> set_seed(42) >>> model_inputs = tokenizer([\"I am a cat.',\n",
       "  'marker': 39},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': '\"], return_tensors=\"pt\").to(\"cuda\") >>> # LLM + greedy decoding = repetitive, boring output >>> generated_ids = model.generate(**model_inputs) >>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] \\'I am a cat.',\n",
       "  'marker': 40},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'I am a cat.',\n",
       "  'marker': 41},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'I am a cat.',\n",
       "  'marker': 42},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': \"I am a cat' >>> # With sampling, the output becomes more creative!\",\n",
       "  'marker': 43},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': \">>> generated_ids = model.generate(**model_inputs, do_sample=True) >>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] 'I am a cat.\",\n",
       "  'marker': 44},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'Specifically, I am an indoor-only cat.',\n",
       "  'marker': 45},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': \"I'\\nWrong padding side\\nLLMs are decoder-only architectures, meaning they continue to iterate on your input prompt.\",\n",
       "  'marker': 46},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'If your inputs do not have the same length, they need to be padded.',\n",
       "  'marker': 47},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'Since LLMs are not trained to continue from pad tokens, your input needs to be left-padded.',\n",
       "  'marker': 48},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'Make sure you also don’t forget to pass the attention mask to generate!',\n",
       "  'marker': 49},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'Copied\\n>>> # The tokenizer initialized above has right-padding active by default: the 1st sequence, >>> # which is shorter, has padding on the right side.',\n",
       "  'marker': 50},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'Generation fails to capture the logic.',\n",
       "  'marker': 51},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': '>>> model_inputs = tokenizer( ... [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\" ... ).to(\"cuda\") >>> generated_ids = model.generate(**model_inputs) >>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] \\'1, 2, 33333333333\\' >>> # With left-padding, it works as expected!',\n",
       "  'marker': 52},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': '>>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\") >>> tokenizer.pad_token = tokenizer.eos_token # Most LLMs don\\'t have a pad token by default >>> model_inputs = tokenizer( ... [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\" ... ).to(\"cuda\") >>> generated_ids = model.generate(**model_inputs) >>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] \\'1, 2, 3, 4, 5, 6,\\'\\nWrong prompt\\nSome models and tasks expect a certain input prompt format to work properly.',\n",
       "  'marker': 53},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'When this format is not applied, you will get a silent performance degradation: the model kinda works, but not as well as if you were following the expected prompt.',\n",
       "  'marker': 54},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'More information about prompting, including which models and tasks need to be careful, is available in this guide .',\n",
       "  'marker': 55},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'Let’s see an example with a chat LLM, which makes use of chat templating :\\nCopied\\n>>> tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\") >>> model = AutoModelForCausalLM.from_pretrained( ... \"HuggingFaceH4/zephyr-7b-alpha\", device_map=\"auto\", load_in_4bit=True ... ) >>> set_seed(0) >>> prompt = \"\"\"How many helicopters can a human eat in one sitting?',\n",
       "  'marker': 56},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'Reply as a thug.\"\"\"',\n",
       "  'marker': 57},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': '>>> model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\") >>> input_length = model_inputs.input_ids.shape[1] >>> generated_ids = model.generate(**model_inputs, max_new_tokens=20) >>> print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0]) \"I\\'m not a thug, but i can tell you that a human cannot eat\" >>> # Oh no, it did not follow our instruction to reply as a thug!',\n",
       "  'marker': 58},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'Let\\'s see what happens when we write >>> # a better prompt and use the right template for this model (through `tokenizer.apply_chat_template`) >>> set_seed(0) >>> messages = [ ... { ... \"role\": \"system\", ... \"content\": \"You are a friendly chatbot who always responds in the style of a thug\", ... }, ... {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?',\n",
       "  'marker': 59},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': '\"}, ... ] >>> model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\") >>> input_length = model_inputs.shape[1] >>> generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=20) >>> print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0]) \\'None, you thug.',\n",
       "  'marker': 60},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': \"How bout you try to focus on more useful questions?'\",\n",
       "  'marker': 61},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': '>>> # As we can see, it followed a proper thug style 😎\\nFurther resources\\nWhile the autoregressive generation process is relatively straightforward, making the most out of your LLM can be a challenging endeavor because there are many moving parts.',\n",
       "  'marker': 62},\n",
       " {'file_type': 'html',\n",
       "  'file_name': '../data/input/full/references/Generation with LLMs.html',\n",
       "  'text': 'For your next steps to help you dive deeper into LLM usage and understanding:\\nAdvanced generate usage\\nGuide on how to control different generation methods , how to set up the generation configuration file, and how to stream the output;',\n",
       "  'marker': 63}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838b40cc-7bd0-4ea7-9e59-6f05ebf2d01d",
   "metadata": {},
   "source": [
    "#### Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7e87a8-c3cb-4974-8c89-501544947b9a",
   "metadata": {},
   "source": [
    "For notebooks, we inspect contents at a cell level, separating content into code, text and output blocks. While it is true that, in most cases, the output would be largely pointless, sometimes it does contain important statistics or algorithmic run-throughs that can be useful to refer to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a5c4b47-8b49-4208-9bdc-32a63b505e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_code_md_outputs_from_notebook_sequence(file_path):\n",
    "    # Load the notebook file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        notebook = json.load(f)\n",
    "\n",
    "    content = []\n",
    "    seq_num = 0\n",
    "    \n",
    "    # Extract cells\n",
    "    for cell in notebook.get('cells', []):\n",
    "        cell_type = cell.get('cell_type')\n",
    "        if cell_type == 'markdown': # Markdown cell\n",
    "            md_content = \"Text block:\\n\"\n",
    "            md_content += \"\".join(cell.get('source', [])) + \"\\n\"\n",
    "            # print(md_content)\n",
    "            content.append({\n",
    "                \"file_type\": \"ipynb\",\n",
    "                \"file_name\": file_path,\n",
    "                \"marker\": seq_num,\n",
    "                \"text\": md_content\n",
    "            })\n",
    "        elif cell_type == 'code':\n",
    "            code_content = \"Code block:\\n\"\n",
    "            code_content += \"\".join(cell.get('source', [])) + \"\\n\"\n",
    "            # print(code_content)\n",
    "            # code_source = ''.join(code_content)\n",
    "            # outputs = []\n",
    "            code_content += \"Output:\\n\"\n",
    "            \n",
    "            # Extract outputs\n",
    "            for output in cell.get('outputs', []):\n",
    "                if output.get('output_type') == 'stream':\n",
    "                    code_content += \"\".join(output.get('text', [])) + \"\\n\"\n",
    "                    # print(output_content)\n",
    "                    # outputs.append(output_content)\n",
    "                elif output.get('output_type') == 'execute_result':\n",
    "                    code_content += \"\".join(output.get('data', {}).get('text/plain', [])) + \"\\n\"\n",
    "                    # print(output_content)\n",
    "                    # outputs.append(output_content)\n",
    "                elif output.get('output_type') == 'error':\n",
    "                    code_content += \"Error: \".join(output.get('traceback', [])) + \"\\n\"\n",
    "                    # print(output_content)\n",
    "                    # outputs.append('Error: ' + output_content)\n",
    "\n",
    "            content.append({\n",
    "                \"file_type\": \"ipynb\",\n",
    "                \"file_name\": file_path,\n",
    "                \"marker\": seq_num,\n",
    "                \"text\": code_content\n",
    "            })\n",
    "\n",
    "        seq_num += 1\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ea1acc7-8882-4805-840b-e8f96abd8899",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_doc_path = \"../data/input/full/notebooks/bpe.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ded8e49-1c37-48d7-a4c7-95a0d6b63b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_data = read_code_md_outputs_from_notebook_sequence(notebook_doc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b0d6a4a-c2f8-44bb-9a2b-3eb873175ad1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 0,\n",
       "  'text': 'Text block:\\n# Byte-Pair Encoding tokenization\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 1,\n",
       "  'text': 'Text block:\\nBPE training starts by computing the unique set of words used in the corpus (after the normalization and pre-tokenization steps are completed), then building the vocabulary by taking all the symbols used to write those words. As a very simple example, let’s say our corpus uses these five words:\\n\\nThis material was adapted from the Huggingface tutorial available here:\\n\\nhttps://huggingface.co/learn/nlp-course/chapter6/5?fw=pt\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 2,\n",
       "  'text': 'Code block:\\ncorpus = [\"hug\", \"pug\", \"pun\", \"bun\", \"hugs\"]\\nOutput:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 3,\n",
       "  'text': \"Code block:\\nvocab = set([ c for w in corpus for c in w ])\\nprint(vocab)\\nOutput:\\n{'n', 'g', 'h', 'p', 'b', 's', 'u'}\\n\\n\"},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 4,\n",
       "  'text': 'Text block:\\nAfter getting this base vocabulary, we add new tokens until the desired vocabulary size is reached by learning merges, which are rules to merge two elements of the existing vocabulary together into a new one. So, at the beginning these merges will create tokens with two characters, and then, as training progresses, longer subwords.\\n\\nAt any step during the tokenizer training, the BPE algorithm will search for the most frequent pair of existing tokens (by “pair,” here we mean two consecutive tokens in a word). That most frequent pair is the one that will be merged, and we rinse and repeat for the next step.\\n\\nGoing back to our previous example, let’s assume the words had the following frequencies:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 5,\n",
       "  'text': 'Code block:\\ncorpus = [(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)]\\nOutput:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 6,\n",
       "  'text': 'Text block:\\n\"hug\" was present 10 times in the corpus, \"pug\" 5 times, \"pun\" 12 times, \"bun\" 4 times, and \"hugs\" 5 times. We start the training by splitting each word into characters (the ones that form our initial vocabulary) so we can see each word as a list of tokens:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 7,\n",
       "  'text': 'Code block:\\ncorpus = [(\"h\" \"u\" \"g\", 10), (\"p\" \"u\" \"g\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"u\" \"g\" \"s\", 5)]\\nOutput:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 8,\n",
       "  'text': 'Text block:\\nThen we look at pairs. The pair (\"h\", \"u\") is present in the words \"hug\" and \"hugs\", so 15 times total in the corpus. It’s not the most frequent pair, though: the most frequent is (\"u\", \"g\"), which is present in \"hug\", \"pug\", and \"hugs\", for a grand total of 20 times in the vocabulary.\\n\\nThus, the first merge rule learned by the tokenizer is (\"u\", \"g\") -> \"ug\", which means that \"ug\" will be added to the vocabulary, and the pair should be merged in all the words of the corpus. At the end of this stage, the vocabulary and corpus look like this:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 9,\n",
       "  'text': 'Code block:\\nvocab = [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\"]\\ncorpus = [(\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"ug\" \"s\", 5)]\\nOutput:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 10,\n",
       "  'text': 'Text block:\\nNow we have some pairs that result in a token longer than two characters: the pair (\"h\", \"ug\"), for instance (present 15 times in the corpus). The most frequent pair at this stage is (\"u\", \"n\"), however, present 16 times in the corpus, so the second merge rule learned is (\"u\", \"n\") -> \"un\". Adding that to the vocabulary and merging all existing occurrences leads us to:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 11,\n",
       "  'text': 'Code block:\\nvocab = [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\"]\\ncorpus = [(\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"h\" \"ug\" \"s\", 5)]\\nOutput:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 12,\n",
       "  'text': 'Text block:\\nNow the most frequent pair is (\"h\", \"ug\"), so we learn the merge rule (\"h\", \"ug\") -> \"hug\", which gives us our first three-letter token. After the merge, the corpus looks like this:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 13,\n",
       "  'text': 'Code block:\\nvocab = [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"]\\ncorpus = [(\"hug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"hug\" \"s\", 5)]\\nOutput:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 14,\n",
       "  'text': 'Text block:\\nAnd we continue like this until we reach the desired vocabulary size. Usually we provide the number of merges we want to obtain a particular vocabulary size.\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 15,\n",
       "  'text': 'Text block:\\n### Tokenization Algorithm\\n\\nTokenization follows the training process closely, in the sense that new inputs are tokenized by applying the following steps:\\n\\n1. Normalization\\n1. Pre-tokenization\\n1. Splitting the words into individual characters\\n1. Applying the merge rules learned in order on those splits\\n\\nLet’s take the example we used during training, with the three merge rules learned:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 16,\n",
       "  'text': 'Text block:\\n```\\n(\"u\", \"g\") -> \"ug\"\\n(\"u\", \"n\") -> \"un\"\\n(\"h\", \"ug\") -> \"hug\"\\n```\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 17,\n",
       "  'text': 'Text block:\\nThe word \"bug\" will be tokenized as [\"b\", \"ug\"]. \"mug\", however, will be tokenized as [\"[UNK]\", \"ug\"] since the letter \"m\" was not in the base vocabulary. Likewise, the word \"thug\" will be tokenized as [\"[UNK]\", \"hug\"]: the letter \"t\" is not in the base vocabulary, and applying the merge rules results first in \"u\" and \"g\" being merged and then \"hu\" and \"g\" being merged.\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 18,\n",
       "  'text': 'Text block:\\n**Question**: How do you think the word \"unhug\" will be tokenized?\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 19,\n",
       "  'text': 'Text block:\\n### Implementing BPE for sub-word tokenization\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 20,\n",
       "  'text': 'Text block:\\nInstall the Transformers, Datasets, and Evaluate libraries to run this notebook.\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 21,\n",
       "  'text': 'Code block:\\n!pip install datasets evaluate transformers[sentencepiece]\\nOutput:\\nCollecting datasets\\n  Using cached datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\\nCollecting evaluate\\n  Using cached evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\\nRequirement already satisfied: transformers[sentencepiece] in /Users/anoop/git-repos/teaching/nlp-class-hw/bertchunker/venv/lib/python3.12/site-packages (4.45.1)\\nRequirement already satisfied: filelock in /Users/anoop/git-repos/teaching/nlp-class-hw/bertchunker/venv/lib/python3.12/site-packages (from datasets) (3.16.1)\\nRequirement already satisfied: numpy>=1.17 in /Users/anoop/git-repos/teaching/nlp-class-hw/bertchunker/venv/lib/python3.12/site-packages (from datasets) (1.26.4)\\nCollecting pyarrow>=15.0.0 (from datasets)\\n  Using cached pyarrow-17.0.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.3 kB)\\nCollecting dill<0.3.9,>=0.3.0 (from datasets)\\n  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\\nRequirement already satisfied: pandas in /Users/anoop/git-repos/teaching/nlp-class-hw/bertchunker/venv/lib/python3.12/site-packages (from datasets) (2.2.3)\\nRequirement already satisfied: requests>=2.32.2 in /Users/anoop/git-repos/teaching/nlp-class-hw/bertchunker/venv/lib/python3.12/site-packages (from datasets) (2.32.3)\\nRequirement already satisfied: tqdm>=4.66.3 in /Users/anoop/git-repos/teaching/nlp-class-hw/bertchunker/venv/lib/python3.12/site-packages (from datasets) (4.66.5)\\nCollecting xxhash (from datasets)\\n  Using cached xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\\nCollecting multiprocess (from datasets)\\n  Using cached multiprocess-0.70.17-py312-none-any.whl.metadata (7.2 kB)\\nCollecting fsspec<=2024.6.1,>=2023.1.0 (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets)\\n  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\\nCollecting aiohttp (from datasets)\\n  Downloading aiohttp-3.10.9-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.6 kB)\\nRequirement already satisfied: huggingface-hub>=0.22.0 in /Users/anoop/git-repos/teaching/nlp-class-hw/bertchunker/venv/lib/python3.12/site-packages (from datasets) (0.25.1)\\nRequirement already satisfied: packaging in /Users/anoop/git-repos/teaching/nlp-class-hw/bertchunker/venv/lib/python3.12/site-packages (from datasets) (24.1)\\nRequirement already satisfied: pyyaml>=5.1 in /Users/anoop/git-repos/teaching/nlp-class-hw/bertchunker/venv/lib/python3.12/site-packages (from datasets) (6.0.2)\\nRequirement already satisfied: regex!=2019.12.17 in /Users/anoop/git-repos/teaching/nlp-class-hw/bertchunker/venv/lib/python3.12/site-packages (from transformers[sentencepiece]) (2024.9.11)\\nRequirement already satisfied: safetensors>=0.4.1 in /Users/anoop/git-repos/teaching/nlp-class-hw/bertchunker/venv/lib/python3.12/site-packages (from transformers[sentencepiece]) (0.4.5)\\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /Users/anoop/git-repos/teaching/nlp-class-hw/bertchunker/venv/lib/python3.12/site-packages (from transformers[sentencepiece]) (0.20.0)\\nCollecting protobuf (from transformers[sentencepiece])\\n  Using cached protobuf-5.28.2-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\\nCollecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece])\\n  Using cached sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\\nCollecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\\n  Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\\nCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\\n  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\\nRequirement already satisfied: attrs>=17.3.0 in /Users/anoop/git-repos/teaching/nlp-class-hw/bertchunker/venv/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\\nCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\\n  Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\\nCollecting multidict<7.0,>=4.5 (from aiohttp->datasets)\\n  Using cached multidict-6.1.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.0 kB)\\nCollecting yarl<2.0,>=1.12.0 (from aiohttp->datasets)\\n  Downloading yarl-1.14.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (52 kB)\\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /Users/anoop/git-repos/teaching/nlp-class-hw/bertchunker/venv/lib/python3.12/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\\nRequirement already satisfied: charset-normalizer<4,>=2 in /Users/anoop/git-repos/teaching/nlp-class-hw/bertchunker/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\\nRequirement already satisfied: idna<4,>=2.5 in /Users/anoop/git-repos/teaching/nlp-class-hw/bertchunker/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\\nRequirement already satisfied: urllib3<3,>=1.21.1 in /Users/anoop/git-repos/teaching/nlp-class-hw/bertchunker/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\\nRequirement already satisfied: certifi>=2017.4.17 in /Users/anoop/git-repos/teaching/nlp-class-hw/bertchunker/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\\nINFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\\nCollecting multiprocess (from datasets)\\n  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\\nRequirement already satisfied: python-dateutil>=2.8.2 in /Users/anoop/git-repos/teaching/nlp-class-hw/bertchunker/venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\\nRequirement already satisfied: pytz>=2020.1 in /Users/anoop/git-repos/teaching/nlp-class-hw/bertchunker/venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\\nRequirement already satisfied: tzdata>=2022.7 in /Users/anoop/git-repos/teaching/nlp-class-hw/bertchunker/venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\\nRequirement already satisfied: six>=1.5 in /Users/anoop/git-repos/teaching/nlp-class-hw/bertchunker/venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\\nCollecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp->datasets)\\n  Downloading propcache-0.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\\nUsing cached datasets-3.0.1-py3-none-any.whl (471 kB)\\nUsing cached evaluate-0.4.3-py3-none-any.whl (84 kB)\\nUsing cached dill-0.3.8-py3-none-any.whl (116 kB)\\nUsing cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\\nDownloading aiohttp-3.10.9-cp312-cp312-macosx_11_0_arm64.whl (391 kB)\\nUsing cached pyarrow-17.0.0-cp312-cp312-macosx_11_0_arm64.whl (27.2 MB)\\nUsing cached sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl (1.2 MB)\\nUsing cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\\nUsing cached protobuf-5.28.2-cp38-abi3-macosx_10_9_universal2.whl (414 kB)\\nUsing cached xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\\nUsing cached aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\\nUsing cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\\nUsing cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl (51 kB)\\nUsing cached multidict-6.1.0-cp312-cp312-macosx_11_0_arm64.whl (29 kB)\\nDownloading yarl-1.14.0-cp312-cp312-macosx_11_0_arm64.whl (85 kB)\\nDownloading propcache-0.2.0-cp312-cp312-macosx_11_0_arm64.whl (45 kB)\\nInstalling collected packages: sentencepiece, xxhash, pyarrow, protobuf, propcache, multidict, fsspec, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets, evaluate\\n  Attempting uninstall: fsspec\\n    Found existing installation: fsspec 2024.9.0\\n    Uninstalling fsspec-2024.9.0:\\n      Successfully uninstalled fsspec-2024.9.0\\nSuccessfully installed aiohappyeyeballs-2.4.3 aiohttp-3.10.9 aiosignal-1.3.1 datasets-3.0.1 dill-0.3.8 evaluate-0.4.3 frozenlist-1.4.1 fsspec-2024.6.1 multidict-6.1.0 multiprocess-0.70.16 propcache-0.2.0 protobuf-5.28.2 pyarrow-17.0.0 sentencepiece-0.2.0 xxhash-3.5.0 yarl-1.14.0\\n\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 22,\n",
       "  'text': 'Text block:\\nFirst we need a corpus, so let’s create a simple one with a few sentences:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 23,\n",
       "  'text': 'Code block:\\ncorpus = [\\n    \"This is a sample corpus.\",\\n    \"This corpus will be used to show how subword tokenization works.\",\\n    \"This section shows several tokenizer algorithms.\",\\n    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\\n]\\nOutput:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 24,\n",
       "  'text': 'Text block:\\nNext, we need to pre-tokenize that corpus into words. Since we are replicating a BPE tokenizer (like GPT-2), we will use the gpt2 tokenizer for the pre-tokenization:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 25,\n",
       "  'text': 'Code block:\\nfrom transformers import AutoTokenizer\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\\nOutput:\\n/Users/anoop/git-repos/teaching/nlp-class-hw/bertchunker/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\\n  warnings.warn(\\n\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 26,\n",
       "  'text': 'Text block:\\nThen we compute the frequencies of each word in the corpus as we do the pre-tokenization:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 27,\n",
       "  'text': \"Code block:\\nfrom collections import defaultdict\\n\\nword_freqs = defaultdict(int)\\n\\nfor text in corpus:\\n    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\\n    new_words = [word for word, offset in words_with_offsets]\\n    for word in new_words:\\n        word_freqs[word] += 1\\n\\nprint(word_freqs)\\nOutput:\\ndefaultdict(<class 'int'>, {'This': 3, 'Ġis': 1, 'Ġa': 1, 'Ġsample': 1, 'Ġcorpus': 2, '.': 4, 'Ġwill': 2, 'Ġbe': 2, 'Ġused': 1, 'Ġto': 2, 'Ġshow': 1, 'Ġhow': 2, 'Ġsubword': 1, 'Ġtokenization': 1, 'Ġworks': 1, 'Ġsection': 1, 'Ġshows': 1, 'Ġseveral': 1, 'Ġtokenizer': 1, 'Ġalgorithms': 1, 'Hopefully': 1, ',': 1, 'Ġyou': 1, 'Ġable': 1, 'Ġunderstand': 1, 'Ġthey': 1, 'Ġare': 1, 'Ġtrained': 1, 'Ġand': 1, 'Ġgenerate': 1, 'Ġtokens': 1})\\n\\n\"},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 28,\n",
       "  'text': 'Text block:\\nThe next step is to compute the base vocabulary, formed by all the characters used in the corpus:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 29,\n",
       "  'text': \"Code block:\\nalphabet = []\\n\\nfor word in word_freqs.keys():\\n    for letter in word:\\n        if letter not in alphabet:\\n            alphabet.append(letter)\\nalphabet.sort()\\n\\nprint(alphabet)\\nOutput:\\n[',', '.', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ']\\n\\n\"},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 30,\n",
       "  'text': 'Text block:\\nWe also add the special tokens used by the model at the beginning of that vocabulary. In the case of GPT-2, the only special token is `<|endoftext|>`:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 31,\n",
       "  'text': \"Code block:\\nvocab = ['<|endoftext|>'] + alphabet.copy()\\nOutput:\\n\"},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 32,\n",
       "  'text': 'Text block:\\nWe now need to split each word into individual characters, to be able to start training:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 33,\n",
       "  'text': 'Code block:\\nsplits = {word: [c for c in word] for word in word_freqs.keys()}\\nOutput:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 34,\n",
       "  'text': 'Text block:\\nNow that we are ready for training, let’s write a function that computes the frequency of each pair. We’ll need to use this at each step of the training:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 35,\n",
       "  'text': 'Code block:\\ndef compute_pair_freqs(splits):\\n    pair_freqs = defaultdict(int)\\n    for word, freq in word_freqs.items():\\n        split = splits[word]\\n        if len(split) == 1:\\n            continue\\n        for i in range(len(split) - 1):\\n            pair = (split[i], split[i + 1])\\n            pair_freqs[pair] += freq\\n    return pair_freqs\\nOutput:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 36,\n",
       "  'text': 'Text block:\\nLet’s have a look at a part of this dictionary after the initial splits:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 37,\n",
       "  'text': 'Code block:\\npair_freqs = compute_pair_freqs(splits)\\n\\nfor i, key in enumerate(pair_freqs.keys()):\\n    print(f\"{key}: {pair_freqs[key]}\")\\n    if i >= 5:\\n        break\\nOutput:\\n(\\'T\\', \\'h\\'): 3\\n(\\'h\\', \\'i\\'): 3\\n(\\'i\\', \\'s\\'): 4\\n(\\'Ġ\\', \\'i\\'): 1\\n(\\'Ġ\\', \\'a\\'): 5\\n(\\'Ġ\\', \\'s\\'): 6\\n\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 38,\n",
       "  'text': 'Text block:\\nFinding the most frequent pair only takes a quick loop:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 39,\n",
       "  'text': 'Code block:\\nbest_pair = \"\"\\nmax_freq = None\\n\\nfor pair, freq in pair_freqs.items():\\n    if max_freq is None or max_freq < freq:\\n        best_pair = pair\\n        max_freq = freq\\n\\nprint(best_pair, max_freq)\\nOutput:\\n(\\'Ġ\\', \\'t\\') 7\\n\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 40,\n",
       "  'text': \"Text block:\\nSo the first merge to learn is ('Ġ', 't') -> 'Ġt', and we add 'Ġt' to the vocabulary:\\n\"},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 41,\n",
       "  'text': 'Code block:\\nmerges = {(\"Ġ\", \"t\"): \"Ġt\"}\\nvocab.append(\"Ġt\")\\nOutput:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 42,\n",
       "  'text': 'Text block:\\nTo continue, we need to apply that merge in our splits dictionary. Let’s write another function for this:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 43,\n",
       "  'text': 'Code block:\\ndef merge_pair(a, b, splits):\\n    for word in word_freqs:\\n        split = splits[word]\\n        if len(split) == 1:\\n            continue\\n\\n        i = 0\\n        while i < len(split) - 1:\\n            if split[i] == a and split[i + 1] == b:\\n                split = split[:i] + [a + b] + split[i + 2 :]\\n            else:\\n                i += 1\\n        splits[word] = split\\n    return splits\\nOutput:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 44,\n",
       "  'text': 'Text block:\\nAnd we can have a look at the result of the first merge:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 45,\n",
       "  'text': 'Code block:\\nsplits = merge_pair(\"Ġ\", \"t\", splits)\\nprint(splits[\"Ġtrained\"])\\nOutput:\\n[\\'Ġt\\', \\'r\\', \\'a\\', \\'i\\', \\'n\\', \\'e\\', \\'d\\']\\n\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 46,\n",
       "  'text': 'Text block:\\nNow we have everything we need to loop until we have learned all the merges we want. Let’s aim for a vocab size of 50:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 47,\n",
       "  'text': 'Code block:\\nvocab_size = 50\\n\\nwhile len(vocab) < vocab_size:\\n    pair_freqs = compute_pair_freqs(splits)\\n    best_pair = \"\"\\n    max_freq = None\\n    for pair, freq in pair_freqs.items():\\n        if max_freq is None or max_freq < freq:\\n            best_pair = pair\\n            max_freq = freq\\n    splits = merge_pair(*best_pair, splits)\\n    merges[best_pair] = best_pair[0] + best_pair[1]\\n    vocab.append(best_pair[0] + best_pair[1])\\nOutput:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 48,\n",
       "  'text': 'Text block:\\nAs a result, we’ve learned 19 merge rules (the initial vocabulary had a size of 31 — 30 characters in the alphabet, plus the special token):\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 49,\n",
       "  'text': \"Code block:\\nprint(merges)\\nOutput:\\n{('Ġ', 't'): 'Ġt', ('Ġ', 's'): 'Ġs', ('Ġ', 'a'): 'Ġa', ('o', 'r'): 'or', ('Ġt', 'o'): 'Ġto', ('i', 's'): 'is', ('h', 'o'): 'ho', ('ho', 'w'): 'how', ('e', 'n'): 'en', ('e', 'r'): 'er', ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('u', 's'): 'us', ('Ġ', 'w'): 'Ġw', ('l', 'l'): 'll', ('Ġto', 'k'): 'Ġtok', ('Ġtok', 'en'): 'Ġtoken', ('n', 'd'): 'nd', ('l', 'e'): 'le', ('Ġ', 'c'): 'Ġc', ('Ġc', 'or'): 'Ġcor'}\\n\\n\"},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 50,\n",
       "  'text': \"Code block:\\nprint(vocab)\\nOutput:\\n['<|endoftext|>', ',', '.', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ', 'Ġt', 'Ġs', 'Ġa', 'or', 'Ġto', 'is', 'ho', 'how', 'en', 'er', 'Th', 'This', 'us', 'Ġw', 'll', 'Ġtok', 'Ġtoken', 'nd', 'le', 'Ġc', 'Ġcor']\\n\\n\"},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 51,\n",
       "  'text': 'Text block:\\nTo tokenize a new text, we pre-tokenize it, split it, then apply all the merge rules learned:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 52,\n",
       "  'text': 'Code block:\\ndef tokenize(text):\\n    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\\n    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\\n    splits = [[l for l in word] for word in pre_tokenized_text]\\n    for pair, merge in merges.items():\\n        for idx, split in enumerate(splits):\\n            i = 0\\n            while i < len(split) - 1:\\n                if split[i] == pair[0] and split[i + 1] == pair[1]:\\n                    split = split[:i] + [merge] + split[i + 2 :]\\n                else:\\n                    i += 1\\n            splits[idx] = split\\n\\n    return sum(splits, [])\\nOutput:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 53,\n",
       "  'text': 'Text block:\\nWe can try this on any text composed of characters in the alphabet:\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 54,\n",
       "  'text': 'Code block:\\ntokenize(\"This is not a token.\")\\nOutput:\\n[\\'This\\', \\'Ġ\\', \\'is\\', \\'Ġ\\', \\'n\\', \\'o\\', \\'t\\', \\'Ġa\\', \\'Ġtoken\\', \\'.\\']\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 55,\n",
       "  'text': 'Text block:\\nOur implementation will throw an error if there is an unknown character since we didn’t do anything to handle them. GPT-2 doesn’t actually have an unknown token (it’s impossible to get an unknown character when using byte-level BPE), but this could happen here because we did not include all the possible bytes in the initial vocabulary. This aspect of BPE is beyond the scope of this section, so we’ve left the details out.\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 56,\n",
       "  'text': 'Text block:\\n### Training a transformers library tokenizer\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 57,\n",
       "  'text': \"Code block:\\ntraining_corpus = [ [i] for i in corpus ]\\nprint(training_corpus)\\nOutput:\\n[['This is a sample corpus.'], ['This corpus will be used to show how subword tokenization works.'], ['This section shows several tokenizer algorithms.'], ['Hopefully, you will be able to understand how they are trained and generate tokens.']]\\n\\n\"},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 58,\n",
       "  'text': 'Code block:\\nbpe_tokenizer = tokenizer.train_new_from_iterator(training_corpus, 275) # do 275 merges\\ntokens = bpe_tokenizer.tokenize(\"This is not a token\")\\nprint(tokens)\\nOutput:\\n\\n\\n\\n[\\'This\\', \\'Ġ\\', \\'is\\', \\'Ġ\\', \\'n\\', \\'o\\', \\'t\\', \\'Ġa\\', \\'Ġtoken\\']\\n\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 59,\n",
       "  'text': \"Text block:\\n## OpenAI vocab\\n\\nThis 50K vocabulary is created using OpenAI's variant of BPE sub-word tokenization called [tiktoken](https://github.com/openai/tiktoken) and is available here:\\n\\nhttps://huggingface.co/gpt2/blob/main/vocab.json\\n\"},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 60,\n",
       "  'text': \"Code block:\\nimport json\\ngpt_vocab = None\\nwith open('gpt_vocab.json', 'r') as f:\\n    gpt_vocab = json.load(f)\\nif gpt_vocab:\\n    print(gpt_vocab['<|endoftext|>'])\\n    try:\\n        print(gpt_vocab['Anoop'])\\n    except:\\n        print('Anoop does not exist')\\n    print(gpt_vocab['An'])\\n    print(gpt_vocab['oop'])\\nOutput:\\n50256\\nAnoop does not exist\\n2025\\n11224\\n\\n\"},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 61,\n",
       "  'text': 'Text block:\\n## End\\n'},\n",
       " {'file_type': 'ipynb',\n",
       "  'file_name': '../data/input/full/notebooks/bpe.ipynb',\n",
       "  'marker': 62,\n",
       "  'text': 'Code block:\\nfrom IPython.core.display import HTML\\n\\n\\ndef css_styling():\\n    styles = open(\"../css/notebook.css\", \"r\").read()\\n    return HTML(styles)\\ncss_styling()\\nOutput:\\n<IPython.core.display.HTML object>\\n'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbbe225-dfb8-492c-9582-82cdbf4460cc",
   "metadata": {},
   "source": [
    "### Overall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c612dfe-8a52-49a7-bfbe-b3ccfbcb52b5",
   "metadata": {},
   "source": [
    "We parse 4 different kinds of files:\n",
    "<ul>\n",
    "    <li>PDFs - PyPDF (our experiments with PyMuPDF4LLM showed worse results than this)</li>\n",
    "    <li>CSVs (very specific to this use case though) - pandas</li>\n",
    "    <li>HTMLs - boilerpy + nltk</li>\n",
    "    <li>ipynb notebooks - json</li>\n",
    "</ul>\n",
    "\n",
    "These we get from CMPT-713's course site\n",
    "\n",
    "The output of these functions is a list of extracted text contents, and these functions are later used in the pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
