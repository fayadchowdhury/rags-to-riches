{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key = OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model = MODEL,\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a haiku about recursion in programming\"\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In cycles we dive,  \n",
      "A function calls itself back,  \n",
      "Infinite within.  \n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_content(prompt: str, client: OpenAI = client, model: str = MODEL) -> str:\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model = model,\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        result = completion.choices[0].message.content\n",
    "        return result\n",
    "    except:\n",
    "        print(f\"Failed to generate content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! Self-attention is a key component of transformer architectures, which are widely used in natural language processing (NLP) and other domains. Let's break down the concept step by step.\n",
       "\n",
       "### What Is Self-Attention?\n",
       "\n",
       "Self-attention is a mechanism that allows a model to weigh the importance of different words (or tokens) in an input sequence relative to each other. This means that when processing a given word, the model can take into account the other words in the sequence to better understand context and relationships.\n",
       "\n",
       "### Why Use Self-Attention?\n",
       "\n",
       "In traditional models, like RNNs (Recurrent Neural Networks), processing sequences is inherently sequential. This can lead to difficulties in capturing long-range dependencies because information needs to be passed step-by-step. Self-attention, on the other hand, can concurrently process all tokens in the input, enabling the model to understand relationships that are far apart in the sequence without the bottleneck of sequential processing.\n",
       "\n",
       "### The Self-Attention Mechanism\n",
       "\n",
       "Let's break down the self-attention mechanism into its core components:\n",
       "\n",
       "1. **Input Representation**:\n",
       "   Each word (or token) in the input sequence is first represented as an embedding (a dense vector). For a sequence of length \\( n \\), you will have an input matrix \\( X \\) of dimensions \\( n \\times d \\), where \\( d \\) is the embedding dimension.\n",
       "\n",
       "2. **Generating Queries, Keys, and Values**:\n",
       "   For each embedding in \\( X \\), we compute three separate vectors: Query (\\( Q \\)), Key (\\( K \\)), and Value (\\( V \\)). This is done by multiplying the input matrix by three learned weight matrices:\n",
       "   \\[\n",
       "   Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V\n",
       "   \\]\n",
       "   where \\( W_Q \\), \\( W_K \\), and \\( W_V \\) are the weight matrices for queries, keys, and values, respectively.\n",
       "\n",
       "3. **Attention Scores**:\n",
       "   The next step involves calculating attention scores, which indicate how much focus to place on each word when computing the representation for a specific word. This is done using the dot product of the query vector with all key vectors, followed by a scaling factor (to prevent large gradients with softmax):\n",
       "   \\[\n",
       "   \\text{Attention}(Q, K) = \\frac{QK^T}{\\sqrt{d_k}}\n",
       "   \\]\n",
       "   Here, \\( d_k \\) is the dimension of the key vectors (to normalize).\n",
       "\n",
       "4. **Softmax**:\n",
       "   Apply the softmax function to the attention scores to convert them into a probability distribution. Each score will be in the range (0, 1), and the scores will sum to 1:\n",
       "   \\[\n",
       "   \\text{Attention Weights} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)\n",
       "   \\]\n",
       "\n",
       "5. **Weighted Sum of Values**:\n",
       "   Finally, these attention weights are used to compute a weighted sum of the value vectors \\( V \\):\n",
       "   \\[\n",
       "   \\text{Output} = \\text{Attention Weights} \\cdot V\n",
       "   \\]\n",
       "   The output for each token now reflects the context provided by all other tokens in the sequence.\n",
       "\n",
       "### Multi-Head Self-Attention\n",
       "\n",
       "In practice, transformers use a technique called multi-head attention. Instead of computing a single set of \\( Q \\), \\( K \\), and \\( V \\) vectors, multiple sets are computed (each with independent weight matrices). Each set is referred to as a \"head.\" The outputs of all heads are concatenated and then linearly transformed. This allows the model to capture different types of relationships and dependencies.\n",
       "\n",
       "### Summary\n",
       "\n",
       "In summary, self-attention allows a transformer model to dynamically compute the relevance of words in relation to one another, attending to important words regardless of their position within the sequence. This makes transformers powerful for capturing long-range dependencies and context in a parallel manner, which significantly improves tasks like translation, summarization, and more compared to earlier architectures.\n",
       "\n",
       "This mechanism, combined with the layered structure of transformers and the application of position encoding for sequential information, leads to the robust performance of models like BERT, GPT, and others in NLP tasks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(generate_content(prompt=\"Can you explain self-attention in transformer architectures to a graduate student\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
