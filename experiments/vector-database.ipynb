{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db09a1c2-11bb-4acc-9f62-191794f41ddf",
   "metadata": {},
   "source": [
    "# Pushing our embeddings to a vector database\n",
    "\n",
    "For our vector database of choice, we opted for Pinecone primarily because it is available online (we wish to extend our work to build a Streamlit application that can demonstrate the functionality of our system) and additionally it directly supports the text-embedding-ada-002 embeddings without any extra configuration. It is also free!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c45d141-72c0-499d-85e2-64ee73461b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "645c19ca-c56a-42a9-aa92-a9383aed33e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5948893-6b3b-4ed5-8358-d738ca07cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.environ[\"PINECONE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47904a68-3e42-461f-87f8-e9b83f28ae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5da0b66-c818-400e-9323-5e07a68bb594",
   "metadata": {},
   "source": [
    "### Read JSON files with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751c1b2e-a07d-441d-96f8-b6c20df1b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "lectures_embeddings_df = pd.read_json(\"../data/subset/lectures_chunks.json\", orient=\"records\", lines=False)\n",
    "exercises_embeddings_df = pd.read_json(\"../data/subset/exercises_chunks.json\", orient=\"records\", lines=False)\n",
    "notebooks_embeddings_df = pd.read_json(\"../data/subset/notebooks_chunks.json\", orient=\"records\", lines=False)\n",
    "qas_embeddings_df = pd.read_json(\"../data/subset/qas_chunks.json\", orient=\"records\", lines=False)\n",
    "references_embeddings_df = pd.read_json(\"../data/subset/references_chunks.json\", orient=\"records\", lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae577209-9c4f-4e4b-9a6b-acb3e8c5ec70",
   "metadata": {},
   "source": [
    "### Create data to upsert into Pinecone\n",
    "\n",
    "Use a random UUID4 as ID and pass the metadata dictionary from chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fcd87a-63b9-4804-a8fd-ca2518d36885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_pinecone(df):\n",
    "    pinecone_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        embedding = list(row['embedding'])  # Ensure embedding is in list format\n",
    "        # Prepare the metadata dictionary with the relevant columns\n",
    "        metadata = {\n",
    "            \"file_type\": sample['file_type'],\n",
    "            \"file_name\": sample['file_name'],\n",
    "            \"marker\": str(sample['marker']),\n",
    "            \"sub_marker\": str(sample['sub_marker']),\n",
    "            \"first_10_words\": sample['first_10_words']\n",
    "        }\n",
    "        pinecone_data.append({\n",
    "            'id': str(uuid.uuid4()),\n",
    "            'values': embedding,\n",
    "            'metadata': metadata\n",
    "        })\n",
    "    return pinecone_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72a6fc3-0b96-4ced-850a-48c2c4082013",
   "metadata": {},
   "outputs": [],
   "source": [
    "lectures_pc_data = prepare_data_for_pinecone(lectures_embeddings_df)\n",
    "exercises_pc_data = prepare_data_for_pinecone(exercises_embeddings_df)\n",
    "notebooks_pc_data = prepare_data_for_pinecone(notebooks_embeddings_df)\n",
    "qas_pc_data = prepare_data_for_pinecone(qas_embeddings_df)\n",
    "references_pc_data = prepare_data_for_pinecone(references_embeddings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f759d8b0-1745-4800-9b48-2d12e78de766",
   "metadata": {},
   "source": [
    "### Send to Pinecone\n",
    "\n",
    "If this is done once, don't do it again please"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba929052-2d9d-454a-8d86-46c327764eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_name = \"subset\"\n",
    "index_name = \"nlp-material-full\"\n",
    "index = pc.Index(name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dfe5f6-53ad-42b7-b80e-e16f4b339004",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.upsert(vectors=lectures_pc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e0c4d7-b830-45e1-893b-1a4e97847419",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.upsert(vectors=exercises_pc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49e32dc-ffec-46b5-9aad-dce2c3638e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.upsert(vectors=notebooks_pc_data) # This failed because the request size needs to be smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd8f6a9-d399-4578-a981-2a7a509415cd",
   "metadata": {},
   "source": [
    "This failed because there was too much data (not necessarily incredibly large chunks, but an incredibly large number of chunks) in the overall request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba13a881-7d53-45ce-af97-f3a6f9f922af",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.upsert(vectors=qas_pc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3323539e-e137-4af8-ba08-eb8e844b8797",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.upsert(vectors=references_pc_data) # This failed because the message length is too large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8775a8cf-c7af-4fcf-9fc0-118f7de84392",
   "metadata": {},
   "source": [
    "This failed because there were a few very large chunks (particularly from HTMLs and reference PDFs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca4828b-df0f-4c5d-a0c2-85f6b21402c2",
   "metadata": {},
   "source": [
    "The two failures prompted us to do the following:\n",
    "<ul>\n",
    "    <li>Use tiktoken by OpenAI to tokenize the text during chunking to get an accurate estimate of the size of each chunk.</li>\n",
    "    <li>Include a regex matching for a special, weird tag of \"\\<latexit>\" when converting a few LaTeX PDFs (mostly reference papers) in our PDF parser to remove that content; it turned out to be a continuous string of 256 characters (possibly some hash) that held no value but was unnecessarily increasing the size of the chunks.</li>\n",
    "    <li>Batch the data to be upserted</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
